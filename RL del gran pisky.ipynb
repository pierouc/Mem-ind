{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from gym_anytrading.envs import StocksEnv\n",
    "from finta import TA\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f30d7eff5e960be",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "tickers_negro = [\n",
    "    (\"NVDA\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"INTC\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"FRT\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"NKE\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"TSM\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"USB\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"XOM\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"BA\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"NEM\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"HBAN\", \"2012-01-01\", \"2023-12-31\"),\n",
    "\n",
    "    (\"VZ\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"PCG\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"FCX\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"C\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"OXY\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"KEY\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"WFC\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"MRO\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"GOOGL\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"NFLX\", \"2012-01-01\", \"2023-12-31\"),\n",
    "\n",
    "    (\"HDB\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"MU\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"AVY\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"MET\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"MSTR\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"WMB\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"BSX\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"EBAY\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"SO\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"CTSH\", \"2012-01-01\", \"2023-12-31\"),\n",
    "\n",
    "    (\"BBWI\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"V\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"VFC\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"MOS\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"CRM\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"SCHW\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"CNP\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"MDT\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"EXC\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"MTCH\", \"2012-01-01\", \"2023-12-31\"),\n",
    "\n",
    "    (\"PARA\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"UAL\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"JNJ\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"LUV\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"MCD\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"JPM\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"GILD\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"CVS\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"ORCL\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"UHS\", \"2012-01-01\", \"2023-12-31\")\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T05:11:00.909822Z",
     "start_time": "2024-04-18T05:11:00.904011Z"
    }
   },
   "id": "be1d7d6cd209d8c4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "RSI"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c61b8fd8b31c21b"
  },
  {
   "cell_type": "code",
   "source": [
    "datas = {}\n",
    "\n",
    "for k,i in enumerate(tickers_negro): \n",
    "    datas[i[0]] = yf.download(i[0], start=i[1], end= i[2],interval=\"1d\")\n",
    "\n",
    "    # prices = np.array(data['Adj Close'].values) #Adj Close prices\n",
    "    dates = np.array(datas[i[0]].index.values).astype('datetime64[ns]') #Dates\n",
    "    datas[i[0]]['rsi'] = TA.RSI(datas[i[0]],14,column='adj close') #Relative Strength Index (RSI)\n",
    "    datas[i[0]]['macd'] = TA.MACD(datas[i[0]],column='adj close')['MACD'] #MACD Line\n",
    "    datas[i[0]]['macd_signal'] = TA.MACD(datas[i[0]], column='adj close')['SIGNAL'] #MACD Signal Line\n",
    "    # datas[i[0]]['bb_bbm'] = TA.BBANDS(datas[i[0]], column='adj close')['BB_MIDDLE'] #Bollinger Bands (BB) middle band (BBM)\n",
    "    datas[i[0]]['bb_bbu'] = TA.BBANDS(datas[i[0]], column='adj close')['BB_UPPER'] #Bollinger Bands (BB) upper band (BBU)\n",
    "    datas[i[0]]['bb_bbl'] = TA.BBANDS(datas[i[0]],column='adj close')['BB_LOWER'] #Bollinger Bands (BB) lower band (BBL)\n",
    "    # datas[i[0]]['bb_width'] = TA.BBWIDTH(datas[i[0]],column='adj close') #Bollinger Bands (BB) width\n",
    "    datas[i[0]]['obv'] = TA.OBV(datas[i[0]],'adj close') #On Balance Volume (OBV)\n",
    "    # datas[i[0]]['from'] = pd.DataFrame([k] * len(datas[i[0]]))\n",
    "    \n",
    "    datas[i[0]].fillna(0, inplace=True)\n",
    "    datas[i[0]] = datas[i[0]][datas[i[0]].index > '2013-01-01']\n",
    "    datas[i[0]]\n",
    "    \n",
    "df= pd.concat([df for df in datas.values()], axis=0)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T05:11:31.403082Z",
     "start_time": "2024-04-18T05:11:01.670742Z"
    }
   },
   "id": "b28d72a1a8c1956d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "Date                                                                     \n",
       "2013-01-02    3.140000    3.182500    3.127500    3.180000    2.935840   \n",
       "2013-01-03    3.180000    3.217500    3.145000    3.182500    2.938148   \n",
       "2013-01-04    3.187500    3.297500    3.177500    3.287500    3.035087   \n",
       "2013-01-07    3.285000    3.295000    3.170000    3.192500    2.947381   \n",
       "2013-01-08    3.200000    3.210000    3.100000    3.122500    2.882755   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "2023-12-22  151.500000  153.570007  151.250000  153.080002  152.890701   \n",
       "2023-12-26  153.279999  155.179993  152.630005  154.130005  153.939407   \n",
       "2023-12-27  153.929993  153.949997  151.699997  152.610001  152.421280   \n",
       "2023-12-28  152.460007  153.759995  151.910004  152.679993  152.491180   \n",
       "2023-12-29  152.699997  153.050003  151.350006  152.440002  152.251495   \n",
       "\n",
       "              Volume        rsi      macd  macd_signal      bb_bbu  \\\n",
       "Date                                                                 \n",
       "2013-01-02  47883600  60.223756  0.011304     0.011125    3.206539   \n",
       "2013-01-03  29888800  60.423360  0.017204     0.012340    3.216985   \n",
       "2013-01-04  52496800  67.742526  0.029362     0.015745    3.246299   \n",
       "2013-01-07  61073200  57.399465  0.031557     0.018907    3.249990   \n",
       "2013-01-08  46642400  51.196623  0.027762     0.020678    3.243191   \n",
       "...              ...        ...       ...          ...         ...   \n",
       "2023-12-22    382700  71.002017  5.003540     4.321616  156.466948   \n",
       "2023-12-26    288000  72.122515  5.169724     4.491238  157.901989   \n",
       "2023-12-27    374400  68.024730  5.119907     4.616972  158.752432   \n",
       "2023-12-28    332900  68.114562  5.028106     4.699199  159.373893   \n",
       "2023-12-29    239900  67.415177  4.879762     4.735311  159.932114   \n",
       "\n",
       "                bb_bbl          obv  \n",
       "Date                                 \n",
       "2013-01-02    2.971461 -390527600.0  \n",
       "2013-01-03    2.976765 -360638800.0  \n",
       "2013-01-04    2.977201 -308142000.0  \n",
       "2013-01-07    2.993260 -369215200.0  \n",
       "2013-01-08    3.013309 -415857600.0  \n",
       "...                ...          ...  \n",
       "2023-12-22  129.343050   71061900.0  \n",
       "2023-12-26  129.598010   71349900.0  \n",
       "2023-12-27  130.400566   70975500.0  \n",
       "2023-12-28  131.472104   71308400.0  \n",
       "2023-12-29  132.409885   71068500.0  \n",
       "\n",
       "[138400 rows x 12 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>rsi</th>\n",
       "      <th>macd</th>\n",
       "      <th>macd_signal</th>\n",
       "      <th>bb_bbu</th>\n",
       "      <th>bb_bbl</th>\n",
       "      <th>obv</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-02</th>\n",
       "      <td>3.140000</td>\n",
       "      <td>3.182500</td>\n",
       "      <td>3.127500</td>\n",
       "      <td>3.180000</td>\n",
       "      <td>2.935840</td>\n",
       "      <td>47883600</td>\n",
       "      <td>60.223756</td>\n",
       "      <td>0.011304</td>\n",
       "      <td>0.011125</td>\n",
       "      <td>3.206539</td>\n",
       "      <td>2.971461</td>\n",
       "      <td>-390527600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-03</th>\n",
       "      <td>3.180000</td>\n",
       "      <td>3.217500</td>\n",
       "      <td>3.145000</td>\n",
       "      <td>3.182500</td>\n",
       "      <td>2.938148</td>\n",
       "      <td>29888800</td>\n",
       "      <td>60.423360</td>\n",
       "      <td>0.017204</td>\n",
       "      <td>0.012340</td>\n",
       "      <td>3.216985</td>\n",
       "      <td>2.976765</td>\n",
       "      <td>-360638800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-04</th>\n",
       "      <td>3.187500</td>\n",
       "      <td>3.297500</td>\n",
       "      <td>3.177500</td>\n",
       "      <td>3.287500</td>\n",
       "      <td>3.035087</td>\n",
       "      <td>52496800</td>\n",
       "      <td>67.742526</td>\n",
       "      <td>0.029362</td>\n",
       "      <td>0.015745</td>\n",
       "      <td>3.246299</td>\n",
       "      <td>2.977201</td>\n",
       "      <td>-308142000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-07</th>\n",
       "      <td>3.285000</td>\n",
       "      <td>3.295000</td>\n",
       "      <td>3.170000</td>\n",
       "      <td>3.192500</td>\n",
       "      <td>2.947381</td>\n",
       "      <td>61073200</td>\n",
       "      <td>57.399465</td>\n",
       "      <td>0.031557</td>\n",
       "      <td>0.018907</td>\n",
       "      <td>3.249990</td>\n",
       "      <td>2.993260</td>\n",
       "      <td>-369215200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-08</th>\n",
       "      <td>3.200000</td>\n",
       "      <td>3.210000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>3.122500</td>\n",
       "      <td>2.882755</td>\n",
       "      <td>46642400</td>\n",
       "      <td>51.196623</td>\n",
       "      <td>0.027762</td>\n",
       "      <td>0.020678</td>\n",
       "      <td>3.243191</td>\n",
       "      <td>3.013309</td>\n",
       "      <td>-415857600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-22</th>\n",
       "      <td>151.500000</td>\n",
       "      <td>153.570007</td>\n",
       "      <td>151.250000</td>\n",
       "      <td>153.080002</td>\n",
       "      <td>152.890701</td>\n",
       "      <td>382700</td>\n",
       "      <td>71.002017</td>\n",
       "      <td>5.003540</td>\n",
       "      <td>4.321616</td>\n",
       "      <td>156.466948</td>\n",
       "      <td>129.343050</td>\n",
       "      <td>71061900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-26</th>\n",
       "      <td>153.279999</td>\n",
       "      <td>155.179993</td>\n",
       "      <td>152.630005</td>\n",
       "      <td>154.130005</td>\n",
       "      <td>153.939407</td>\n",
       "      <td>288000</td>\n",
       "      <td>72.122515</td>\n",
       "      <td>5.169724</td>\n",
       "      <td>4.491238</td>\n",
       "      <td>157.901989</td>\n",
       "      <td>129.598010</td>\n",
       "      <td>71349900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-27</th>\n",
       "      <td>153.929993</td>\n",
       "      <td>153.949997</td>\n",
       "      <td>151.699997</td>\n",
       "      <td>152.610001</td>\n",
       "      <td>152.421280</td>\n",
       "      <td>374400</td>\n",
       "      <td>68.024730</td>\n",
       "      <td>5.119907</td>\n",
       "      <td>4.616972</td>\n",
       "      <td>158.752432</td>\n",
       "      <td>130.400566</td>\n",
       "      <td>70975500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-28</th>\n",
       "      <td>152.460007</td>\n",
       "      <td>153.759995</td>\n",
       "      <td>151.910004</td>\n",
       "      <td>152.679993</td>\n",
       "      <td>152.491180</td>\n",
       "      <td>332900</td>\n",
       "      <td>68.114562</td>\n",
       "      <td>5.028106</td>\n",
       "      <td>4.699199</td>\n",
       "      <td>159.373893</td>\n",
       "      <td>131.472104</td>\n",
       "      <td>71308400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-29</th>\n",
       "      <td>152.699997</td>\n",
       "      <td>153.050003</td>\n",
       "      <td>151.350006</td>\n",
       "      <td>152.440002</td>\n",
       "      <td>152.251495</td>\n",
       "      <td>239900</td>\n",
       "      <td>67.415177</td>\n",
       "      <td>4.879762</td>\n",
       "      <td>4.735311</td>\n",
       "      <td>159.932114</td>\n",
       "      <td>132.409885</td>\n",
       "      <td>71068500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>138400 rows × 12 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "data = yf.download('GOOG', start='2012-01-01', end='2023-12-31' ,interval=\"1d\")\n",
    "prices = np.array(data['Adj Close'].values) #Adj Close prices\n",
    "\n",
    "\n",
    "dates = np.array(data.index.values).astype('datetime64[ns]') #Dates\n",
    "data['rsi'] = TA.RSI(data,14,column='adj close') #Relative Strength Index (RSI)\n",
    "data['macd'] = TA.MACD(data,column='adj close')['MACD'] #MACD Line\n",
    "data['macd_signal'] = TA.MACD(data, column='adj close')['SIGNAL'] #MACD Signal Line\n",
    "# data['bb_bbm'] = TA.BBANDS(data, column='adj close')['BB_MIDDLE'] #Bollinger Bands (BB) middle band (BBM)\n",
    "data['bb_bbu'] = TA.BBANDS(data, column='adj close')['BB_UPPER'] #Bollinger Bands (BB) upper band (BBU)\n",
    "data['bb_bbl'] = TA.BBANDS(data,column='adj close')['BB_LOWER'] #Bollinger Bands (BB) lower band (BBL)\n",
    "# data['bb_width'] = TA.BBWIDTH(data,column='adj close') #Bollinger Bands (BB) width\n",
    "data['obv'] = TA.OBV(data,'adj close') #On Balance Volume (OBV)\n",
    "\n",
    "data.fillna(0, inplace=True)\n",
    "data = data[data.index > '2013-01-01']\n",
    "len(data)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T05:11:38.005636Z",
     "start_time": "2024-04-18T05:11:37.182110Z"
    }
   },
   "id": "615e4f3698685dfc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2768"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T07:20:14.237742Z",
     "start_time": "2024-04-18T07:20:14.213206Z"
    }
   },
   "cell_type": "code",
   "source": "sns.heatmap(data.corr(), annot=True)",
   "id": "1221cd0abd8315d3",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[104], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43msns\u001B[49m\u001B[38;5;241m.\u001B[39mheatmap(data\u001B[38;5;241m.\u001B[39mcorr(), annot\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'sns' is not defined"
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Enviroment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29948330e9554f1f"
  },
  {
   "cell_type": "code",
   "source": [
    "from enum import Enum\n",
    "def add_signals(env):\n",
    "    start = env.frame_bound[0] - env.window_size\n",
    "    end = env.frame_bound[1]\n",
    "    prices = env.df.loc[:, 'Adj Close'].to_numpy()[start:end]\n",
    "    signal_features = env.df.loc[:,['Adj Close','rsi','Volume','macd','macd_signal','bb_bbu','bb_bbl','obv']].to_numpy()[start:end]\n",
    "    return prices, signal_features\n",
    "\n",
    "def reward_function(env, action):\n",
    "    step_reward = 0\n",
    "\n",
    "    class Positions(Enum):\n",
    "        Short = 0\n",
    "        Long = 1\n",
    "\n",
    "    trade = False\n",
    "    if (\n",
    "            (action == Actions.Buy.value and env._position == Positions.Short) or\n",
    "            (action == Actions.Sell.value and env._position == Positions.Long)\n",
    "    ):\n",
    "        trade = True\n",
    "\n",
    "    if trade:\n",
    "        current_price = env.prices[env._current_tick]\n",
    "        last_trade_price = env.prices[env._last_trade_tick]\n",
    "        price_diff = current_price - last_trade_price\n",
    "\n",
    "        # Penalizar o recompensar en base a la magnitud de la diferencia de precios\n",
    "        if price_diff > 0:  # Si se gana dinero\n",
    "            step_reward += 0.5 * price_diff  # Multiplicador arbitrario para ajustar la importancia\n",
    "        else:  # Si se pierde dinero\n",
    "            step_reward += 1.5 * price_diff  # Multiplicador arbitrario para ajustar la importancia\n",
    "\n",
    "    return step_reward\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T05:11:42.744719Z",
     "start_time": "2024-04-18T05:11:42.739902Z"
    }
   },
   "id": "ef126a3fef2e014b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "class customEnv(StocksEnv):\n",
    "    \"\"\"\n",
    "    Custom Environment for RL trading\n",
    "    \"\"\"\n",
    "    _process_data = add_signals    \n",
    "\n",
    "    # _calculate_reward = reward_function\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T05:12:27.932703Z",
     "start_time": "2024-04-18T05:12:27.930305Z"
    }
   },
   "id": "d4ea84ef234a5f4a",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "996648e2a345f8d4"
  },
  {
   "cell_type": "code",
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "env = customEnv(df=data, window_size=30, frame_bound=(30, len(data)))\n",
    "env.trade_fee_ask_percent = 0\n",
    "env.trade_fee_bid_percent = 0\n",
    "env.reset(seed=42)\n",
    "env = Monitor(env, './logs_v2')  # Envolver con Monitor wrapper\n",
    "env_maker = lambda: env\n",
    "env_train = DummyVecEnv([env_maker])\n",
    "#add dqn\n",
    "model = A2C(policy='MlpPolicy', env=env_train, verbose=1)\n",
    "\n",
    "eval_callback = EvalCallback(env_train, best_model_save_path='./logs_v2/',\n",
    "                             log_path='./logs_v2/', eval_freq=500,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "model.learn(total_timesteps=500000,callback=eval_callback)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T07:20:14.211599Z",
     "start_time": "2024-04-18T05:43:41.888829Z"
    }
   },
   "id": "7ab823a0bfd15f88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Eval num_timesteps=500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.372   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 7.24     |\n",
      "|    value_loss         | 173      |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 71  |\n",
      "|    iterations      | 100 |\n",
      "|    time_elapsed    | 6   |\n",
      "|    total_timesteps | 500 |\n",
      "----------------------------\n",
      "Eval num_timesteps=1000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.3     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.00441  |\n",
      "|    value_loss         | 0.00223  |\n",
      "------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 71   |\n",
      "|    iterations      | 200  |\n",
      "|    time_elapsed    | 13   |\n",
      "|    total_timesteps | 1000 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=1500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.252   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 8.55     |\n",
      "|    value_loss         | 94       |\n",
      "------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 71   |\n",
      "|    iterations      | 300  |\n",
      "|    time_elapsed    | 21   |\n",
      "|    total_timesteps | 1500 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=2000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.481   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 6.12     |\n",
      "|    value_loss         | 63.1     |\n",
      "------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 71   |\n",
      "|    iterations      | 400  |\n",
      "|    time_elapsed    | 27   |\n",
      "|    total_timesteps | 2000 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=2500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.663   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 1.9      |\n",
      "|    value_loss         | 5.91     |\n",
      "------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 71   |\n",
      "|    iterations      | 500  |\n",
      "|    time_elapsed    | 35   |\n",
      "|    total_timesteps | 2500 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=3000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 0         |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.596    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -0.28     |\n",
      "|    value_loss         | 0.379     |\n",
      "-------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 71   |\n",
      "|    iterations      | 600  |\n",
      "|    time_elapsed    | 42   |\n",
      "|    total_timesteps | 3000 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=3500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.691   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -0.0356  |\n",
      "|    value_loss         | 0.00256  |\n",
      "------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 71   |\n",
      "|    iterations      | 700  |\n",
      "|    time_elapsed    | 49   |\n",
      "|    total_timesteps | 3500 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=405.98 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 406      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.691   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.898   |\n",
      "|    value_loss         | 2.5      |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 71   |\n",
      "|    iterations      | 800  |\n",
      "|    time_elapsed    | 56   |\n",
      "|    total_timesteps | 4000 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4500, episode_reward=175.95 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 176      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.689   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 1.41     |\n",
      "|    value_loss         | 4.06     |\n",
      "------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 71   |\n",
      "|    iterations      | 900  |\n",
      "|    time_elapsed    | 62   |\n",
      "|    total_timesteps | 4500 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=5000, episode_reward=50.84 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 50.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.663    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 0.183     |\n",
      "|    value_loss         | 0.747     |\n",
      "-------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 71   |\n",
      "|    iterations      | 1000 |\n",
      "|    time_elapsed    | 70   |\n",
      "|    total_timesteps | 5000 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=5500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.564   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 1.13     |\n",
      "|    value_loss         | 4.75     |\n",
      "------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 70   |\n",
      "|    iterations      | 1100 |\n",
      "|    time_elapsed    | 77   |\n",
      "|    total_timesteps | 5500 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6000, episode_reward=248.43 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 248       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.691    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 1.04      |\n",
      "|    value_loss         | 2.88      |\n",
      "-------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 70   |\n",
      "|    iterations      | 1200 |\n",
      "|    time_elapsed    | 84   |\n",
      "|    total_timesteps | 6000 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.571   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -0.0793  |\n",
      "|    value_loss         | 0.0245   |\n",
      "------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 71   |\n",
      "|    iterations      | 1300 |\n",
      "|    time_elapsed    | 91   |\n",
      "|    total_timesteps | 6500 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=7000, episode_reward=3.42 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 3.42     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.667   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -0.104   |\n",
      "|    value_loss         | 0.0184   |\n",
      "------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 71   |\n",
      "|    iterations      | 1400 |\n",
      "|    time_elapsed    | 97   |\n",
      "|    total_timesteps | 7000 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=7500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.604   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.215    |\n",
      "|    value_loss         | 0.183    |\n",
      "------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 71   |\n",
      "|    iterations      | 1500 |\n",
      "|    time_elapsed    | 104  |\n",
      "|    total_timesteps | 7500 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8000, episode_reward=59.55 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 59.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.66     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 1.46      |\n",
      "|    value_loss         | 4.53      |\n",
      "-------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 71   |\n",
      "|    iterations      | 1600 |\n",
      "|    time_elapsed    | 111  |\n",
      "|    total_timesteps | 8000 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8500, episode_reward=419.27 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 419       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.69     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 1.63      |\n",
      "|    value_loss         | 5.5       |\n",
      "-------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 71   |\n",
      "|    iterations      | 1700 |\n",
      "|    time_elapsed    | 118  |\n",
      "|    total_timesteps | 8500 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=9000, episode_reward=427.98 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 428      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.685   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 1.63     |\n",
      "|    value_loss         | 5.29     |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 71   |\n",
      "|    iterations      | 1800 |\n",
      "|    time_elapsed    | 125  |\n",
      "|    total_timesteps | 9000 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=9500, episode_reward=-3.49 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | -3.49    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.674   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.0571   |\n",
      "|    value_loss         | 0.0122   |\n",
      "------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 72   |\n",
      "|    iterations      | 1900 |\n",
      "|    time_elapsed    | 131  |\n",
      "|    total_timesteps | 9500 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=10000, episode_reward=8.54 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 8.54     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.657   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 0.0315   |\n",
      "|    value_loss         | 0.0013   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 72    |\n",
      "|    iterations      | 2000  |\n",
      "|    time_elapsed    | 138   |\n",
      "|    total_timesteps | 10000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=3.19 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 3.19      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.613    |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | 1.5       |\n",
      "|    value_loss         | 4.34      |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 71    |\n",
      "|    iterations      | 2100  |\n",
      "|    time_elapsed    | 146   |\n",
      "|    total_timesteps | 10500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=69.45 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 69.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.64    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | -0.0349  |\n",
      "|    value_loss         | 0.0145   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 71    |\n",
      "|    iterations      | 2200  |\n",
      "|    time_elapsed    | 153   |\n",
      "|    total_timesteps | 11000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=306.53 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 307      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.681   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | 0.00881  |\n",
      "|    value_loss         | 0.000324 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 72    |\n",
      "|    iterations      | 2300  |\n",
      "|    time_elapsed    | 159   |\n",
      "|    total_timesteps | 11500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=299.77 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 300      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.69    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | 1.64     |\n",
      "|    value_loss         | 5.41     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 72    |\n",
      "|    iterations      | 2400  |\n",
      "|    time_elapsed    | 166   |\n",
      "|    total_timesteps | 12000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=65.58 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 65.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.669   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2499     |\n",
      "|    policy_loss        | 1.62     |\n",
      "|    value_loss         | 4.73     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 72    |\n",
      "|    iterations      | 2500  |\n",
      "|    time_elapsed    | 173   |\n",
      "|    total_timesteps | 12500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=-9.13 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | -9.13    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.6     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2599     |\n",
      "|    policy_loss        | 0.0911   |\n",
      "|    value_loss         | 0.0287   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 72    |\n",
      "|    iterations      | 2600  |\n",
      "|    time_elapsed    | 179   |\n",
      "|    total_timesteps | 13000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.562   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2699     |\n",
      "|    policy_loss        | -0.0844  |\n",
      "|    value_loss         | 0.0366   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 72    |\n",
      "|    iterations      | 2700  |\n",
      "|    time_elapsed    | 185   |\n",
      "|    total_timesteps | 13500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=1.28 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 1.28     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.664   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2799     |\n",
      "|    policy_loss        | 0.11     |\n",
      "|    value_loss         | 0.0315   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 72    |\n",
      "|    iterations      | 2800  |\n",
      "|    time_elapsed    | 192   |\n",
      "|    total_timesteps | 14000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=6.81 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 6.81     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.683   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2899     |\n",
      "|    policy_loss        | -0.0117  |\n",
      "|    value_loss         | 0.000987 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 73    |\n",
      "|    iterations      | 2900  |\n",
      "|    time_elapsed    | 198   |\n",
      "|    total_timesteps | 14500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-4.00 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | -4       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.674   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | -0.191   |\n",
      "|    value_loss         | 0.105    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 73    |\n",
      "|    iterations      | 3000  |\n",
      "|    time_elapsed    | 203   |\n",
      "|    total_timesteps | 15000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=12.98 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 13        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 15500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.687    |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3099      |\n",
      "|    policy_loss        | -0.283    |\n",
      "|    value_loss         | 0.756     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 73    |\n",
      "|    iterations      | 3100  |\n",
      "|    time_elapsed    | 209   |\n",
      "|    total_timesteps | 15500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.637   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3199     |\n",
      "|    policy_loss        | 0.0557   |\n",
      "|    value_loss         | 0.00857  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 74    |\n",
      "|    iterations      | 3200  |\n",
      "|    time_elapsed    | 214   |\n",
      "|    total_timesteps | 16000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 16500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.575   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3299     |\n",
      "|    policy_loss        | -0.0566  |\n",
      "|    value_loss         | 0.027    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 74    |\n",
      "|    iterations      | 3300  |\n",
      "|    time_elapsed    | 220   |\n",
      "|    total_timesteps | 16500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=13.24 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 13.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.692    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | 0.335     |\n",
      "|    value_loss         | 0.415     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 75    |\n",
      "|    iterations      | 3400  |\n",
      "|    time_elapsed    | 226   |\n",
      "|    total_timesteps | 17000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 17500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.675   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3499     |\n",
      "|    policy_loss        | 1.45     |\n",
      "|    value_loss         | 4.57     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 75    |\n",
      "|    iterations      | 3500  |\n",
      "|    time_elapsed    | 231   |\n",
      "|    total_timesteps | 17500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=330.60 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 331      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.648   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3599     |\n",
      "|    policy_loss        | -0.191   |\n",
      "|    value_loss         | 0.0947   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 75    |\n",
      "|    iterations      | 3600  |\n",
      "|    time_elapsed    | 237   |\n",
      "|    total_timesteps | 18000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=51.31 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 51.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 18500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.69    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3699     |\n",
      "|    policy_loss        | 0.0177   |\n",
      "|    value_loss         | 0.00628  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 76    |\n",
      "|    iterations      | 3700  |\n",
      "|    time_elapsed    | 242   |\n",
      "|    total_timesteps | 18500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=201.59 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 202      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 19000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.66    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3799     |\n",
      "|    policy_loss        | -0.208   |\n",
      "|    value_loss         | 0.118    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 76    |\n",
      "|    iterations      | 3800  |\n",
      "|    time_elapsed    | 247   |\n",
      "|    total_timesteps | 19000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=253.20 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 253      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 19500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.677   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3899     |\n",
      "|    policy_loss        | 0.252    |\n",
      "|    value_loss         | 0.303    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 76    |\n",
      "|    iterations      | 3900  |\n",
      "|    time_elapsed    | 253   |\n",
      "|    total_timesteps | 19500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-3.33 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | -3.33    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.679   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | 0.252    |\n",
      "|    value_loss         | 0.209    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 77    |\n",
      "|    iterations      | 4000  |\n",
      "|    time_elapsed    | 258   |\n",
      "|    total_timesteps | 20000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=-10.47 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | -10.5    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 20500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.682   |\n",
      "|    explained_variance | 3.58e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4099     |\n",
      "|    policy_loss        | 1.68     |\n",
      "|    value_loss         | 5.08     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 77    |\n",
      "|    iterations      | 4100  |\n",
      "|    time_elapsed    | 264   |\n",
      "|    total_timesteps | 20500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=162.98 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 163       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 21000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.611    |\n",
      "|    explained_variance | -3.58e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4199      |\n",
      "|    policy_loss        | 1.44      |\n",
      "|    value_loss         | 4.03      |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 77    |\n",
      "|    iterations      | 4200  |\n",
      "|    time_elapsed    | 269   |\n",
      "|    total_timesteps | 21000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=155.65 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 156      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 21500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.594   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4299     |\n",
      "|    policy_loss        | 3.82     |\n",
      "|    value_loss         | 35.9     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 78    |\n",
      "|    iterations      | 4300  |\n",
      "|    time_elapsed    | 274   |\n",
      "|    total_timesteps | 21500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=162.38 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 162      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 22000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.656   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4399     |\n",
      "|    policy_loss        | -0.0124  |\n",
      "|    value_loss         | 0.000331 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 78    |\n",
      "|    iterations      | 4400  |\n",
      "|    time_elapsed    | 280   |\n",
      "|    total_timesteps | 22000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=148.95 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 149      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 22500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.671   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4499     |\n",
      "|    policy_loss        | -0.343   |\n",
      "|    value_loss         | 0.576    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 78    |\n",
      "|    iterations      | 4500  |\n",
      "|    time_elapsed    | 285   |\n",
      "|    total_timesteps | 22500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=150.34 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 150      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 23000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.664   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4599     |\n",
      "|    policy_loss        | -0.527   |\n",
      "|    value_loss         | 1.1      |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 78    |\n",
      "|    iterations      | 4600  |\n",
      "|    time_elapsed    | 291   |\n",
      "|    total_timesteps | 23000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=314.73 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 315      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 23500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.56    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4699     |\n",
      "|    policy_loss        | 1.53     |\n",
      "|    value_loss         | 4.5      |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 79    |\n",
      "|    iterations      | 4700  |\n",
      "|    time_elapsed    | 296   |\n",
      "|    total_timesteps | 23500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=325.70 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 326      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 24000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.653   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4799     |\n",
      "|    policy_loss        | -0.0515  |\n",
      "|    value_loss         | 0.00944  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 79    |\n",
      "|    iterations      | 4800  |\n",
      "|    time_elapsed    | 302   |\n",
      "|    total_timesteps | 24000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=309.21 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 309      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 24500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.616   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4899     |\n",
      "|    policy_loss        | 0.0108   |\n",
      "|    value_loss         | 0.000777 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 79    |\n",
      "|    iterations      | 4900  |\n",
      "|    time_elapsed    | 307   |\n",
      "|    total_timesteps | 24500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=298.53 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 299       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 25000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.535    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4999      |\n",
      "|    policy_loss        | 5.08      |\n",
      "|    value_loss         | 72        |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 79    |\n",
      "|    iterations      | 5000  |\n",
      "|    time_elapsed    | 313   |\n",
      "|    total_timesteps | 25000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=136.26 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 136      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 25500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.627   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5099     |\n",
      "|    policy_loss        | 0.196    |\n",
      "|    value_loss         | 0.214    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 79    |\n",
      "|    iterations      | 5100  |\n",
      "|    time_elapsed    | 319   |\n",
      "|    total_timesteps | 25500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=199.15 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 199       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.631    |\n",
      "|    explained_variance | -3.58e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5199      |\n",
      "|    policy_loss        | 1.59      |\n",
      "|    value_loss         | 3.89      |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 80    |\n",
      "|    iterations      | 5200  |\n",
      "|    time_elapsed    | 324   |\n",
      "|    total_timesteps | 26000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=120.08 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 120      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 26500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.605   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5299     |\n",
      "|    policy_loss        | -0.00493 |\n",
      "|    value_loss         | 0.000465 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 80    |\n",
      "|    iterations      | 5300  |\n",
      "|    time_elapsed    | 330   |\n",
      "|    total_timesteps | 26500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=11.55 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 11.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 27000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.601   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5399     |\n",
      "|    policy_loss        | -1.13    |\n",
      "|    value_loss         | 2.77     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 80    |\n",
      "|    iterations      | 5400  |\n",
      "|    time_elapsed    | 336   |\n",
      "|    total_timesteps | 27000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=120.20 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 120      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 27500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.492   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5499     |\n",
      "|    policy_loss        | 7.49     |\n",
      "|    value_loss         | 100      |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 80    |\n",
      "|    iterations      | 5500  |\n",
      "|    time_elapsed    | 341   |\n",
      "|    total_timesteps | 27500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=69.28 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 69.3      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.611    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5599      |\n",
      "|    policy_loss        | -1.08     |\n",
      "|    value_loss         | 2.74      |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 80    |\n",
      "|    iterations      | 5600  |\n",
      "|    time_elapsed    | 347   |\n",
      "|    total_timesteps | 28000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=60.79 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 60.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 28500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.629   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5699     |\n",
      "|    policy_loss        | 1.67     |\n",
      "|    value_loss         | 4.53     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 80    |\n",
      "|    iterations      | 5700  |\n",
      "|    time_elapsed    | 352   |\n",
      "|    total_timesteps | 28500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=133.35 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 133      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 29000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.449   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5799     |\n",
      "|    policy_loss        | -0.0032  |\n",
      "|    value_loss         | 0.000526 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 81    |\n",
      "|    iterations      | 5800  |\n",
      "|    time_elapsed    | 357   |\n",
      "|    total_timesteps | 29000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=249.65 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 250      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 29500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.632   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5899     |\n",
      "|    policy_loss        | -0.236   |\n",
      "|    value_loss         | 0.207    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 81    |\n",
      "|    iterations      | 5900  |\n",
      "|    time_elapsed    | 363   |\n",
      "|    total_timesteps | 29500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=144.32 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 144      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 30000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.487   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5999     |\n",
      "|    policy_loss        | 6.72     |\n",
      "|    value_loss         | 330      |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 81    |\n",
      "|    iterations      | 6000  |\n",
      "|    time_elapsed    | 368   |\n",
      "|    total_timesteps | 30000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=36.35 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 36.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 30500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.623   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6099     |\n",
      "|    policy_loss        | 1.28     |\n",
      "|    value_loss         | 3.38     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 81    |\n",
      "|    iterations      | 6100  |\n",
      "|    time_elapsed    | 374   |\n",
      "|    total_timesteps | 30500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=12.42 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 12.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 31000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.59    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6199     |\n",
      "|    policy_loss        | 0.0175   |\n",
      "|    value_loss         | 0.00749  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 81    |\n",
      "|    iterations      | 6200  |\n",
      "|    time_elapsed    | 379   |\n",
      "|    total_timesteps | 31000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=15.73 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 15.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 31500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.573   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6299     |\n",
      "|    policy_loss        | 0.476    |\n",
      "|    value_loss         | 4.5      |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 81    |\n",
      "|    iterations      | 6300  |\n",
      "|    time_elapsed    | 385   |\n",
      "|    total_timesteps | 31500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=219.80 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 220      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 32000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.599   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6399     |\n",
      "|    policy_loss        | 1.08     |\n",
      "|    value_loss         | 3.04     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 81    |\n",
      "|    iterations      | 6400  |\n",
      "|    time_elapsed    | 390   |\n",
      "|    total_timesteps | 32000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=198.71 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 199       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 32500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.603    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6499      |\n",
      "|    policy_loss        | -0.589    |\n",
      "|    value_loss         | 0.978     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 82    |\n",
      "|    iterations      | 6500  |\n",
      "|    time_elapsed    | 396   |\n",
      "|    total_timesteps | 32500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=247.88 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 248      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 33000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.504   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6599     |\n",
      "|    policy_loss        | 0.389    |\n",
      "|    value_loss         | 2.41     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 82    |\n",
      "|    iterations      | 6600  |\n",
      "|    time_elapsed    | 401   |\n",
      "|    total_timesteps | 33000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=33500, episode_reward=44.03 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 44        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 33500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.591    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6699      |\n",
      "|    policy_loss        | 0.22      |\n",
      "|    value_loss         | 0.245     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 82    |\n",
      "|    iterations      | 6700  |\n",
      "|    time_elapsed    | 407   |\n",
      "|    total_timesteps | 33500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=37.44 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 37.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 34000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.584   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6799     |\n",
      "|    policy_loss        | -0.708   |\n",
      "|    value_loss         | 2.01     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 82    |\n",
      "|    iterations      | 6800  |\n",
      "|    time_elapsed    | 413   |\n",
      "|    total_timesteps | 34000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=34500, episode_reward=38.35 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 38.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 34500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.603   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6899     |\n",
      "|    policy_loss        | 0.692    |\n",
      "|    value_loss         | 1.98     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 82    |\n",
      "|    iterations      | 6900  |\n",
      "|    time_elapsed    | 418   |\n",
      "|    total_timesteps | 34500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=247.70 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 248      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 35000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.573   |\n",
      "|    explained_variance | 1.43e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6999     |\n",
      "|    policy_loss        | 0.811    |\n",
      "|    value_loss         | 3.83     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 82    |\n",
      "|    iterations      | 7000  |\n",
      "|    time_elapsed    | 424   |\n",
      "|    total_timesteps | 35000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=35500, episode_reward=214.84 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 215      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 35500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.58    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7099     |\n",
      "|    policy_loss        | -0.753   |\n",
      "|    value_loss         | 2.01     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 82    |\n",
      "|    iterations      | 7100  |\n",
      "|    time_elapsed    | 430   |\n",
      "|    total_timesteps | 35500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=63.19 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 63.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 36000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.602   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7199     |\n",
      "|    policy_loss        | -0.166   |\n",
      "|    value_loss         | 0.0852   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 82    |\n",
      "|    iterations      | 7200  |\n",
      "|    time_elapsed    | 438   |\n",
      "|    total_timesteps | 36000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=36500, episode_reward=1.85 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 1.85      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 36500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.618    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7299      |\n",
      "|    policy_loss        | -0.277    |\n",
      "|    value_loss         | 0.494     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 82    |\n",
      "|    iterations      | 7300  |\n",
      "|    time_elapsed    | 443   |\n",
      "|    total_timesteps | 36500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=290.68 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 291      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 37000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.591   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7399     |\n",
      "|    policy_loss        | -0.588   |\n",
      "|    value_loss         | 0.639    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 82    |\n",
      "|    iterations      | 7400  |\n",
      "|    time_elapsed    | 448   |\n",
      "|    total_timesteps | 37000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=37500, episode_reward=114.63 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 115      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 37500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.538   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7499     |\n",
      "|    policy_loss        | -0.313   |\n",
      "|    value_loss         | 0.299    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 82    |\n",
      "|    iterations      | 7500  |\n",
      "|    time_elapsed    | 454   |\n",
      "|    total_timesteps | 37500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | -0.00285 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 38000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.627   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7599     |\n",
      "|    policy_loss        | -0.301   |\n",
      "|    value_loss         | 0.682    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 82    |\n",
      "|    iterations      | 7600  |\n",
      "|    time_elapsed    | 459   |\n",
      "|    total_timesteps | 38000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=38500, episode_reward=314.03 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 314      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 38500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.279   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7699     |\n",
      "|    policy_loss        | -0.00269 |\n",
      "|    value_loss         | 0.00153  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 82    |\n",
      "|    iterations      | 7700  |\n",
      "|    time_elapsed    | 464   |\n",
      "|    total_timesteps | 38500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=300.39 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 300      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 39000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.548   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7799     |\n",
      "|    policy_loss        | -0.0108  |\n",
      "|    value_loss         | 0.0011   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 82    |\n",
      "|    iterations      | 7800  |\n",
      "|    time_elapsed    | 469   |\n",
      "|    total_timesteps | 39000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=39500, episode_reward=75.65 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 75.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 39500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.562   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7899     |\n",
      "|    policy_loss        | -0.189   |\n",
      "|    value_loss         | 0.53     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 83    |\n",
      "|    iterations      | 7900  |\n",
      "|    time_elapsed    | 475   |\n",
      "|    total_timesteps | 39500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=68.75 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 68.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 40000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.551   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7999     |\n",
      "|    policy_loss        | -0.0299  |\n",
      "|    value_loss         | 0.00351  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 83    |\n",
      "|    iterations      | 8000  |\n",
      "|    time_elapsed    | 480   |\n",
      "|    total_timesteps | 40000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=40500, episode_reward=43.45 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 43.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 40500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.56    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8099     |\n",
      "|    policy_loss        | -0.0803  |\n",
      "|    value_loss         | 0.0512   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 83    |\n",
      "|    iterations      | 8100  |\n",
      "|    time_elapsed    | 485   |\n",
      "|    total_timesteps | 40500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=42.26 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 42.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 41000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.584   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8199     |\n",
      "|    policy_loss        | -0.765   |\n",
      "|    value_loss         | 2.02     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 83    |\n",
      "|    iterations      | 8200  |\n",
      "|    time_elapsed    | 490   |\n",
      "|    total_timesteps | 41000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=41500, episode_reward=104.14 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 104      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 41500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.551   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8299     |\n",
      "|    policy_loss        | 2.27     |\n",
      "|    value_loss         | 9.71     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 83    |\n",
      "|    iterations      | 8300  |\n",
      "|    time_elapsed    | 495   |\n",
      "|    total_timesteps | 41500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=47.18 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 47.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 42000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.582   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8399     |\n",
      "|    policy_loss        | -0.293   |\n",
      "|    value_loss         | 0.683    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 83    |\n",
      "|    iterations      | 8400  |\n",
      "|    time_elapsed    | 500   |\n",
      "|    total_timesteps | 42000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=42500, episode_reward=13.97 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 14       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 42500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.493   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8499     |\n",
      "|    policy_loss        | -0.194   |\n",
      "|    value_loss         | 0.687    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 84    |\n",
      "|    iterations      | 8500  |\n",
      "|    time_elapsed    | 505   |\n",
      "|    total_timesteps | 42500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=33.97 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 34       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 43000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.579   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8599     |\n",
      "|    policy_loss        | -0.502   |\n",
      "|    value_loss         | 2.87     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 84    |\n",
      "|    iterations      | 8600  |\n",
      "|    time_elapsed    | 510   |\n",
      "|    total_timesteps | 43000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=43500, episode_reward=76.23 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 76.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 43500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.609   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8699     |\n",
      "|    policy_loss        | 1.58     |\n",
      "|    value_loss         | 3.73     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 84    |\n",
      "|    iterations      | 8700  |\n",
      "|    time_elapsed    | 515   |\n",
      "|    total_timesteps | 43500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=6.74 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 6.74     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 44000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.578   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8799     |\n",
      "|    policy_loss        | -0.132   |\n",
      "|    value_loss         | 0.0552   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 84    |\n",
      "|    iterations      | 8800  |\n",
      "|    time_elapsed    | 521   |\n",
      "|    total_timesteps | 44000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=44500, episode_reward=242.82 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 243      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 44500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.549   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8899     |\n",
      "|    policy_loss        | 3.49     |\n",
      "|    value_loss         | 216      |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 84    |\n",
      "|    iterations      | 8900  |\n",
      "|    time_elapsed    | 526   |\n",
      "|    total_timesteps | 44500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=142.47 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 142      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 45000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.488   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8999     |\n",
      "|    policy_loss        | 3.42     |\n",
      "|    value_loss         | 180      |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 84    |\n",
      "|    iterations      | 9000  |\n",
      "|    time_elapsed    | 531   |\n",
      "|    total_timesteps | 45000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=45500, episode_reward=240.16 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 240      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 45500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.576   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9099     |\n",
      "|    policy_loss        | -0.0315  |\n",
      "|    value_loss         | 0.00287  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 84    |\n",
      "|    iterations      | 9100  |\n",
      "|    time_elapsed    | 537   |\n",
      "|    total_timesteps | 45500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=74.67 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 74.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 46000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.557   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9199     |\n",
      "|    policy_loss        | -0.402   |\n",
      "|    value_loss         | 1        |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 84    |\n",
      "|    iterations      | 9200  |\n",
      "|    time_elapsed    | 541   |\n",
      "|    total_timesteps | 46000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=46500, episode_reward=97.04 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 97       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 46500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.55    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9299     |\n",
      "|    policy_loss        | -0.381   |\n",
      "|    value_loss         | 0.974    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 84    |\n",
      "|    iterations      | 9300  |\n",
      "|    time_elapsed    | 547   |\n",
      "|    total_timesteps | 46500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=255.58 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 256      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 47000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.587   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9399     |\n",
      "|    policy_loss        | 0.133    |\n",
      "|    value_loss         | 0.157    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 85    |\n",
      "|    iterations      | 9400  |\n",
      "|    time_elapsed    | 552   |\n",
      "|    total_timesteps | 47000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=47500, episode_reward=230.59 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 231      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 47500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.581   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9499     |\n",
      "|    policy_loss        | 0.867    |\n",
      "|    value_loss         | 2.52     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 85    |\n",
      "|    iterations      | 9500  |\n",
      "|    time_elapsed    | 557   |\n",
      "|    total_timesteps | 47500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=68.02 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 68       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 48000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.43    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9599     |\n",
      "|    policy_loss        | -0.0511  |\n",
      "|    value_loss         | 0.00553  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 85    |\n",
      "|    iterations      | 9600  |\n",
      "|    time_elapsed    | 562   |\n",
      "|    total_timesteps | 48000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=48500, episode_reward=40.41 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 40.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 48500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.365   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9699     |\n",
      "|    policy_loss        | -0.0421  |\n",
      "|    value_loss         | 0.00395  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 85    |\n",
      "|    iterations      | 9700  |\n",
      "|    time_elapsed    | 567   |\n",
      "|    total_timesteps | 48500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=92.73 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 92.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 49000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.569   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9799     |\n",
      "|    policy_loss        | -0.455   |\n",
      "|    value_loss         | 1        |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 85    |\n",
      "|    iterations      | 9800  |\n",
      "|    time_elapsed    | 572   |\n",
      "|    total_timesteps | 49000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=49500, episode_reward=100.89 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 101      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 49500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.576   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9899     |\n",
      "|    policy_loss        | -0.501   |\n",
      "|    value_loss         | 1.14     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 85    |\n",
      "|    iterations      | 9900  |\n",
      "|    time_elapsed    | 577   |\n",
      "|    total_timesteps | 49500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=247.90 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 248      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 50000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.524   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9999     |\n",
      "|    policy_loss        | 3.05     |\n",
      "|    value_loss         | 172      |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 85    |\n",
      "|    iterations      | 10000 |\n",
      "|    time_elapsed    | 582   |\n",
      "|    total_timesteps | 50000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=50500, episode_reward=104.84 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 105      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 50500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.509   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10099    |\n",
      "|    policy_loss        | 1.02     |\n",
      "|    value_loss         | 2.29     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 86    |\n",
      "|    iterations      | 10100 |\n",
      "|    time_elapsed    | 587   |\n",
      "|    total_timesteps | 50500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=102.04 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 102      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 51000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.554   |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10199    |\n",
      "|    policy_loss        | -0.2     |\n",
      "|    value_loss         | 0.565    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 86    |\n",
      "|    iterations      | 10200 |\n",
      "|    time_elapsed    | 592   |\n",
      "|    total_timesteps | 51000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=51500, episode_reward=18.84 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 51500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.465   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10299    |\n",
      "|    policy_loss        | 1.67     |\n",
      "|    value_loss         | 4.74     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 86    |\n",
      "|    iterations      | 10300 |\n",
      "|    time_elapsed    | 597   |\n",
      "|    total_timesteps | 51500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=108.50 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 109      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 52000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.58    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10399    |\n",
      "|    policy_loss        | -0.0759  |\n",
      "|    value_loss         | 0.0226   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 86    |\n",
      "|    iterations      | 10400 |\n",
      "|    time_elapsed    | 602   |\n",
      "|    total_timesteps | 52000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=52500, episode_reward=101.79 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 102      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 52500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.584   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10499    |\n",
      "|    policy_loss        | -0.228   |\n",
      "|    value_loss         | 0.209    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 86    |\n",
      "|    iterations      | 10500 |\n",
      "|    time_elapsed    | 607   |\n",
      "|    total_timesteps | 52500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=315.51 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 316      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 53000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.141   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10599    |\n",
      "|    policy_loss        | -0.00381 |\n",
      "|    value_loss         | 0.0103   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 86    |\n",
      "|    iterations      | 10600 |\n",
      "|    time_elapsed    | 612   |\n",
      "|    total_timesteps | 53000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=53500, episode_reward=101.27 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 101      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 53500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.571   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10699    |\n",
      "|    policy_loss        | -0.139   |\n",
      "|    value_loss         | 0.0754   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 86    |\n",
      "|    iterations      | 10700 |\n",
      "|    time_elapsed    | 617   |\n",
      "|    total_timesteps | 53500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=23.95 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 23.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 54000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.377   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10799    |\n",
      "|    policy_loss        | 0.0474   |\n",
      "|    value_loss         | 0.00946  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 86    |\n",
      "|    iterations      | 10800 |\n",
      "|    time_elapsed    | 622   |\n",
      "|    total_timesteps | 54000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=54500, episode_reward=100.75 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 101      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 54500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.499   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10899    |\n",
      "|    policy_loss        | -0.0575  |\n",
      "|    value_loss         | 0.0164   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 86    |\n",
      "|    iterations      | 10900 |\n",
      "|    time_elapsed    | 627   |\n",
      "|    total_timesteps | 54500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=100.75 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 101      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 55000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.545   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10999    |\n",
      "|    policy_loss        | 0.222    |\n",
      "|    value_loss         | 0.692    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 87    |\n",
      "|    iterations      | 11000 |\n",
      "|    time_elapsed    | 632   |\n",
      "|    total_timesteps | 55000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=55500, episode_reward=288.57 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 289      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 55500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.556   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11099    |\n",
      "|    policy_loss        | -0.438   |\n",
      "|    value_loss         | 0.804    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 87    |\n",
      "|    iterations      | 11100 |\n",
      "|    time_elapsed    | 637   |\n",
      "|    total_timesteps | 55500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=102.50 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 102      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 56000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.467   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11199    |\n",
      "|    policy_loss        | -0.461   |\n",
      "|    value_loss         | 0.903    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 87    |\n",
      "|    iterations      | 11200 |\n",
      "|    time_elapsed    | 642   |\n",
      "|    total_timesteps | 56000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=56500, episode_reward=264.53 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 265      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 56500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.383   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11299    |\n",
      "|    policy_loss        | 5.86     |\n",
      "|    value_loss         | 338      |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 87    |\n",
      "|    iterations      | 11300 |\n",
      "|    time_elapsed    | 646   |\n",
      "|    total_timesteps | 56500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=96.94 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 96.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 57000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.55    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11399    |\n",
      "|    policy_loss        | -0.0772  |\n",
      "|    value_loss         | 0.0403   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 87    |\n",
      "|    iterations      | 11400 |\n",
      "|    time_elapsed    | 651   |\n",
      "|    total_timesteps | 57000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=57500, episode_reward=87.03 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 87        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 57500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.532    |\n",
      "|    explained_variance | -5.96e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11499     |\n",
      "|    policy_loss        | 1.89      |\n",
      "|    value_loss         | 4         |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 87    |\n",
      "|    iterations      | 11500 |\n",
      "|    time_elapsed    | 657   |\n",
      "|    total_timesteps | 57500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=278.14 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 278      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 58000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.543   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11599    |\n",
      "|    policy_loss        | -0.504   |\n",
      "|    value_loss         | 3.04     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 87    |\n",
      "|    iterations      | 11600 |\n",
      "|    time_elapsed    | 661   |\n",
      "|    total_timesteps | 58000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=58500, episode_reward=102.95 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 103      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 58500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.537   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11699    |\n",
      "|    policy_loss        | 0.0174   |\n",
      "|    value_loss         | 0.00642  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 87    |\n",
      "|    iterations      | 11700 |\n",
      "|    time_elapsed    | 666   |\n",
      "|    total_timesteps | 58500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=297.09 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 297      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 59000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.563   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11799    |\n",
      "|    policy_loss        | 0.412    |\n",
      "|    value_loss         | 2.41     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 87    |\n",
      "|    iterations      | 11800 |\n",
      "|    time_elapsed    | 671   |\n",
      "|    total_timesteps | 59000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=59500, episode_reward=276.91 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 277      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 59500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.557   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11899    |\n",
      "|    policy_loss        | 0.0258   |\n",
      "|    value_loss         | 0.00729  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 87    |\n",
      "|    iterations      | 11900 |\n",
      "|    time_elapsed    | 676   |\n",
      "|    total_timesteps | 59500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=244.73 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 245      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 60000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.543   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11999    |\n",
      "|    policy_loss        | 1.34     |\n",
      "|    value_loss         | 3.6      |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 88    |\n",
      "|    iterations      | 12000 |\n",
      "|    time_elapsed    | 681   |\n",
      "|    total_timesteps | 60000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=60500, episode_reward=275.99 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 276      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 60500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.554   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12099    |\n",
      "|    policy_loss        | -0.0731  |\n",
      "|    value_loss         | 0.0585   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 88    |\n",
      "|    iterations      | 12100 |\n",
      "|    time_elapsed    | 686   |\n",
      "|    total_timesteps | 60500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=230.13 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 230      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 61000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.522   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12199    |\n",
      "|    policy_loss        | -0.0443  |\n",
      "|    value_loss         | 0.00732  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 88    |\n",
      "|    iterations      | 12200 |\n",
      "|    time_elapsed    | 691   |\n",
      "|    total_timesteps | 61000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=61500, episode_reward=277.16 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 277      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 61500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.574   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12299    |\n",
      "|    policy_loss        | 6.68     |\n",
      "|    value_loss         | 148      |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 88    |\n",
      "|    iterations      | 12300 |\n",
      "|    time_elapsed    | 696   |\n",
      "|    total_timesteps | 61500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=95.36 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 95.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 62000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.514    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12399     |\n",
      "|    policy_loss        | -0.0337   |\n",
      "|    value_loss         | 0.00939   |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 88    |\n",
      "|    iterations      | 12400 |\n",
      "|    time_elapsed    | 701   |\n",
      "|    total_timesteps | 62000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=62500, episode_reward=93.25 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 93.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 62500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.412   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12499    |\n",
      "|    policy_loss        | -0.0165  |\n",
      "|    value_loss         | 0.00674  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 88    |\n",
      "|    iterations      | 12500 |\n",
      "|    time_elapsed    | 705   |\n",
      "|    total_timesteps | 62500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=80.61 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 80.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 63000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.532    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12599     |\n",
      "|    policy_loss        | -0.215    |\n",
      "|    value_loss         | 0.63      |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 88    |\n",
      "|    iterations      | 12600 |\n",
      "|    time_elapsed    | 710   |\n",
      "|    total_timesteps | 63000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=63500, episode_reward=276.67 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 277      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 63500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.548   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12699    |\n",
      "|    policy_loss        | -0.0825  |\n",
      "|    value_loss         | 0.0283   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 88    |\n",
      "|    iterations      | 12700 |\n",
      "|    time_elapsed    | 715   |\n",
      "|    total_timesteps | 63500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=104.67 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 105      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 64000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.469   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12799    |\n",
      "|    policy_loss        | -1.31    |\n",
      "|    value_loss         | 3.1      |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 88    |\n",
      "|    iterations      | 12800 |\n",
      "|    time_elapsed    | 720   |\n",
      "|    total_timesteps | 64000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=64500, episode_reward=104.90 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 105      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 64500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.353   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12899    |\n",
      "|    policy_loss        | -0.358   |\n",
      "|    value_loss         | 0.239    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 88    |\n",
      "|    iterations      | 12900 |\n",
      "|    time_elapsed    | 725   |\n",
      "|    total_timesteps | 64500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=95.15 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 95.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 65000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.504   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12999    |\n",
      "|    policy_loss        | -0.258   |\n",
      "|    value_loss         | 0.237    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 88    |\n",
      "|    iterations      | 13000 |\n",
      "|    time_elapsed    | 730   |\n",
      "|    total_timesteps | 65000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=65500, episode_reward=19.20 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 19.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 65500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.177   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13099    |\n",
      "|    policy_loss        | -0.0317  |\n",
      "|    value_loss         | 0.678    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 89    |\n",
      "|    iterations      | 13100 |\n",
      "|    time_elapsed    | 735   |\n",
      "|    total_timesteps | 65500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=94.03 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 94       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 66000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.348   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13199    |\n",
      "|    policy_loss        | 0.0169   |\n",
      "|    value_loss         | 0.00848  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 89    |\n",
      "|    iterations      | 13200 |\n",
      "|    time_elapsed    | 740   |\n",
      "|    total_timesteps | 66000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=66500, episode_reward=96.07 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 96.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 66500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.508   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13299    |\n",
      "|    policy_loss        | -0.107   |\n",
      "|    value_loss         | 0.037    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 89    |\n",
      "|    iterations      | 13300 |\n",
      "|    time_elapsed    | 745   |\n",
      "|    total_timesteps | 66500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=247.44 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 247      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 67000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.502   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13399    |\n",
      "|    policy_loss        | 9.34     |\n",
      "|    value_loss         | 213      |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 89    |\n",
      "|    iterations      | 13400 |\n",
      "|    time_elapsed    | 749   |\n",
      "|    total_timesteps | 67000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=67500, episode_reward=302.55 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 303      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 67500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.538   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13499    |\n",
      "|    policy_loss        | 0.72     |\n",
      "|    value_loss         | 1.78     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 89    |\n",
      "|    iterations      | 13500 |\n",
      "|    time_elapsed    | 754   |\n",
      "|    total_timesteps | 67500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=97.36 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 97.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 68000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.521   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13599    |\n",
      "|    policy_loss        | -0.161   |\n",
      "|    value_loss         | 0.0984   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 89    |\n",
      "|    iterations      | 13600 |\n",
      "|    time_elapsed    | 759   |\n",
      "|    total_timesteps | 68000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=68500, episode_reward=289.83 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 290      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 68500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.538   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13699    |\n",
      "|    policy_loss        | -0.141   |\n",
      "|    value_loss         | 0.0584   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 89    |\n",
      "|    iterations      | 13700 |\n",
      "|    time_elapsed    | 764   |\n",
      "|    total_timesteps | 68500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=248.38 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 248      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 69000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.473   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13799    |\n",
      "|    policy_loss        | -0.172   |\n",
      "|    value_loss         | 0.641    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 89    |\n",
      "|    iterations      | 13800 |\n",
      "|    time_elapsed    | 769   |\n",
      "|    total_timesteps | 69000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=69500, episode_reward=311.24 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 311      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 69500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.554   |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13899    |\n",
      "|    policy_loss        | 2.72     |\n",
      "|    value_loss         | 10.7     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 89    |\n",
      "|    iterations      | 13900 |\n",
      "|    time_elapsed    | 774   |\n",
      "|    total_timesteps | 69500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=257.51 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 258      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 70000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.481   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13999    |\n",
      "|    policy_loss        | 2.82     |\n",
      "|    value_loss         | 133      |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 89    |\n",
      "|    iterations      | 14000 |\n",
      "|    time_elapsed    | 779   |\n",
      "|    total_timesteps | 70000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=70500, episode_reward=80.40 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 80.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 70500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.561   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14099    |\n",
      "|    policy_loss        | -0.38    |\n",
      "|    value_loss         | 0.663    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 89    |\n",
      "|    iterations      | 14100 |\n",
      "|    time_elapsed    | 784   |\n",
      "|    total_timesteps | 70500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=247.10 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 247      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 71000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.496   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14199    |\n",
      "|    policy_loss        | 2.86     |\n",
      "|    value_loss         | 211      |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 89    |\n",
      "|    iterations      | 14200 |\n",
      "|    time_elapsed    | 789   |\n",
      "|    total_timesteps | 71000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=71500, episode_reward=69.57 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 69.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 71500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.391   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14299    |\n",
      "|    policy_loss        | -0.016   |\n",
      "|    value_loss         | 0.00704  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 14300 |\n",
      "|    time_elapsed    | 794   |\n",
      "|    total_timesteps | 71500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=87.54 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 87.5      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 72000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.552    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14399     |\n",
      "|    policy_loss        | -0.156    |\n",
      "|    value_loss         | 0.0745    |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 14400 |\n",
      "|    time_elapsed    | 798   |\n",
      "|    total_timesteps | 72000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=72500, episode_reward=96.07 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 96.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 72500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.563   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14499    |\n",
      "|    policy_loss        | -0.136   |\n",
      "|    value_loss         | 0.0745   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 14500 |\n",
      "|    time_elapsed    | 803   |\n",
      "|    total_timesteps | 72500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=73000, episode_reward=42.40 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 42.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 73000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.398   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14599    |\n",
      "|    policy_loss        | 0.0317   |\n",
      "|    value_loss         | 0.0072   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 14600 |\n",
      "|    time_elapsed    | 809   |\n",
      "|    total_timesteps | 73000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=73500, episode_reward=45.75 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 45.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 73500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.412   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14699    |\n",
      "|    policy_loss        | -0.552   |\n",
      "|    value_loss         | 1.02     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 14700 |\n",
      "|    time_elapsed    | 814   |\n",
      "|    total_timesteps | 73500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=42.59 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 42.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 74000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.524   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14799    |\n",
      "|    policy_loss        | -0.209   |\n",
      "|    value_loss         | 0.143    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 14800 |\n",
      "|    time_elapsed    | 818   |\n",
      "|    total_timesteps | 74000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=74500, episode_reward=313.99 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 314      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 74500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.595   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14899    |\n",
      "|    policy_loss        | 1.77     |\n",
      "|    value_loss         | 15.2     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 14900 |\n",
      "|    time_elapsed    | 824   |\n",
      "|    total_timesteps | 74500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=95.73 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 95.7      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 75000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.549    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14999     |\n",
      "|    policy_loss        | 0.583     |\n",
      "|    value_loss         | 1.81      |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 15000 |\n",
      "|    time_elapsed    | 828   |\n",
      "|    total_timesteps | 75000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=75500, episode_reward=266.15 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 266       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 75500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.352    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15099     |\n",
      "|    policy_loss        | 3.76      |\n",
      "|    value_loss         | 215       |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 15100 |\n",
      "|    time_elapsed    | 833   |\n",
      "|    total_timesteps | 75500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=93.94 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 93.9      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 76000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.46     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15199     |\n",
      "|    policy_loss        | -0.185    |\n",
      "|    value_loss         | 0.0684    |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 15200 |\n",
      "|    time_elapsed    | 839   |\n",
      "|    total_timesteps | 76000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=76500, episode_reward=94.45 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 94.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 76500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.47    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15299    |\n",
      "|    policy_loss        | -0.0551  |\n",
      "|    value_loss         | 0.0214   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 15300 |\n",
      "|    time_elapsed    | 844   |\n",
      "|    total_timesteps | 76500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=77000, episode_reward=94.81 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 94.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 77000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.458   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15399    |\n",
      "|    policy_loss        | -0.296   |\n",
      "|    value_loss         | 0.365    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 15400 |\n",
      "|    time_elapsed    | 849   |\n",
      "|    total_timesteps | 77000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=77500, episode_reward=94.81 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 94.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 77500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.537   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15499    |\n",
      "|    policy_loss        | -0.154   |\n",
      "|    value_loss         | 0.0905   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 15500 |\n",
      "|    time_elapsed    | 854   |\n",
      "|    total_timesteps | 77500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=278.04 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 278      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 78000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.552   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15599    |\n",
      "|    policy_loss        | -0.247   |\n",
      "|    value_loss         | 0.655    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 15600 |\n",
      "|    time_elapsed    | 859   |\n",
      "|    total_timesteps | 78000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=78500, episode_reward=48.83 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 48.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 78500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.233   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15699    |\n",
      "|    policy_loss        | -0.00653 |\n",
      "|    value_loss         | 0.00769  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 15700 |\n",
      "|    time_elapsed    | 863   |\n",
      "|    total_timesteps | 78500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=249.79 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 250      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 79000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.567   |\n",
      "|    explained_variance | 2.03e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15799    |\n",
      "|    policy_loss        | 1.71     |\n",
      "|    value_loss         | 15.4     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 15800 |\n",
      "|    time_elapsed    | 869   |\n",
      "|    total_timesteps | 79000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=79500, episode_reward=96.45 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 96.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 79500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.589   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15899    |\n",
      "|    policy_loss        | 3.42     |\n",
      "|    value_loss         | 56.5     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 15900 |\n",
      "|    time_elapsed    | 874   |\n",
      "|    total_timesteps | 79500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=248.12 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 248      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 80000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.556   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15999    |\n",
      "|    policy_loss        | -0.0676  |\n",
      "|    value_loss         | 0.0153   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 16000 |\n",
      "|    time_elapsed    | 879   |\n",
      "|    total_timesteps | 80000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=80500, episode_reward=94.70 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 94.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 80500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.567   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16099    |\n",
      "|    policy_loss        | 0.117    |\n",
      "|    value_loss         | 0.59     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 16100 |\n",
      "|    time_elapsed    | 884   |\n",
      "|    total_timesteps | 80500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=81000, episode_reward=276.77 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 277      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 81000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.507   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16199    |\n",
      "|    policy_loss        | -0.0474  |\n",
      "|    value_loss         | 0.0539   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 16200 |\n",
      "|    time_elapsed    | 890   |\n",
      "|    total_timesteps | 81000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=81500, episode_reward=258.58 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 259      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 81500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.531   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16299    |\n",
      "|    policy_loss        | -0.2     |\n",
      "|    value_loss         | 0.079    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 16300 |\n",
      "|    time_elapsed    | 897   |\n",
      "|    total_timesteps | 81500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=295.63 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 296       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 82000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.497    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16399     |\n",
      "|    policy_loss        | -0.164    |\n",
      "|    value_loss         | 0.67      |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 16400 |\n",
      "|    time_elapsed    | 903   |\n",
      "|    total_timesteps | 82000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=82500, episode_reward=277.80 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 278      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 82500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.532   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16499    |\n",
      "|    policy_loss        | 0.281    |\n",
      "|    value_loss         | 0.611    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 16500 |\n",
      "|    time_elapsed    | 910   |\n",
      "|    total_timesteps | 82500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=83000, episode_reward=27.29 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 27.3      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 83000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.142    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16599     |\n",
      "|    policy_loss        | -0.00378  |\n",
      "|    value_loss         | 0.0103    |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 16600 |\n",
      "|    time_elapsed    | 916   |\n",
      "|    total_timesteps | 83000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=83500, episode_reward=277.95 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 278      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 83500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.503   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16699    |\n",
      "|    policy_loss        | -0.166   |\n",
      "|    value_loss         | 0.0999   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 16700 |\n",
      "|    time_elapsed    | 922   |\n",
      "|    total_timesteps | 83500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=47.78 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 47.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 84000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.472   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16799    |\n",
      "|    policy_loss        | -0.172   |\n",
      "|    value_loss         | 0.0742   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 16800 |\n",
      "|    time_elapsed    | 928   |\n",
      "|    total_timesteps | 84000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=84500, episode_reward=295.46 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 295      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 84500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.439   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16899    |\n",
      "|    policy_loss        | 7.41     |\n",
      "|    value_loss         | 164      |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 16900 |\n",
      "|    time_elapsed    | 934   |\n",
      "|    total_timesteps | 84500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=279.38 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 279      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 85000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.462   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16999    |\n",
      "|    policy_loss        | -0.177   |\n",
      "|    value_loss         | 0.706    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 17000 |\n",
      "|    time_elapsed    | 940   |\n",
      "|    total_timesteps | 85000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=85500, episode_reward=245.81 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 246      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 85500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.498   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17099    |\n",
      "|    policy_loss        | -0.697   |\n",
      "|    value_loss         | 1.19     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 17100 |\n",
      "|    time_elapsed    | 945   |\n",
      "|    total_timesteps | 85500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=279.77 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 280      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 86000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.509   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17199    |\n",
      "|    policy_loss        | 0.00616  |\n",
      "|    value_loss         | 0.00572  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 17200 |\n",
      "|    time_elapsed    | 951   |\n",
      "|    total_timesteps | 86000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=86500, episode_reward=94.98 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 95       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 86500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.504   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17299    |\n",
      "|    policy_loss        | 1.89     |\n",
      "|    value_loss         | 35       |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 17300 |\n",
      "|    time_elapsed    | 957   |\n",
      "|    total_timesteps | 86500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=87000, episode_reward=276.87 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 277      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 87000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.514   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17399    |\n",
      "|    policy_loss        | -0.728   |\n",
      "|    value_loss         | 2.14     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 17400 |\n",
      "|    time_elapsed    | 963   |\n",
      "|    total_timesteps | 87000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=87500, episode_reward=260.78 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 261      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 87500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.474   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17499    |\n",
      "|    policy_loss        | 1.87     |\n",
      "|    value_loss         | 12.1     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 17500 |\n",
      "|    time_elapsed    | 968   |\n",
      "|    total_timesteps | 87500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=94.98 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 95       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 88000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.46    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17599    |\n",
      "|    policy_loss        | -0.486   |\n",
      "|    value_loss         | 1.03     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 17600 |\n",
      "|    time_elapsed    | 974   |\n",
      "|    total_timesteps | 88000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=88500, episode_reward=44.40 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 44.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 88500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.511   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17699    |\n",
      "|    policy_loss        | -0.0904  |\n",
      "|    value_loss         | 0.0259   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 17700 |\n",
      "|    time_elapsed    | 980   |\n",
      "|    total_timesteps | 88500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=89000, episode_reward=276.91 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 277      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 89000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.474   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17799    |\n",
      "|    policy_loss        | 6.89     |\n",
      "|    value_loss         | 54       |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 17800 |\n",
      "|    time_elapsed    | 986   |\n",
      "|    total_timesteps | 89000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=89500, episode_reward=95.39 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 95.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 89500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.531   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17899    |\n",
      "|    policy_loss        | -0.505   |\n",
      "|    value_loss         | 1.12     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 17900 |\n",
      "|    time_elapsed    | 991   |\n",
      "|    total_timesteps | 89500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=42.87 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 42.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 90000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.364   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17999    |\n",
      "|    policy_loss        | -0.152   |\n",
      "|    value_loss         | 0.0852   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 18000 |\n",
      "|    time_elapsed    | 997   |\n",
      "|    total_timesteps | 90000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=90500, episode_reward=265.10 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 265      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 90500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.546   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18099    |\n",
      "|    policy_loss        | -0.327   |\n",
      "|    value_loss         | 0.35     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 18100 |\n",
      "|    time_elapsed    | 1003  |\n",
      "|    total_timesteps | 90500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=91000, episode_reward=278.20 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 278      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 91000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.538   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18199    |\n",
      "|    policy_loss        | 1.56     |\n",
      "|    value_loss         | 4.34     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 18200 |\n",
      "|    time_elapsed    | 1010  |\n",
      "|    total_timesteps | 91000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=91500, episode_reward=269.88 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 270      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 91500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.578   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18299    |\n",
      "|    policy_loss        | 1.1      |\n",
      "|    value_loss         | 3.77     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 18300 |\n",
      "|    time_elapsed    | 1015  |\n",
      "|    total_timesteps | 91500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=252.15 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 252      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 92000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.573   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18399    |\n",
      "|    policy_loss        | -0.105   |\n",
      "|    value_loss         | 0.0749   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 89    |\n",
      "|    iterations      | 18400 |\n",
      "|    time_elapsed    | 1022  |\n",
      "|    total_timesteps | 92000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=92500, episode_reward=96.69 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 96.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 92500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.55    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18499    |\n",
      "|    policy_loss        | -0.785   |\n",
      "|    value_loss         | 2.03     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 89    |\n",
      "|    iterations      | 18500 |\n",
      "|    time_elapsed    | 1027  |\n",
      "|    total_timesteps | 92500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=93000, episode_reward=70.49 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 70.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 93000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.378   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18599    |\n",
      "|    policy_loss        | -0.154   |\n",
      "|    value_loss         | 0.0881   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 89    |\n",
      "|    iterations      | 18600 |\n",
      "|    time_elapsed    | 1033  |\n",
      "|    total_timesteps | 93000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=93500, episode_reward=96.61 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 96.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 93500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.47    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18699    |\n",
      "|    policy_loss        | -0.0594  |\n",
      "|    value_loss         | 0.00843  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 89    |\n",
      "|    iterations      | 18700 |\n",
      "|    time_elapsed    | 1039  |\n",
      "|    total_timesteps | 93500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=279.54 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 280      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 94000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.541   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18799    |\n",
      "|    policy_loss        | -0.072   |\n",
      "|    value_loss         | 0.084    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 89    |\n",
      "|    iterations      | 18800 |\n",
      "|    time_elapsed    | 1044  |\n",
      "|    total_timesteps | 94000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=94500, episode_reward=281.65 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 282      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 94500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.547   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18899    |\n",
      "|    policy_loss        | 3.45     |\n",
      "|    value_loss         | 47.2     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 89    |\n",
      "|    iterations      | 18900 |\n",
      "|    time_elapsed    | 1050  |\n",
      "|    total_timesteps | 94500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=96.61 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 96.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 95000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.506   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18999    |\n",
      "|    policy_loss        | -0.0663  |\n",
      "|    value_loss         | 0.0254   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 89    |\n",
      "|    iterations      | 19000 |\n",
      "|    time_elapsed    | 1055  |\n",
      "|    total_timesteps | 95000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=95500, episode_reward=243.33 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 243      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 95500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.415   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19099    |\n",
      "|    policy_loss        | -0.0272  |\n",
      "|    value_loss         | 0.00926  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 19100 |\n",
      "|    time_elapsed    | 1060  |\n",
      "|    total_timesteps | 95500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=252.43 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 252      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 96000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.512   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19199    |\n",
      "|    policy_loss        | 3.81     |\n",
      "|    value_loss         | 45.1     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 19200 |\n",
      "|    time_elapsed    | 1065  |\n",
      "|    total_timesteps | 96000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=96500, episode_reward=251.22 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 251       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 96500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.503    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19299     |\n",
      "|    policy_loss        | 5.52      |\n",
      "|    value_loss         | 111       |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 19300 |\n",
      "|    time_elapsed    | 1070  |\n",
      "|    total_timesteps | 96500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=97000, episode_reward=97.11 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 97.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 97000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.254   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19399    |\n",
      "|    policy_loss        | -0.0038  |\n",
      "|    value_loss         | 0.00169  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 19400 |\n",
      "|    time_elapsed    | 1075  |\n",
      "|    total_timesteps | 97000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=97500, episode_reward=278.47 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 278       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 97500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.495    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19499     |\n",
      "|    policy_loss        | 0.0361    |\n",
      "|    value_loss         | 0.00896   |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 19500 |\n",
      "|    time_elapsed    | 1081  |\n",
      "|    total_timesteps | 97500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=98000, episode_reward=94.98 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 95       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 98000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.501   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19599    |\n",
      "|    policy_loss        | 0.112    |\n",
      "|    value_loss         | 0.599    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 19600 |\n",
      "|    time_elapsed    | 1086  |\n",
      "|    total_timesteps | 98000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=98500, episode_reward=238.66 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 239      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 98500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.384   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19699    |\n",
      "|    policy_loss        | 2.36     |\n",
      "|    value_loss         | 216      |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 19700 |\n",
      "|    time_elapsed    | 1090  |\n",
      "|    total_timesteps | 98500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=99000, episode_reward=260.94 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 261       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 99000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.476    |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19799     |\n",
      "|    policy_loss        | 0.337     |\n",
      "|    value_loss         | 2.36      |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 19800 |\n",
      "|    time_elapsed    | 1095  |\n",
      "|    total_timesteps | 99000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=99500, episode_reward=253.75 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 254      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 99500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.484   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19899    |\n",
      "|    policy_loss        | -0.0807  |\n",
      "|    value_loss         | 0.0149   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 90    |\n",
      "|    iterations      | 19900 |\n",
      "|    time_elapsed    | 1100  |\n",
      "|    total_timesteps | 99500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=280.24 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 280      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 100000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.485   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19999    |\n",
      "|    policy_loss        | -0.0325  |\n",
      "|    value_loss         | 0.00443  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 20000  |\n",
      "|    time_elapsed    | 1105   |\n",
      "|    total_timesteps | 100000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=100500, episode_reward=277.44 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 277       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 100500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.452    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20099     |\n",
      "|    policy_loss        | 2.32      |\n",
      "|    value_loss         | 214       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 20100  |\n",
      "|    time_elapsed    | 1110   |\n",
      "|    total_timesteps | 100500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=101000, episode_reward=261.35 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 261      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 101000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.509   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20199    |\n",
      "|    policy_loss        | -0.581   |\n",
      "|    value_loss         | 1.21     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 20200  |\n",
      "|    time_elapsed    | 1116   |\n",
      "|    total_timesteps | 101000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=101500, episode_reward=287.15 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 287      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 101500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.359   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20299    |\n",
      "|    policy_loss        | -0.0957  |\n",
      "|    value_loss         | 0.637    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 20300  |\n",
      "|    time_elapsed    | 1121   |\n",
      "|    total_timesteps | 101500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=102000, episode_reward=283.02 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 283      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 102000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.496   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20399    |\n",
      "|    policy_loss        | -0.392   |\n",
      "|    value_loss         | 0.383    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 20400  |\n",
      "|    time_elapsed    | 1126   |\n",
      "|    total_timesteps | 102000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=102500, episode_reward=275.04 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 275      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 102500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.473   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20499    |\n",
      "|    policy_loss        | -0.222   |\n",
      "|    value_loss         | 0.644    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 20500  |\n",
      "|    time_elapsed    | 1132   |\n",
      "|    total_timesteps | 102500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=103000, episode_reward=96.42 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 96.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 103000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.393   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20599    |\n",
      "|    policy_loss        | -0.16    |\n",
      "|    value_loss         | 0.0986   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 20600  |\n",
      "|    time_elapsed    | 1138   |\n",
      "|    total_timesteps | 103000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=103500, episode_reward=96.33 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 96.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 103500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.426   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20699    |\n",
      "|    policy_loss        | -0.0759  |\n",
      "|    value_loss         | 0.0297   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 20700  |\n",
      "|    time_elapsed    | 1144   |\n",
      "|    total_timesteps | 103500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=281.39 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 281      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 104000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.501   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20799    |\n",
      "|    policy_loss        | -0.0176  |\n",
      "|    value_loss         | 0.00243  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 20800  |\n",
      "|    time_elapsed    | 1150   |\n",
      "|    total_timesteps | 104000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=104500, episode_reward=57.05 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 57.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 104500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.234   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20899    |\n",
      "|    policy_loss        | -0.0045  |\n",
      "|    value_loss         | 0.00328  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 20900  |\n",
      "|    time_elapsed    | 1155   |\n",
      "|    total_timesteps | 104500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=288.99 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 289      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 105000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.566   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20999    |\n",
      "|    policy_loss        | 0.507    |\n",
      "|    value_loss         | 3.83     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 21000  |\n",
      "|    time_elapsed    | 1161   |\n",
      "|    total_timesteps | 105000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=105500, episode_reward=282.86 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 283      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 105500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.561   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21099    |\n",
      "|    policy_loss        | -0.131   |\n",
      "|    value_loss         | 0.055    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 21100  |\n",
      "|    time_elapsed    | 1166   |\n",
      "|    total_timesteps | 105500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=106000, episode_reward=96.52 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 96.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 106000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.431   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21199    |\n",
      "|    policy_loss        | -0.576   |\n",
      "|    value_loss         | 1.07     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 21200  |\n",
      "|    time_elapsed    | 1171   |\n",
      "|    total_timesteps | 106000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=106500, episode_reward=423.93 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 424      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 106500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.553   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21299    |\n",
      "|    policy_loss        | 0.0421   |\n",
      "|    value_loss         | 0.0102   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 21300  |\n",
      "|    time_elapsed    | 1176   |\n",
      "|    total_timesteps | 106500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=107000, episode_reward=254.57 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 255      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 107000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.575   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21399    |\n",
      "|    policy_loss        | -0.347   |\n",
      "|    value_loss         | 0.738    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 21400  |\n",
      "|    time_elapsed    | 1181   |\n",
      "|    total_timesteps | 107000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=107500, episode_reward=274.89 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 275      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 107500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.513   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21499    |\n",
      "|    policy_loss        | -0.406   |\n",
      "|    value_loss         | 1.07     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 21500  |\n",
      "|    time_elapsed    | 1187   |\n",
      "|    total_timesteps | 107500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=96.33 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 96.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 108000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.402   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21599    |\n",
      "|    policy_loss        | -0.182   |\n",
      "|    value_loss         | 0.133    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 21600  |\n",
      "|    time_elapsed    | 1192   |\n",
      "|    total_timesteps | 108000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=108500, episode_reward=94.35 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 94.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 108500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.283   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21699    |\n",
      "|    policy_loss        | -0.012   |\n",
      "|    value_loss         | 0.0139   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 21700  |\n",
      "|    time_elapsed    | 1197   |\n",
      "|    total_timesteps | 108500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=109000, episode_reward=275.09 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 275      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 109000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.53    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21799    |\n",
      "|    policy_loss        | 6.08     |\n",
      "|    value_loss         | 227      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 21800  |\n",
      "|    time_elapsed    | 1203   |\n",
      "|    total_timesteps | 109000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=109500, episode_reward=307.32 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 307      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 109500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.55    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21899    |\n",
      "|    policy_loss        | 7.05     |\n",
      "|    value_loss         | 77.6     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 21900  |\n",
      "|    time_elapsed    | 1207   |\n",
      "|    total_timesteps | 109500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=267.97 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 268      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 110000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.573   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21999    |\n",
      "|    policy_loss        | -0.214   |\n",
      "|    value_loss         | 0.182    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 22000  |\n",
      "|    time_elapsed    | 1212   |\n",
      "|    total_timesteps | 110000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=110500, episode_reward=96.46 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 96.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 110500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.511   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22099    |\n",
      "|    policy_loss        | -0.144   |\n",
      "|    value_loss         | 0.0804   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 22100  |\n",
      "|    time_elapsed    | 1218   |\n",
      "|    total_timesteps | 110500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=111000, episode_reward=285.71 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 286       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 111000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.548    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22199     |\n",
      "|    policy_loss        | 0.763     |\n",
      "|    value_loss         | 3.77      |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 22200  |\n",
      "|    time_elapsed    | 1223   |\n",
      "|    total_timesteps | 111000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=111500, episode_reward=279.43 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 279      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 111500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.545   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22299    |\n",
      "|    policy_loss        | -0.0189  |\n",
      "|    value_loss         | 0.0111   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 22300  |\n",
      "|    time_elapsed    | 1228   |\n",
      "|    total_timesteps | 111500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=288.99 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 289      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 112000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.464   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22399    |\n",
      "|    policy_loss        | -0.0605  |\n",
      "|    value_loss         | 0.0188   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 22400  |\n",
      "|    time_elapsed    | 1233   |\n",
      "|    total_timesteps | 112000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=112500, episode_reward=288.99 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 289      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 112500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.5     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22499    |\n",
      "|    policy_loss        | -0.535   |\n",
      "|    value_loss         | 1.12     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 22500  |\n",
      "|    time_elapsed    | 1238   |\n",
      "|    total_timesteps | 112500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=113000, episode_reward=302.37 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 302      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 113000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.497   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22599    |\n",
      "|    policy_loss        | 3.01     |\n",
      "|    value_loss         | 217      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 22600  |\n",
      "|    time_elapsed    | 1243   |\n",
      "|    total_timesteps | 113000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=113500, episode_reward=277.95 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 278      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 113500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.492   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22699    |\n",
      "|    policy_loss        | -0.091   |\n",
      "|    value_loss         | 0.0331   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 22700  |\n",
      "|    time_elapsed    | 1248   |\n",
      "|    total_timesteps | 113500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=114000, episode_reward=279.08 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 279      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 114000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.491   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22799    |\n",
      "|    policy_loss        | -0.592   |\n",
      "|    value_loss         | 0.929    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 22800  |\n",
      "|    time_elapsed    | 1254   |\n",
      "|    total_timesteps | 114000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=114500, episode_reward=47.43 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 47.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 114500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.489   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22899    |\n",
      "|    policy_loss        | -0.0515  |\n",
      "|    value_loss         | 0.0234   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 22900  |\n",
      "|    time_elapsed    | 1259   |\n",
      "|    total_timesteps | 114500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=56.95 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 57       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 115000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.442   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22999    |\n",
      "|    policy_loss        | -0.0722  |\n",
      "|    value_loss         | 0.0276   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 23000  |\n",
      "|    time_elapsed    | 1264   |\n",
      "|    total_timesteps | 115000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=115500, episode_reward=275.44 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 275      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 115500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.56    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23099    |\n",
      "|    policy_loss        | -0.692   |\n",
      "|    value_loss         | 1.95     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 23100  |\n",
      "|    time_elapsed    | 1269   |\n",
      "|    total_timesteps | 115500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=72.24 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 72.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 116000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.498   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23199    |\n",
      "|    policy_loss        | -0.202   |\n",
      "|    value_loss         | 0.137    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 23200  |\n",
      "|    time_elapsed    | 1274   |\n",
      "|    total_timesteps | 116000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=116500, episode_reward=54.75 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 54.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 116500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.424   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23299    |\n",
      "|    policy_loss        | -1.56    |\n",
      "|    value_loss         | 2.74     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 23300  |\n",
      "|    time_elapsed    | 1279   |\n",
      "|    total_timesteps | 116500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=117000, episode_reward=277.32 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 277       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 117000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.58     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23399     |\n",
      "|    policy_loss        | 2.07      |\n",
      "|    value_loss         | 4.46      |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 23400  |\n",
      "|    time_elapsed    | 1284   |\n",
      "|    total_timesteps | 117000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=117500, episode_reward=89.99 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 90       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 117500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.515   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23499    |\n",
      "|    policy_loss        | 1.62     |\n",
      "|    value_loss         | 3.15     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 23500  |\n",
      "|    time_elapsed    | 1289   |\n",
      "|    total_timesteps | 117500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=118000, episode_reward=277.01 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 277      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 118000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.554   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23599    |\n",
      "|    policy_loss        | -0.765   |\n",
      "|    value_loss         | 1.98     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 23600  |\n",
      "|    time_elapsed    | 1294   |\n",
      "|    total_timesteps | 118000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=118500, episode_reward=58.04 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 58       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 118500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.284   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23699    |\n",
      "|    policy_loss        | -0.0144  |\n",
      "|    value_loss         | 0.0186   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 23700  |\n",
      "|    time_elapsed    | 1299   |\n",
      "|    total_timesteps | 118500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=119000, episode_reward=46.94 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 46.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 119000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.356   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23799    |\n",
      "|    policy_loss        | -0.0963  |\n",
      "|    value_loss         | 0.0432   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 23800  |\n",
      "|    time_elapsed    | 1304   |\n",
      "|    total_timesteps | 119000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=119500, episode_reward=89.71 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 89.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 119500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.462   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23899    |\n",
      "|    policy_loss        | -0.0704  |\n",
      "|    value_loss         | 0.025    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 23900  |\n",
      "|    time_elapsed    | 1309   |\n",
      "|    total_timesteps | 119500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=280.46 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 280      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 120000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.588   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23999    |\n",
      "|    policy_loss        | -0.0765  |\n",
      "|    value_loss         | 0.0187   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 24000  |\n",
      "|    time_elapsed    | 1314   |\n",
      "|    total_timesteps | 120000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=120500, episode_reward=259.96 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 260      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 120500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.392   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24099    |\n",
      "|    policy_loss        | 10.3     |\n",
      "|    value_loss         | 62.4     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 24100  |\n",
      "|    time_elapsed    | 1319   |\n",
      "|    total_timesteps | 120500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=121000, episode_reward=97.27 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 97.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 121000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.44    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24199    |\n",
      "|    policy_loss        | -0.00196 |\n",
      "|    value_loss         | 0.000219 |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 24200  |\n",
      "|    time_elapsed    | 1324   |\n",
      "|    total_timesteps | 121000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=121500, episode_reward=97.27 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 97.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 121500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.379   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24299    |\n",
      "|    policy_loss        | -0.0576  |\n",
      "|    value_loss         | 0.0162   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 24300  |\n",
      "|    time_elapsed    | 1329   |\n",
      "|    total_timesteps | 121500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=122000, episode_reward=294.33 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 294      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 122000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.576   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24399    |\n",
      "|    policy_loss        | 1.55     |\n",
      "|    value_loss         | 15.8     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 24400  |\n",
      "|    time_elapsed    | 1334   |\n",
      "|    total_timesteps | 122000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=122500, episode_reward=56.58 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 56.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 122500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.381   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24499    |\n",
      "|    policy_loss        | -0.068   |\n",
      "|    value_loss         | 0.0226   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 24500  |\n",
      "|    time_elapsed    | 1338   |\n",
      "|    total_timesteps | 122500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=123000, episode_reward=280.03 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 280      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 123000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.55    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24599    |\n",
      "|    policy_loss        | -0.179   |\n",
      "|    value_loss         | 0.604    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 24600  |\n",
      "|    time_elapsed    | 1343   |\n",
      "|    total_timesteps | 123000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=123500, episode_reward=274.82 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 275       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 123500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.615    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 24699     |\n",
      "|    policy_loss        | -0.398    |\n",
      "|    value_loss         | 0.84      |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 24700  |\n",
      "|    time_elapsed    | 1348   |\n",
      "|    total_timesteps | 123500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=276.73 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 277      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 124000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.402   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24799    |\n",
      "|    policy_loss        | -0.00433 |\n",
      "|    value_loss         | 0.000733 |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 24800  |\n",
      "|    time_elapsed    | 1353   |\n",
      "|    total_timesteps | 124000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=124500, episode_reward=273.19 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 273      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 124500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.534   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24899    |\n",
      "|    policy_loss        | -0.048   |\n",
      "|    value_loss         | 0.0309   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 24900  |\n",
      "|    time_elapsed    | 1358   |\n",
      "|    total_timesteps | 124500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=268.86 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 269      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 125000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.279   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24999    |\n",
      "|    policy_loss        | -0.00297 |\n",
      "|    value_loss         | 0.00103  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 25000  |\n",
      "|    time_elapsed    | 1363   |\n",
      "|    total_timesteps | 125000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=125500, episode_reward=270.72 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 271       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 125500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.523    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 25099     |\n",
      "|    policy_loss        | -0.166    |\n",
      "|    value_loss         | 0.0892    |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 25100  |\n",
      "|    time_elapsed    | 1368   |\n",
      "|    total_timesteps | 125500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=126000, episode_reward=292.98 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 293      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 126000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.39    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25199    |\n",
      "|    policy_loss        | 3.02     |\n",
      "|    value_loss         | 236      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 25200  |\n",
      "|    time_elapsed    | 1373   |\n",
      "|    total_timesteps | 126000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=126500, episode_reward=57.97 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 58       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 126500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.414   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25299    |\n",
      "|    policy_loss        | 0.0894   |\n",
      "|    value_loss         | 0.694    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 25300  |\n",
      "|    time_elapsed    | 1378   |\n",
      "|    total_timesteps | 126500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=127000, episode_reward=268.12 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 268      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 127000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.447   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25399    |\n",
      "|    policy_loss        | 1.06     |\n",
      "|    value_loss         | 3.78     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 25400  |\n",
      "|    time_elapsed    | 1383   |\n",
      "|    total_timesteps | 127000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=127500, episode_reward=288.99 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 289       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 127500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.423    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 25499     |\n",
      "|    policy_loss        | -0.14     |\n",
      "|    value_loss         | 0.59      |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 25500  |\n",
      "|    time_elapsed    | 1387   |\n",
      "|    total_timesteps | 127500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=284.37 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 284      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 128000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.506   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25599    |\n",
      "|    policy_loss        | -0.689   |\n",
      "|    value_loss         | 1.91     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 25600  |\n",
      "|    time_elapsed    | 1392   |\n",
      "|    total_timesteps | 128000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=128500, episode_reward=281.27 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 281      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 128500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.499   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25699    |\n",
      "|    policy_loss        | 0.593    |\n",
      "|    value_loss         | 1.83     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 25700  |\n",
      "|    time_elapsed    | 1397   |\n",
      "|    total_timesteps | 128500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=129000, episode_reward=303.41 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 303       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 129000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.47     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 25799     |\n",
      "|    policy_loss        | 2.47      |\n",
      "|    value_loss         | 217       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 25800  |\n",
      "|    time_elapsed    | 1403   |\n",
      "|    total_timesteps | 129000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=129500, episode_reward=72.65 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 72.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 129500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.46    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25899    |\n",
      "|    policy_loss        | -0.257   |\n",
      "|    value_loss         | 0.825    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 25900  |\n",
      "|    time_elapsed    | 1409   |\n",
      "|    total_timesteps | 129500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=288.90 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 289       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 130000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.464    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 25999     |\n",
      "|    policy_loss        | 2.65      |\n",
      "|    value_loss         | 216       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 26000  |\n",
      "|    time_elapsed    | 1415   |\n",
      "|    total_timesteps | 130000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=130500, episode_reward=267.08 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 267       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 130500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.481    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 26099     |\n",
      "|    policy_loss        | 8.23      |\n",
      "|    value_loss         | 147       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 26100  |\n",
      "|    time_elapsed    | 1421   |\n",
      "|    total_timesteps | 130500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=131000, episode_reward=286.88 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 287       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 131000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.49     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 26199     |\n",
      "|    policy_loss        | -0.168    |\n",
      "|    value_loss         | 0.109     |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 26200  |\n",
      "|    time_elapsed    | 1427   |\n",
      "|    total_timesteps | 131000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=131500, episode_reward=107.19 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 107      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 131500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.264   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26299    |\n",
      "|    policy_loss        | 0.0368   |\n",
      "|    value_loss         | 0.00728  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 26300  |\n",
      "|    time_elapsed    | 1433   |\n",
      "|    total_timesteps | 131500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=280.12 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 280      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 132000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.512   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26399    |\n",
      "|    policy_loss        | 1.72     |\n",
      "|    value_loss         | 4.4      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 26400  |\n",
      "|    time_elapsed    | 1439   |\n",
      "|    total_timesteps | 132000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=132500, episode_reward=239.40 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 239      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 132500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.528   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26499    |\n",
      "|    policy_loss        | -0.176   |\n",
      "|    value_loss         | 0.145    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 26500  |\n",
      "|    time_elapsed    | 1444   |\n",
      "|    total_timesteps | 132500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=133000, episode_reward=300.30 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 300      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 133000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.481   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26599    |\n",
      "|    policy_loss        | 6.89     |\n",
      "|    value_loss         | 344      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 26600  |\n",
      "|    time_elapsed    | 1449   |\n",
      "|    total_timesteps | 133000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=133500, episode_reward=296.13 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 296      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 133500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.552   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26699    |\n",
      "|    policy_loss        | -0.353   |\n",
      "|    value_loss         | 0.617    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 26700  |\n",
      "|    time_elapsed    | 1454   |\n",
      "|    total_timesteps | 133500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=134000, episode_reward=283.77 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 284      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 134000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.562   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26799    |\n",
      "|    policy_loss        | -0.685   |\n",
      "|    value_loss         | 1.92     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 26800  |\n",
      "|    time_elapsed    | 1459   |\n",
      "|    total_timesteps | 134000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=134500, episode_reward=303.80 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 304      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 134500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.515   |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26899    |\n",
      "|    policy_loss        | 3.07     |\n",
      "|    value_loss         | 216      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 26900  |\n",
      "|    time_elapsed    | 1465   |\n",
      "|    total_timesteps | 134500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=269.14 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 269      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 135000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.588   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26999    |\n",
      "|    policy_loss        | 3.51     |\n",
      "|    value_loss         | 134      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 27000  |\n",
      "|    time_elapsed    | 1470   |\n",
      "|    total_timesteps | 135000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=135500, episode_reward=276.94 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 277      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 135500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.559   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27099    |\n",
      "|    policy_loss        | -0.339   |\n",
      "|    value_loss         | 0.382    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 27100  |\n",
      "|    time_elapsed    | 1476   |\n",
      "|    total_timesteps | 135500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=289.89 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 290      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 136000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.486   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27199    |\n",
      "|    policy_loss        | -0.216   |\n",
      "|    value_loss         | 0.175    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 27200  |\n",
      "|    time_elapsed    | 1481   |\n",
      "|    total_timesteps | 136000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=136500, episode_reward=270.66 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 271      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 136500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.483   |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27299    |\n",
      "|    policy_loss        | 2.61     |\n",
      "|    value_loss         | 217      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 27300  |\n",
      "|    time_elapsed    | 1487   |\n",
      "|    total_timesteps | 136500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=137000, episode_reward=280.55 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 281      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 137000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.537   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27399    |\n",
      "|    policy_loss        | -0.00407 |\n",
      "|    value_loss         | 0.000452 |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 27400  |\n",
      "|    time_elapsed    | 1492   |\n",
      "|    total_timesteps | 137000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=137500, episode_reward=289.28 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 289      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 137500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.535   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27499    |\n",
      "|    policy_loss        | 0.0207   |\n",
      "|    value_loss         | 0.00674  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 27500  |\n",
      "|    time_elapsed    | 1497   |\n",
      "|    total_timesteps | 137500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=138000, episode_reward=276.36 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 276      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 138000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.559   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27599    |\n",
      "|    policy_loss        | -0.373   |\n",
      "|    value_loss         | 0.799    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 27600  |\n",
      "|    time_elapsed    | 1502   |\n",
      "|    total_timesteps | 138000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=138500, episode_reward=99.34 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 99.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 138500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.535   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27699    |\n",
      "|    policy_loss        | 0.0701   |\n",
      "|    value_loss         | 0.0221   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 27700  |\n",
      "|    time_elapsed    | 1507   |\n",
      "|    total_timesteps | 138500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=139000, episode_reward=81.48 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 81.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 139000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.343   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27799    |\n",
      "|    policy_loss        | -0.115   |\n",
      "|    value_loss         | 0.0474   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 27800  |\n",
      "|    time_elapsed    | 1512   |\n",
      "|    total_timesteps | 139000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=139500, episode_reward=99.34 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 99.3      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 139500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.54     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 27899     |\n",
      "|    policy_loss        | -0.0388   |\n",
      "|    value_loss         | 0.00497   |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 27900  |\n",
      "|    time_elapsed    | 1517   |\n",
      "|    total_timesteps | 139500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=269.66 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 270       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 140000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.555    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 27999     |\n",
      "|    policy_loss        | -0.136    |\n",
      "|    value_loss         | 0.072     |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 28000  |\n",
      "|    time_elapsed    | 1522   |\n",
      "|    total_timesteps | 140000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=140500, episode_reward=94.50 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 94.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 140500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.359   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28099    |\n",
      "|    policy_loss        | -0.0886  |\n",
      "|    value_loss         | 0.61     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 28100  |\n",
      "|    time_elapsed    | 1527   |\n",
      "|    total_timesteps | 140500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=141000, episode_reward=275.35 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 275      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 141000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.573   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28199    |\n",
      "|    policy_loss        | -0.0476  |\n",
      "|    value_loss         | 0.0049   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 92     |\n",
      "|    iterations      | 28200  |\n",
      "|    time_elapsed    | 1532   |\n",
      "|    total_timesteps | 141000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=141500, episode_reward=277.18 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 277       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 141500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.518    |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 28299     |\n",
      "|    policy_loss        | 6.62      |\n",
      "|    value_loss         | 165       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 92     |\n",
      "|    iterations      | 28300  |\n",
      "|    time_elapsed    | 1537   |\n",
      "|    total_timesteps | 141500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=142000, episode_reward=99.25 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 99.3      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 142000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.473    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 28399     |\n",
      "|    policy_loss        | -0.146    |\n",
      "|    value_loss         | 0.0929    |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 92     |\n",
      "|    iterations      | 28400  |\n",
      "|    time_elapsed    | 1542   |\n",
      "|    total_timesteps | 142000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=142500, episode_reward=272.40 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 272      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 142500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.568   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28499    |\n",
      "|    policy_loss        | -0.0218  |\n",
      "|    value_loss         | 0.00672  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 92     |\n",
      "|    iterations      | 28500  |\n",
      "|    time_elapsed    | 1547   |\n",
      "|    total_timesteps | 142500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=143000, episode_reward=57.84 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 57.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 143000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.323   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28599    |\n",
      "|    policy_loss        | -0.0193  |\n",
      "|    value_loss         | 0.0705   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 92     |\n",
      "|    iterations      | 28600  |\n",
      "|    time_elapsed    | 1552   |\n",
      "|    total_timesteps | 143000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=143500, episode_reward=54.48 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 54.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 143500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.293   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28699    |\n",
      "|    policy_loss        | -0.0498  |\n",
      "|    value_loss         | 0.567    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 92     |\n",
      "|    iterations      | 28700  |\n",
      "|    time_elapsed    | 1558   |\n",
      "|    total_timesteps | 143500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=284.73 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 285      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 144000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.635   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28799    |\n",
      "|    policy_loss        | 0.0132   |\n",
      "|    value_loss         | 0.00654  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 92     |\n",
      "|    iterations      | 28800  |\n",
      "|    time_elapsed    | 1564   |\n",
      "|    total_timesteps | 144000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=144500, episode_reward=54.47 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 54.5      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 144500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.471    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 28899     |\n",
      "|    policy_loss        | -0.000506 |\n",
      "|    value_loss         | 0.000488  |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 92     |\n",
      "|    iterations      | 28900  |\n",
      "|    time_elapsed    | 1569   |\n",
      "|    total_timesteps | 144500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=276.29 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 276      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 145000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.617   |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28999    |\n",
      "|    policy_loss        | -0.306   |\n",
      "|    value_loss         | 0.635    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 92     |\n",
      "|    iterations      | 29000  |\n",
      "|    time_elapsed    | 1575   |\n",
      "|    total_timesteps | 145000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=145500, episode_reward=54.05 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 54.1      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 145500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.511    |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 29099     |\n",
      "|    policy_loss        | -0.441    |\n",
      "|    value_loss         | 0.848     |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 92     |\n",
      "|    iterations      | 29100  |\n",
      "|    time_elapsed    | 1580   |\n",
      "|    total_timesteps | 145500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=146000, episode_reward=275.85 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 276      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 146000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.646   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29199    |\n",
      "|    policy_loss        | -0.0847  |\n",
      "|    value_loss         | 0.0366   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 92     |\n",
      "|    iterations      | 29200  |\n",
      "|    time_elapsed    | 1586   |\n",
      "|    total_timesteps | 146000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=146500, episode_reward=272.12 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 272      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 146500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.65    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29299    |\n",
      "|    policy_loss        | 0.81     |\n",
      "|    value_loss         | 3.96     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 29300  |\n",
      "|    time_elapsed    | 1592   |\n",
      "|    total_timesteps | 146500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=147000, episode_reward=287.52 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 288      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 147000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.638   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29399    |\n",
      "|    policy_loss        | 0.289    |\n",
      "|    value_loss         | 0.156    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 29400  |\n",
      "|    time_elapsed    | 1598   |\n",
      "|    total_timesteps | 147000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=147500, episode_reward=279.48 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 279      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 147500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.667   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29499    |\n",
      "|    policy_loss        | 1.7      |\n",
      "|    value_loss         | 5.21     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 29500  |\n",
      "|    time_elapsed    | 1603   |\n",
      "|    total_timesteps | 147500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=83.85 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 83.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 148000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.621   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29599    |\n",
      "|    policy_loss        | 0.191    |\n",
      "|    value_loss         | 0.61     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 29600  |\n",
      "|    time_elapsed    | 1609   |\n",
      "|    total_timesteps | 148000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=148500, episode_reward=268.06 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 268      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 148500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.606   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29699    |\n",
      "|    policy_loss        | -0.0857  |\n",
      "|    value_loss         | 0.0712   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 29700  |\n",
      "|    time_elapsed    | 1615   |\n",
      "|    total_timesteps | 148500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=149000, episode_reward=303.05 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 303      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 149000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.512   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29799    |\n",
      "|    policy_loss        | 6.82     |\n",
      "|    value_loss         | 320      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 29800  |\n",
      "|    time_elapsed    | 1620   |\n",
      "|    total_timesteps | 149000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=149500, episode_reward=89.29 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 89.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 149500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.595   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29899    |\n",
      "|    policy_loss        | 0.284    |\n",
      "|    value_loss         | 0.772    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 29900  |\n",
      "|    time_elapsed    | 1626   |\n",
      "|    total_timesteps | 149500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=256.72 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 257      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 150000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.458   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29999    |\n",
      "|    policy_loss        | -0.00716 |\n",
      "|    value_loss         | 0.00158  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 30000  |\n",
      "|    time_elapsed    | 1632   |\n",
      "|    total_timesteps | 150000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=150500, episode_reward=287.41 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 287      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 150500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.496   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30099    |\n",
      "|    policy_loss        | 0.016    |\n",
      "|    value_loss         | 0.000507 |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 30100  |\n",
      "|    time_elapsed    | 1638   |\n",
      "|    total_timesteps | 150500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=151000, episode_reward=277.17 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 277      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 151000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.527   |\n",
      "|    explained_variance | 2.38e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30199    |\n",
      "|    policy_loss        | 2.22     |\n",
      "|    value_loss         | 5.44     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 30200  |\n",
      "|    time_elapsed    | 1644   |\n",
      "|    total_timesteps | 151000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=151500, episode_reward=278.51 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 279      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 151500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.575   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30299    |\n",
      "|    policy_loss        | 1.08     |\n",
      "|    value_loss         | 4.45     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 30300  |\n",
      "|    time_elapsed    | 1650   |\n",
      "|    total_timesteps | 151500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=282.30 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 282       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 152000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.584    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 30399     |\n",
      "|    policy_loss        | 1.46      |\n",
      "|    value_loss         | 3.31      |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 30400  |\n",
      "|    time_elapsed    | 1656   |\n",
      "|    total_timesteps | 152000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=152500, episode_reward=284.09 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 284      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 152500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.609   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30499    |\n",
      "|    policy_loss        | 1.7      |\n",
      "|    value_loss         | 4.48     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 30500  |\n",
      "|    time_elapsed    | 1662   |\n",
      "|    total_timesteps | 152500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=153000, episode_reward=227.38 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 227      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 153000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.587   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30599    |\n",
      "|    policy_loss        | -0.427   |\n",
      "|    value_loss         | 0.931    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 30600  |\n",
      "|    time_elapsed    | 1667   |\n",
      "|    total_timesteps | 153000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=153500, episode_reward=316.41 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 316      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 153500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.644   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30699    |\n",
      "|    policy_loss        | 0.221    |\n",
      "|    value_loss         | 0.186    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 30700  |\n",
      "|    time_elapsed    | 1673   |\n",
      "|    total_timesteps | 153500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=154000, episode_reward=44.01 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 44       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 154000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.523   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30799    |\n",
      "|    policy_loss        | 0.644    |\n",
      "|    value_loss         | 1.96     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 30800  |\n",
      "|    time_elapsed    | 1679   |\n",
      "|    total_timesteps | 154000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=154500, episode_reward=286.88 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 287      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 154500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.586   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30899    |\n",
      "|    policy_loss        | -0.245   |\n",
      "|    value_loss         | 0.542    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 30900  |\n",
      "|    time_elapsed    | 1684   |\n",
      "|    total_timesteps | 154500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=294.95 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 295      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 155000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.621   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30999    |\n",
      "|    policy_loss        | -0.0301  |\n",
      "|    value_loss         | 0.00524  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 31000  |\n",
      "|    time_elapsed    | 1690   |\n",
      "|    total_timesteps | 155000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=155500, episode_reward=269.95 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 270      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 155500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.539   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31099    |\n",
      "|    policy_loss        | 0.0821   |\n",
      "|    value_loss         | 0.0143   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 31100  |\n",
      "|    time_elapsed    | 1696   |\n",
      "|    total_timesteps | 155500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=94.51 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 94.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 156000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.495   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31199    |\n",
      "|    policy_loss        | -0.388   |\n",
      "|    value_loss         | 0.479    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 31200  |\n",
      "|    time_elapsed    | 1702   |\n",
      "|    total_timesteps | 156000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=156500, episode_reward=94.11 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 94.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 156500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.353   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31299    |\n",
      "|    policy_loss        | 0.0609   |\n",
      "|    value_loss         | 0.0132   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 31300  |\n",
      "|    time_elapsed    | 1708   |\n",
      "|    total_timesteps | 156500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=157000, episode_reward=285.05 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 285      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 157000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.472   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31399    |\n",
      "|    policy_loss        | 0.011    |\n",
      "|    value_loss         | 0.00301  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 31400  |\n",
      "|    time_elapsed    | 1714   |\n",
      "|    total_timesteps | 157000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=157500, episode_reward=287.99 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 288      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 157500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.508   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31499    |\n",
      "|    policy_loss        | 0.0508   |\n",
      "|    value_loss         | 0.0103   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 31500  |\n",
      "|    time_elapsed    | 1720   |\n",
      "|    total_timesteps | 157500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=158000, episode_reward=256.78 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 257      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 158000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.573   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31599    |\n",
      "|    policy_loss        | -0.0713  |\n",
      "|    value_loss         | 0.0104   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 31600  |\n",
      "|    time_elapsed    | 1726   |\n",
      "|    total_timesteps | 158000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=158500, episode_reward=76.18 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 76.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 158500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.278    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 31699     |\n",
      "|    policy_loss        | -0.0207   |\n",
      "|    value_loss         | 0.00365   |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 31700  |\n",
      "|    time_elapsed    | 1732   |\n",
      "|    total_timesteps | 158500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=159000, episode_reward=98.52 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 98.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 159000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.21    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31799    |\n",
      "|    policy_loss        | -0.0394  |\n",
      "|    value_loss         | 0.00606  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 31800  |\n",
      "|    time_elapsed    | 1739   |\n",
      "|    total_timesteps | 159000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=159500, episode_reward=256.50 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 257       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 159500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.509    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 31899     |\n",
      "|    policy_loss        | 5.22      |\n",
      "|    value_loss         | 102       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 31900  |\n",
      "|    time_elapsed    | 1746   |\n",
      "|    total_timesteps | 159500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=100.32 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 100      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 160000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.358   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31999    |\n",
      "|    policy_loss        | 0.00382  |\n",
      "|    value_loss         | 0.00171  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 32000  |\n",
      "|    time_elapsed    | 1751   |\n",
      "|    total_timesteps | 160000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=160500, episode_reward=263.61 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 264      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 160500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.548   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32099    |\n",
      "|    policy_loss        | 6.77     |\n",
      "|    value_loss         | 167      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 32100  |\n",
      "|    time_elapsed    | 1757   |\n",
      "|    total_timesteps | 160500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=161000, episode_reward=255.23 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 255      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 161000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.505   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32199    |\n",
      "|    policy_loss        | -0.409   |\n",
      "|    value_loss         | 0.763    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 32200  |\n",
      "|    time_elapsed    | 1763   |\n",
      "|    total_timesteps | 161000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=161500, episode_reward=255.41 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 255      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 161500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.419   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32299    |\n",
      "|    policy_loss        | 0.0304   |\n",
      "|    value_loss         | 0.00595  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 32300  |\n",
      "|    time_elapsed    | 1769   |\n",
      "|    total_timesteps | 161500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=162000, episode_reward=231.78 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 232      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 162000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.411   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32399    |\n",
      "|    policy_loss        | 5.32     |\n",
      "|    value_loss         | 219      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 32400  |\n",
      "|    time_elapsed    | 1775   |\n",
      "|    total_timesteps | 162000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=162500, episode_reward=278.01 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 278       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 162500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.435    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 32499     |\n",
      "|    policy_loss        | 1.02      |\n",
      "|    value_loss         | 3         |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 32500  |\n",
      "|    time_elapsed    | 1782   |\n",
      "|    total_timesteps | 162500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=163000, episode_reward=278.98 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 279      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 163000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.484   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32599    |\n",
      "|    policy_loss        | 0.0169   |\n",
      "|    value_loss         | 0.0061   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 32600  |\n",
      "|    time_elapsed    | 1787   |\n",
      "|    total_timesteps | 163000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=163500, episode_reward=287.80 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 288       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 163500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.453    |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 32699     |\n",
      "|    policy_loss        | 2.29      |\n",
      "|    value_loss         | 183       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 32700  |\n",
      "|    time_elapsed    | 1793   |\n",
      "|    total_timesteps | 163500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=294.31 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 294      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 164000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.507   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32799    |\n",
      "|    policy_loss        | -0.0256  |\n",
      "|    value_loss         | 0.00933  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 32800  |\n",
      "|    time_elapsed    | 1799   |\n",
      "|    total_timesteps | 164000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=164500, episode_reward=276.77 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 277      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 164500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.562   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32899    |\n",
      "|    policy_loss        | -0.0193  |\n",
      "|    value_loss         | 0.000771 |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 32900  |\n",
      "|    time_elapsed    | 1805   |\n",
      "|    total_timesteps | 164500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=280.16 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 280      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 165000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.569   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32999    |\n",
      "|    policy_loss        | -0.459   |\n",
      "|    value_loss         | 0.991    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 33000  |\n",
      "|    time_elapsed    | 1811   |\n",
      "|    total_timesteps | 165000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=165500, episode_reward=280.27 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 280      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 165500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.523   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33099    |\n",
      "|    policy_loss        | -0.303   |\n",
      "|    value_loss         | 0.695    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 33100  |\n",
      "|    time_elapsed    | 1817   |\n",
      "|    total_timesteps | 165500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=166000, episode_reward=271.11 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 271      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 166000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.548   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33199    |\n",
      "|    policy_loss        | 0.00987  |\n",
      "|    value_loss         | 0.00471  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 91     |\n",
      "|    iterations      | 33200  |\n",
      "|    time_elapsed    | 1823   |\n",
      "|    total_timesteps | 166000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=166500, episode_reward=97.27 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 97.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 166500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.411   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33299    |\n",
      "|    policy_loss        | -0.0315  |\n",
      "|    value_loss         | 0.00657  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 33300  |\n",
      "|    time_elapsed    | 1829   |\n",
      "|    total_timesteps | 166500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=167000, episode_reward=97.27 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 97.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 167000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.403   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33399    |\n",
      "|    policy_loss        | 0.00695  |\n",
      "|    value_loss         | 0.00283  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 33400  |\n",
      "|    time_elapsed    | 1835   |\n",
      "|    total_timesteps | 167000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=167500, episode_reward=93.21 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 93.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 167500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.417   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33499    |\n",
      "|    policy_loss        | 0.0112   |\n",
      "|    value_loss         | 0.00445  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 33500  |\n",
      "|    time_elapsed    | 1841   |\n",
      "|    total_timesteps | 167500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=255.29 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 255      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 168000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.426   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33599    |\n",
      "|    policy_loss        | -0.0421  |\n",
      "|    value_loss         | 0.0572   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 33600  |\n",
      "|    time_elapsed    | 1847   |\n",
      "|    total_timesteps | 168000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=168500, episode_reward=276.32 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 276      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 168500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.477   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33699    |\n",
      "|    policy_loss        | 0.00107  |\n",
      "|    value_loss         | 0.00649  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 33700  |\n",
      "|    time_elapsed    | 1854   |\n",
      "|    total_timesteps | 168500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=169000, episode_reward=284.52 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 285      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 169000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.394   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33799    |\n",
      "|    policy_loss        | -0.143   |\n",
      "|    value_loss         | 0.664    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 33800  |\n",
      "|    time_elapsed    | 1860   |\n",
      "|    total_timesteps | 169000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=169500, episode_reward=280.22 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 280      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 169500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.459   |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33899    |\n",
      "|    policy_loss        | 3.11     |\n",
      "|    value_loss         | 53.4     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 33900  |\n",
      "|    time_elapsed    | 1866   |\n",
      "|    total_timesteps | 169500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=97.05 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 97.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 170000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.394   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33999    |\n",
      "|    policy_loss        | -0.149   |\n",
      "|    value_loss         | 0.0924   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 34000  |\n",
      "|    time_elapsed    | 1871   |\n",
      "|    total_timesteps | 170000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=170500, episode_reward=287.45 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 287      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 170500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.417   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34099    |\n",
      "|    policy_loss        | -0.103   |\n",
      "|    value_loss         | 0.0274   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 34100  |\n",
      "|    time_elapsed    | 1877   |\n",
      "|    total_timesteps | 170500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=171000, episode_reward=286.45 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 286      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 171000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.351   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34199    |\n",
      "|    policy_loss        | -0.0185  |\n",
      "|    value_loss         | 0.00998  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 34200  |\n",
      "|    time_elapsed    | 1883   |\n",
      "|    total_timesteps | 171000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=171500, episode_reward=95.86 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 95.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 171500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.35    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34299    |\n",
      "|    policy_loss        | -0.343   |\n",
      "|    value_loss         | 0.242    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 34300  |\n",
      "|    time_elapsed    | 1888   |\n",
      "|    total_timesteps | 171500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=42.15 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 42.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 172000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0407   |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 34399     |\n",
      "|    policy_loss        | -0.000598 |\n",
      "|    value_loss         | 0.00507   |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 34400  |\n",
      "|    time_elapsed    | 1894   |\n",
      "|    total_timesteps | 172000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=172500, episode_reward=84.48 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 84.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 172500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.442   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34499    |\n",
      "|    policy_loss        | -0.0685  |\n",
      "|    value_loss         | 0.0228   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 34500  |\n",
      "|    time_elapsed    | 1900   |\n",
      "|    total_timesteps | 172500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=173000, episode_reward=250.25 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 250      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 173000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.495   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34599    |\n",
      "|    policy_loss        | -0.142   |\n",
      "|    value_loss         | 0.097    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 34600  |\n",
      "|    time_elapsed    | 1905   |\n",
      "|    total_timesteps | 173000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=173500, episode_reward=255.74 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 256       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 173500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.451    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 34699     |\n",
      "|    policy_loss        | -0.0205   |\n",
      "|    value_loss         | 0.0168    |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 34700  |\n",
      "|    time_elapsed    | 1911   |\n",
      "|    total_timesteps | 173500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=174000, episode_reward=255.54 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 256      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 174000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.494   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34799    |\n",
      "|    policy_loss        | 0.0518   |\n",
      "|    value_loss         | 0.0127   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 34800  |\n",
      "|    time_elapsed    | 1917   |\n",
      "|    total_timesteps | 174000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=174500, episode_reward=252.95 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 253      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 174500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.477   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34899    |\n",
      "|    policy_loss        | -0.368   |\n",
      "|    value_loss         | 0.59     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 34900  |\n",
      "|    time_elapsed    | 1922   |\n",
      "|    total_timesteps | 174500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=175000, episode_reward=252.40 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 252      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 175000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.379   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34999    |\n",
      "|    policy_loss        | 0.00122  |\n",
      "|    value_loss         | 0.000458 |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 35000  |\n",
      "|    time_elapsed    | 1928   |\n",
      "|    total_timesteps | 175000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=175500, episode_reward=253.10 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 253      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 175500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.379   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35099    |\n",
      "|    policy_loss        | 2.49     |\n",
      "|    value_loss         | 134      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 35100  |\n",
      "|    time_elapsed    | 1934   |\n",
      "|    total_timesteps | 175500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=273.18 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 273      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 176000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.462   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35199    |\n",
      "|    policy_loss        | -0.0808  |\n",
      "|    value_loss         | 0.0458   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 35200  |\n",
      "|    time_elapsed    | 1939   |\n",
      "|    total_timesteps | 176000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=176500, episode_reward=94.41 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 94.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 176500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.35    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35299    |\n",
      "|    policy_loss        | -0.0107  |\n",
      "|    value_loss         | 0.0139   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 35300  |\n",
      "|    time_elapsed    | 1945   |\n",
      "|    total_timesteps | 176500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=177000, episode_reward=96.37 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 96.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 177000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.279    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 35399     |\n",
      "|    policy_loss        | 0.0575    |\n",
      "|    value_loss         | 0.00946   |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 35400  |\n",
      "|    time_elapsed    | 1951   |\n",
      "|    total_timesteps | 177000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=177500, episode_reward=57.94 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 57.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 177500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.174   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35499    |\n",
      "|    policy_loss        | 0.000911 |\n",
      "|    value_loss         | 0.00149  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 35500  |\n",
      "|    time_elapsed    | 1956   |\n",
      "|    total_timesteps | 177500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=178000, episode_reward=254.77 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 255       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 178000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.538    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 35599     |\n",
      "|    policy_loss        | 4.05      |\n",
      "|    value_loss         | 136       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 35600  |\n",
      "|    time_elapsed    | 1962   |\n",
      "|    total_timesteps | 178000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=178500, episode_reward=268.78 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 269      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 178500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.505   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35699    |\n",
      "|    policy_loss        | -0.0724  |\n",
      "|    value_loss         | 0.0625   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 35700  |\n",
      "|    time_elapsed    | 1968   |\n",
      "|    total_timesteps | 178500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=179000, episode_reward=259.14 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 259      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 179000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.375   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35799    |\n",
      "|    policy_loss        | 0.0267   |\n",
      "|    value_loss         | 0.0126   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 35800  |\n",
      "|    time_elapsed    | 1973   |\n",
      "|    total_timesteps | 179000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=179500, episode_reward=52.83 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 52.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 179500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.348   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35899    |\n",
      "|    policy_loss        | 0.00849  |\n",
      "|    value_loss         | 0.00497  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 35900  |\n",
      "|    time_elapsed    | 1979   |\n",
      "|    total_timesteps | 179500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=94.41 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 94.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 180000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.342   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35999    |\n",
      "|    policy_loss        | -0.00694 |\n",
      "|    value_loss         | 0.00131  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 36000  |\n",
      "|    time_elapsed    | 1985   |\n",
      "|    total_timesteps | 180000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=180500, episode_reward=269.89 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 270      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 180500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.386   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36099    |\n",
      "|    policy_loss        | 1.83     |\n",
      "|    value_loss         | 236      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 36100  |\n",
      "|    time_elapsed    | 1991   |\n",
      "|    total_timesteps | 180500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=181000, episode_reward=268.27 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 268      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 181000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.381   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36199    |\n",
      "|    policy_loss        | 1.81     |\n",
      "|    value_loss         | 215      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 36200  |\n",
      "|    time_elapsed    | 1996   |\n",
      "|    total_timesteps | 181000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=181500, episode_reward=276.17 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 276      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 181500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.47    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36299    |\n",
      "|    policy_loss        | 0.474    |\n",
      "|    value_loss         | 2.39     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 36300  |\n",
      "|    time_elapsed    | 2002   |\n",
      "|    total_timesteps | 181500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=182000, episode_reward=289.68 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 290      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 182000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.416   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36399    |\n",
      "|    policy_loss        | 10.1     |\n",
      "|    value_loss         | 346      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 36400  |\n",
      "|    time_elapsed    | 2008   |\n",
      "|    total_timesteps | 182000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=182500, episode_reward=254.34 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 254      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 182500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.523   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36499    |\n",
      "|    policy_loss        | 5.21     |\n",
      "|    value_loss         | 115      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 36500  |\n",
      "|    time_elapsed    | 2013   |\n",
      "|    total_timesteps | 182500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=183000, episode_reward=295.59 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 296      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 183000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.523   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36599    |\n",
      "|    policy_loss        | -0.17    |\n",
      "|    value_loss         | 0.125    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 36600  |\n",
      "|    time_elapsed    | 2019   |\n",
      "|    total_timesteps | 183000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=183500, episode_reward=295.26 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 295      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 183500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.526   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36699    |\n",
      "|    policy_loss        | 3.93     |\n",
      "|    value_loss         | 216      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 36700  |\n",
      "|    time_elapsed    | 2025   |\n",
      "|    total_timesteps | 183500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=96.37 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 96.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 184000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.447   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36799    |\n",
      "|    policy_loss        | 0.18     |\n",
      "|    value_loss         | 0.776    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 36800  |\n",
      "|    time_elapsed    | 2030   |\n",
      "|    total_timesteps | 184000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=184500, episode_reward=42.31 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 42.3      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 184500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.292    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 36899     |\n",
      "|    policy_loss        | -0.055    |\n",
      "|    value_loss         | 0.00937   |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 36900  |\n",
      "|    time_elapsed    | 2036   |\n",
      "|    total_timesteps | 184500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=185000, episode_reward=286.92 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 287      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 185000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.449   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36999    |\n",
      "|    policy_loss        | -0.0377  |\n",
      "|    value_loss         | 0.00762  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 37000  |\n",
      "|    time_elapsed    | 2042   |\n",
      "|    total_timesteps | 185000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=185500, episode_reward=261.44 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 261      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 185500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.482   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37099    |\n",
      "|    policy_loss        | -1.01    |\n",
      "|    value_loss         | 2.65     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 37100  |\n",
      "|    time_elapsed    | 2047   |\n",
      "|    total_timesteps | 185500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=186000, episode_reward=285.63 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 286      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 186000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.498   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37199    |\n",
      "|    policy_loss        | 6.48     |\n",
      "|    value_loss         | 168      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 37200  |\n",
      "|    time_elapsed    | 2053   |\n",
      "|    total_timesteps | 186000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=186500, episode_reward=280.51 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 281       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 186500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.46     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 37299     |\n",
      "|    policy_loss        | -0.491    |\n",
      "|    value_loss         | 0.934     |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 37300  |\n",
      "|    time_elapsed    | 2059   |\n",
      "|    total_timesteps | 186500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=187000, episode_reward=280.48 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 280      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 187000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.458   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37399    |\n",
      "|    policy_loss        | -0.482   |\n",
      "|    value_loss         | 1.09     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 37400  |\n",
      "|    time_elapsed    | 2064   |\n",
      "|    total_timesteps | 187000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=187500, episode_reward=282.06 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 282      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 187500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.407   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37499    |\n",
      "|    policy_loss        | -0.17    |\n",
      "|    value_loss         | 0.124    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 37500  |\n",
      "|    time_elapsed    | 2070   |\n",
      "|    total_timesteps | 187500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=269.71 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 270      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 188000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.307   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37599    |\n",
      "|    policy_loss        | -0.0282  |\n",
      "|    value_loss         | 0.102    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 37600  |\n",
      "|    time_elapsed    | 2076   |\n",
      "|    total_timesteps | 188000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=188500, episode_reward=281.05 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 281      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 188500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.437   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37699    |\n",
      "|    policy_loss        | -0.206   |\n",
      "|    value_loss         | 0.164    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 37700  |\n",
      "|    time_elapsed    | 2082   |\n",
      "|    total_timesteps | 188500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=189000, episode_reward=93.21 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 93.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 189000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.266    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 37799     |\n",
      "|    policy_loss        | -0.0136   |\n",
      "|    value_loss         | 0.0143    |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 37800  |\n",
      "|    time_elapsed    | 2087   |\n",
      "|    total_timesteps | 189000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=189500, episode_reward=95.31 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 95.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 189500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.127   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37899    |\n",
      "|    policy_loss        | -0.00188 |\n",
      "|    value_loss         | 0.0023   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 37900  |\n",
      "|    time_elapsed    | 2093   |\n",
      "|    total_timesteps | 189500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=253.95 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 254      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 190000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.363   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37999    |\n",
      "|    policy_loss        | -0.0198  |\n",
      "|    value_loss         | 0.00186  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 38000  |\n",
      "|    time_elapsed    | 2099   |\n",
      "|    total_timesteps | 190000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=190500, episode_reward=277.40 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 277      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 190500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.439   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38099    |\n",
      "|    policy_loss        | -0.179   |\n",
      "|    value_loss         | 0.0916   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 38100  |\n",
      "|    time_elapsed    | 2104   |\n",
      "|    total_timesteps | 190500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=191000, episode_reward=276.27 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 276      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 191000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.425   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38199    |\n",
      "|    policy_loss        | 0.352    |\n",
      "|    value_loss         | 2.43     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 38200  |\n",
      "|    time_elapsed    | 2110   |\n",
      "|    total_timesteps | 191000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=191500, episode_reward=274.34 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 274      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 191500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.466   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38299    |\n",
      "|    policy_loss        | 0.629    |\n",
      "|    value_loss         | 1.74     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 38300  |\n",
      "|    time_elapsed    | 2116   |\n",
      "|    total_timesteps | 191500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=278.04 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 278      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 192000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.412   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38399    |\n",
      "|    policy_loss        | -0.0319  |\n",
      "|    value_loss         | 0.026    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 38400  |\n",
      "|    time_elapsed    | 2121   |\n",
      "|    total_timesteps | 192000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=192500, episode_reward=277.50 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 278      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 192500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.463   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38499    |\n",
      "|    policy_loss        | -0.187   |\n",
      "|    value_loss         | 0.14     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 38500  |\n",
      "|    time_elapsed    | 2127   |\n",
      "|    total_timesteps | 192500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=193000, episode_reward=280.92 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 281      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 193000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.441   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38599    |\n",
      "|    policy_loss        | 1.35     |\n",
      "|    value_loss         | 3.79     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 38600  |\n",
      "|    time_elapsed    | 2133   |\n",
      "|    total_timesteps | 193000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=193500, episode_reward=281.44 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 281      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 193500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.499   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38699    |\n",
      "|    policy_loss        | 0.0223   |\n",
      "|    value_loss         | 0.00549  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 38700  |\n",
      "|    time_elapsed    | 2138   |\n",
      "|    total_timesteps | 193500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=194000, episode_reward=287.39 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 287      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 194000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.499   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38799    |\n",
      "|    policy_loss        | -0.199   |\n",
      "|    value_loss         | 0.157    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 38800  |\n",
      "|    time_elapsed    | 2144   |\n",
      "|    total_timesteps | 194000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=194500, episode_reward=291.07 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 291      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 194500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.404   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38899    |\n",
      "|    policy_loss        | -0.00781 |\n",
      "|    value_loss         | 0.00116  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 38900  |\n",
      "|    time_elapsed    | 2149   |\n",
      "|    total_timesteps | 194500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=195000, episode_reward=288.50 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 288      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 195000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.518   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38999    |\n",
      "|    policy_loss        | -0.0405  |\n",
      "|    value_loss         | 0.0146   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 39000  |\n",
      "|    time_elapsed    | 2155   |\n",
      "|    total_timesteps | 195000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=195500, episode_reward=286.45 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 286      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 195500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.382   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39099    |\n",
      "|    policy_loss        | -0.0163  |\n",
      "|    value_loss         | 0.00617  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 39100  |\n",
      "|    time_elapsed    | 2161   |\n",
      "|    total_timesteps | 195500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=288.50 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 288       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 196000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.502    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 39199     |\n",
      "|    policy_loss        | -0.737    |\n",
      "|    value_loss         | 1.9       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 39200  |\n",
      "|    time_elapsed    | 2167   |\n",
      "|    total_timesteps | 196000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=196500, episode_reward=279.10 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 279      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 196500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.618   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39299    |\n",
      "|    policy_loss        | -0.522   |\n",
      "|    value_loss         | 0.995    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 39300  |\n",
      "|    time_elapsed    | 2172   |\n",
      "|    total_timesteps | 196500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=197000, episode_reward=298.98 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 299      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 197000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.62    |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39399    |\n",
      "|    policy_loss        | 2.72     |\n",
      "|    value_loss         | 55.5     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 39400  |\n",
      "|    time_elapsed    | 2178   |\n",
      "|    total_timesteps | 197000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=197500, episode_reward=293.20 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 293      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 197500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.572   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39499    |\n",
      "|    policy_loss        | 7.49     |\n",
      "|    value_loss         | 152      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 39500  |\n",
      "|    time_elapsed    | 2184   |\n",
      "|    total_timesteps | 197500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=198000, episode_reward=288.91 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 289      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 198000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.445   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39599    |\n",
      "|    policy_loss        | 8.62     |\n",
      "|    value_loss         | 59.5     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 39600  |\n",
      "|    time_elapsed    | 2189   |\n",
      "|    total_timesteps | 198000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=198500, episode_reward=277.27 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 277      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 198500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.479   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39699    |\n",
      "|    policy_loss        | -0.0572  |\n",
      "|    value_loss         | 0.0294   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 39700  |\n",
      "|    time_elapsed    | 2195   |\n",
      "|    total_timesteps | 198500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=199000, episode_reward=93.51 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 93.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 199000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.449   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39799    |\n",
      "|    policy_loss        | 0.341    |\n",
      "|    value_loss         | 0.768    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 39800  |\n",
      "|    time_elapsed    | 2201   |\n",
      "|    total_timesteps | 199000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=199500, episode_reward=283.17 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 283      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 199500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.475   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39899    |\n",
      "|    policy_loss        | -0.0485  |\n",
      "|    value_loss         | 0.00868  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 39900  |\n",
      "|    time_elapsed    | 2206   |\n",
      "|    total_timesteps | 199500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=268.86 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 269      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 200000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.281   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39999    |\n",
      "|    policy_loss        | 9.4      |\n",
      "|    value_loss         | 319      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 40000  |\n",
      "|    time_elapsed    | 2212   |\n",
      "|    total_timesteps | 200000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=200500, episode_reward=285.29 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 285       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 200500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.431    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 40099     |\n",
      "|    policy_loss        | -0.449    |\n",
      "|    value_loss         | 3.07      |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 40100  |\n",
      "|    time_elapsed    | 2218   |\n",
      "|    total_timesteps | 200500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=201000, episode_reward=254.43 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 254      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 201000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.497   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40199    |\n",
      "|    policy_loss        | -0.0641  |\n",
      "|    value_loss         | 0.0388   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 40200  |\n",
      "|    time_elapsed    | 2223   |\n",
      "|    total_timesteps | 201000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=201500, episode_reward=280.11 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 280      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 201500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.438   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40299    |\n",
      "|    policy_loss        | 2.56     |\n",
      "|    value_loss         | 218      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 40300  |\n",
      "|    time_elapsed    | 2229   |\n",
      "|    total_timesteps | 201500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=202000, episode_reward=266.79 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 267      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 202000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.489   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40399    |\n",
      "|    policy_loss        | 7.12     |\n",
      "|    value_loss         | 216      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 40400  |\n",
      "|    time_elapsed    | 2235   |\n",
      "|    total_timesteps | 202000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=202500, episode_reward=265.69 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 266      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 202500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.503   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40499    |\n",
      "|    policy_loss        | -0.0189  |\n",
      "|    value_loss         | 0.00378  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 40500  |\n",
      "|    time_elapsed    | 2240   |\n",
      "|    total_timesteps | 202500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=203000, episode_reward=96.98 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 97       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 203000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.413   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40599    |\n",
      "|    policy_loss        | -0.031   |\n",
      "|    value_loss         | 0.00639  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 40600  |\n",
      "|    time_elapsed    | 2246   |\n",
      "|    total_timesteps | 203000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=203500, episode_reward=254.54 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 255      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 203500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.497   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40699    |\n",
      "|    policy_loss        | -0.0285  |\n",
      "|    value_loss         | 0.0197   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 40700  |\n",
      "|    time_elapsed    | 2252   |\n",
      "|    total_timesteps | 203500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=204000, episode_reward=96.85 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 96.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 204000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.386   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40799    |\n",
      "|    policy_loss        | -0.0301  |\n",
      "|    value_loss         | 0.00693  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 40800  |\n",
      "|    time_elapsed    | 2257   |\n",
      "|    total_timesteps | 204000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=204500, episode_reward=97.34 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 97.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 204500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.353   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40899    |\n",
      "|    policy_loss        | 0.0211   |\n",
      "|    value_loss         | 0.00747  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 40900  |\n",
      "|    time_elapsed    | 2263   |\n",
      "|    total_timesteps | 204500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=205000, episode_reward=278.71 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 279       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 205000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.534    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 40999     |\n",
      "|    policy_loss        | 1.43      |\n",
      "|    value_loss         | 9.3       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 41000  |\n",
      "|    time_elapsed    | 2269   |\n",
      "|    total_timesteps | 205000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=205500, episode_reward=292.10 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 292      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 205500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.444   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41099    |\n",
      "|    policy_loss        | 2.29     |\n",
      "|    value_loss         | 45.8     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 41100  |\n",
      "|    time_elapsed    | 2274   |\n",
      "|    total_timesteps | 205500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=206000, episode_reward=279.95 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 280      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 206000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.467   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41199    |\n",
      "|    policy_loss        | 0.0408   |\n",
      "|    value_loss         | 0.00984  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 41200  |\n",
      "|    time_elapsed    | 2280   |\n",
      "|    total_timesteps | 206000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=206500, episode_reward=87.49 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 87.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 206500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.459   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41299    |\n",
      "|    policy_loss        | 0.747    |\n",
      "|    value_loss         | 2.18     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 41300  |\n",
      "|    time_elapsed    | 2285   |\n",
      "|    total_timesteps | 206500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=207000, episode_reward=275.39 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 275      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 207000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.403   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41399    |\n",
      "|    policy_loss        | 0.0047   |\n",
      "|    value_loss         | 0.0245   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 41400  |\n",
      "|    time_elapsed    | 2291   |\n",
      "|    total_timesteps | 207000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=207500, episode_reward=267.44 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 267      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 207500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.37    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41499    |\n",
      "|    policy_loss        | 1.6      |\n",
      "|    value_loss         | 218      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 41500  |\n",
      "|    time_elapsed    | 2298   |\n",
      "|    total_timesteps | 207500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=208000, episode_reward=283.67 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 284      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 208000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.435   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41599    |\n",
      "|    policy_loss        | 6.86     |\n",
      "|    value_loss         | 166      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 41600  |\n",
      "|    time_elapsed    | 2303   |\n",
      "|    total_timesteps | 208000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=208500, episode_reward=255.32 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 255      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 208500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.437   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41699    |\n",
      "|    policy_loss        | 0.0953   |\n",
      "|    value_loss         | 0.0235   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 41700  |\n",
      "|    time_elapsed    | 2310   |\n",
      "|    total_timesteps | 208500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=209000, episode_reward=271.28 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 271      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 209000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.501   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41799    |\n",
      "|    policy_loss        | -0.03    |\n",
      "|    value_loss         | 0.00687  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 41800  |\n",
      "|    time_elapsed    | 2316   |\n",
      "|    total_timesteps | 209000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=209500, episode_reward=320.60 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 321      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 209500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.358   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41899    |\n",
      "|    policy_loss        | 11.2     |\n",
      "|    value_loss         | 125      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 41900  |\n",
      "|    time_elapsed    | 2322   |\n",
      "|    total_timesteps | 209500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=296.09 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 296      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 210000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.534   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41999    |\n",
      "|    policy_loss        | -0.495   |\n",
      "|    value_loss         | 0.924    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 42000  |\n",
      "|    time_elapsed    | 2329   |\n",
      "|    total_timesteps | 210000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=210500, episode_reward=283.79 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 284      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 210500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.466   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42099    |\n",
      "|    policy_loss        | 5.45     |\n",
      "|    value_loss         | 114      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 42100  |\n",
      "|    time_elapsed    | 2335   |\n",
      "|    total_timesteps | 210500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=211000, episode_reward=279.48 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 279       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 211000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.516    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 42199     |\n",
      "|    policy_loss        | -0.213    |\n",
      "|    value_loss         | 0.584     |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 42200  |\n",
      "|    time_elapsed    | 2342   |\n",
      "|    total_timesteps | 211000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=211500, episode_reward=312.52 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 313      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 211500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.653   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42299    |\n",
      "|    policy_loss        | 6.44     |\n",
      "|    value_loss         | 168      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 42300  |\n",
      "|    time_elapsed    | 2347   |\n",
      "|    total_timesteps | 211500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=212000, episode_reward=287.60 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 288      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 212000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.53    |\n",
      "|    explained_variance | 2.38e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42399    |\n",
      "|    policy_loss        | 6.27     |\n",
      "|    value_loss         | 55.4     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 42400  |\n",
      "|    time_elapsed    | 2353   |\n",
      "|    total_timesteps | 212000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=212500, episode_reward=280.46 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 280      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 212500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.556   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42499    |\n",
      "|    policy_loss        | 0.0163   |\n",
      "|    value_loss         | 0.00361  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 42500  |\n",
      "|    time_elapsed    | 2359   |\n",
      "|    total_timesteps | 212500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=213000, episode_reward=283.11 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 283      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 213000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.486   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42599    |\n",
      "|    policy_loss        | 0.00293  |\n",
      "|    value_loss         | 0.00256  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 42600  |\n",
      "|    time_elapsed    | 2364   |\n",
      "|    total_timesteps | 213000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=213500, episode_reward=267.73 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 268      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 213500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.384   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42699    |\n",
      "|    policy_loss        | -0.0964  |\n",
      "|    value_loss         | 0.507    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 42700  |\n",
      "|    time_elapsed    | 2370   |\n",
      "|    total_timesteps | 213500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=214000, episode_reward=276.42 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 276      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 214000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.442   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42799    |\n",
      "|    policy_loss        | 5.05     |\n",
      "|    value_loss         | 103      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 42800  |\n",
      "|    time_elapsed    | 2376   |\n",
      "|    total_timesteps | 214000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=214500, episode_reward=285.97 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 286       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 214500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.458    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 42899     |\n",
      "|    policy_loss        | 6.49      |\n",
      "|    value_loss         | 165       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 42900  |\n",
      "|    time_elapsed    | 2382   |\n",
      "|    total_timesteps | 214500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=215000, episode_reward=96.37 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 96.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 215000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.384   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42999    |\n",
      "|    policy_loss        | -0.014   |\n",
      "|    value_loss         | 0.00265  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 43000  |\n",
      "|    time_elapsed    | 2387   |\n",
      "|    total_timesteps | 215000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=215500, episode_reward=268.09 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 268      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 215500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.368   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43099    |\n",
      "|    policy_loss        | -0.322   |\n",
      "|    value_loss         | 0.292    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 43100  |\n",
      "|    time_elapsed    | 2393   |\n",
      "|    total_timesteps | 215500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=216000, episode_reward=269.15 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 269      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 216000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.362   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43199    |\n",
      "|    policy_loss        | 1.81     |\n",
      "|    value_loss         | 242      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 43200  |\n",
      "|    time_elapsed    | 2399   |\n",
      "|    total_timesteps | 216000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=216500, episode_reward=83.18 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 83.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 216500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.422   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43299    |\n",
      "|    policy_loss        | -0.877   |\n",
      "|    value_loss         | 2.2      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 43300  |\n",
      "|    time_elapsed    | 2404   |\n",
      "|    total_timesteps | 216500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=217000, episode_reward=281.12 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 281      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 217000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.44    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43399    |\n",
      "|    policy_loss        | -0.491   |\n",
      "|    value_loss         | 0.914    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 43400  |\n",
      "|    time_elapsed    | 2410   |\n",
      "|    total_timesteps | 217000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=217500, episode_reward=278.09 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 278       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 217500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.334    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 43499     |\n",
      "|    policy_loss        | -0.034    |\n",
      "|    value_loss         | 0.0641    |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 43500  |\n",
      "|    time_elapsed    | 2415   |\n",
      "|    total_timesteps | 217500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=218000, episode_reward=268.02 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 268      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 218000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.302   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43599    |\n",
      "|    policy_loss        | 0.919    |\n",
      "|    value_loss         | 3.84     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 43600  |\n",
      "|    time_elapsed    | 2421   |\n",
      "|    total_timesteps | 218000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=218500, episode_reward=281.14 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 281      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 218500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.31    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43699    |\n",
      "|    policy_loss        | 1.37     |\n",
      "|    value_loss         | 215      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 43700  |\n",
      "|    time_elapsed    | 2427   |\n",
      "|    total_timesteps | 218500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=219000, episode_reward=279.33 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 279      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 219000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.35    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43799    |\n",
      "|    policy_loss        | -0.109   |\n",
      "|    value_loss         | 0.563    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 43800  |\n",
      "|    time_elapsed    | 2432   |\n",
      "|    total_timesteps | 219000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=219500, episode_reward=266.65 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 267      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 219500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.341   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43899    |\n",
      "|    policy_loss        | 1.46     |\n",
      "|    value_loss         | 197      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 43900  |\n",
      "|    time_elapsed    | 2438   |\n",
      "|    total_timesteps | 219500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=266.47 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 266      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 220000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.313   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43999    |\n",
      "|    policy_loss        | 1.23     |\n",
      "|    value_loss         | 216      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 44000  |\n",
      "|    time_elapsed    | 2444   |\n",
      "|    total_timesteps | 220000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=220500, episode_reward=278.57 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 279      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 220500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.394   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44099    |\n",
      "|    policy_loss        | -0.196   |\n",
      "|    value_loss         | 0.696    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 90     |\n",
      "|    iterations      | 44100  |\n",
      "|    time_elapsed    | 2449   |\n",
      "|    total_timesteps | 220500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=221000, episode_reward=288.57 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 289       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 221000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.336    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 44199     |\n",
      "|    policy_loss        | 1.58      |\n",
      "|    value_loss         | 213       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 44200  |\n",
      "|    time_elapsed    | 2455   |\n",
      "|    total_timesteps | 221000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=221500, episode_reward=268.05 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 268      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 221500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.333   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44299    |\n",
      "|    policy_loss        | -0.0026  |\n",
      "|    value_loss         | 0.00247  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 44300  |\n",
      "|    time_elapsed    | 2461   |\n",
      "|    total_timesteps | 221500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=222000, episode_reward=278.88 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 279      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 222000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.442   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44399    |\n",
      "|    policy_loss        | -0.152   |\n",
      "|    value_loss         | 0.0856   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 44400  |\n",
      "|    time_elapsed    | 2466   |\n",
      "|    total_timesteps | 222000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=222500, episode_reward=281.66 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 282      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 222500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.468   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44499    |\n",
      "|    policy_loss        | -0.181   |\n",
      "|    value_loss         | 0.11     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 44500  |\n",
      "|    time_elapsed    | 2472   |\n",
      "|    total_timesteps | 222500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=223000, episode_reward=259.88 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 260      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 223000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.367   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44599    |\n",
      "|    policy_loss        | -0.269   |\n",
      "|    value_loss         | 0.16     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 44600  |\n",
      "|    time_elapsed    | 2478   |\n",
      "|    total_timesteps | 223000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=223500, episode_reward=63.81 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 63.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 223500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0155   |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 44699     |\n",
      "|    policy_loss        | -0.000238 |\n",
      "|    value_loss         | 0.008     |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 44700  |\n",
      "|    time_elapsed    | 2483   |\n",
      "|    total_timesteps | 223500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=224000, episode_reward=46.49 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 46.5      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 224000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0137   |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 44799     |\n",
      "|    policy_loss        | -0.000429 |\n",
      "|    value_loss         | 0.0434    |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 44800  |\n",
      "|    time_elapsed    | 2489   |\n",
      "|    total_timesteps | 224000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=224500, episode_reward=64.74 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 64.7      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 224500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0617   |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 44899     |\n",
      "|    policy_loss        | -0.0092   |\n",
      "|    value_loss         | 0.723     |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 44900  |\n",
      "|    time_elapsed    | 2495   |\n",
      "|    total_timesteps | 224500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=225000, episode_reward=294.35 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 294      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 225000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.495   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44999    |\n",
      "|    policy_loss        | -0.562   |\n",
      "|    value_loss         | 3.31     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 45000  |\n",
      "|    time_elapsed    | 2500   |\n",
      "|    total_timesteps | 225000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=225500, episode_reward=102.29 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 102      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 225500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.193   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45099    |\n",
      "|    policy_loss        | -0.257   |\n",
      "|    value_loss         | 0.163    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 45100  |\n",
      "|    time_elapsed    | 2506   |\n",
      "|    total_timesteps | 225500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=226000, episode_reward=91.24 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 91.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 226000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.255   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45199    |\n",
      "|    policy_loss        | -0.00944 |\n",
      "|    value_loss         | 0.00773  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 45200  |\n",
      "|    time_elapsed    | 2511   |\n",
      "|    total_timesteps | 226000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=226500, episode_reward=274.12 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 274      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 226500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.487   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45299    |\n",
      "|    policy_loss        | -0.613   |\n",
      "|    value_loss         | 1.14     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 45300  |\n",
      "|    time_elapsed    | 2517   |\n",
      "|    total_timesteps | 226500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=227000, episode_reward=77.27 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 77.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 227000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.371   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45399    |\n",
      "|    policy_loss        | -0.148   |\n",
      "|    value_loss         | 0.0874   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 45400  |\n",
      "|    time_elapsed    | 2523   |\n",
      "|    total_timesteps | 227000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=227500, episode_reward=79.83 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 79.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 227500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.375   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45499    |\n",
      "|    policy_loss        | -0.0349  |\n",
      "|    value_loss         | 0.027    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 45500  |\n",
      "|    time_elapsed    | 2528   |\n",
      "|    total_timesteps | 227500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=228000, episode_reward=76.35 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 76.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 228000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.336   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45599    |\n",
      "|    policy_loss        | -0.0666  |\n",
      "|    value_loss         | 0.0229   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 45600  |\n",
      "|    time_elapsed    | 2534   |\n",
      "|    total_timesteps | 228000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=228500, episode_reward=301.13 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 301      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 228500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.507   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45699    |\n",
      "|    policy_loss        | 5.03     |\n",
      "|    value_loss         | 53.8     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 45700  |\n",
      "|    time_elapsed    | 2540   |\n",
      "|    total_timesteps | 228500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=229000, episode_reward=280.55 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 281      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 229000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.52    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45799    |\n",
      "|    policy_loss        | -0.241   |\n",
      "|    value_loss         | 0.516    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 45800  |\n",
      "|    time_elapsed    | 2545   |\n",
      "|    total_timesteps | 229000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=229500, episode_reward=85.80 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 85.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 229500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.418   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45899    |\n",
      "|    policy_loss        | -0.187   |\n",
      "|    value_loss         | 0.108    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 45900  |\n",
      "|    time_elapsed    | 2551   |\n",
      "|    total_timesteps | 229500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=278.71 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 279       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 230000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.443    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 45999     |\n",
      "|    policy_loss        | 5.84      |\n",
      "|    value_loss         | 113       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 46000  |\n",
      "|    time_elapsed    | 2556   |\n",
      "|    total_timesteps | 230000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=230500, episode_reward=276.56 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 277       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 230500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.526    |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 46099     |\n",
      "|    policy_loss        | -0.345    |\n",
      "|    value_loss         | 0.663     |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 46100  |\n",
      "|    time_elapsed    | 2562   |\n",
      "|    total_timesteps | 230500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=231000, episode_reward=275.01 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 275       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 231000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.493    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 46199     |\n",
      "|    policy_loss        | 0.0607    |\n",
      "|    value_loss         | 0.0153    |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 46200  |\n",
      "|    time_elapsed    | 2568   |\n",
      "|    total_timesteps | 231000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=231500, episode_reward=107.94 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 108       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 231500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.424    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 46299     |\n",
      "|    policy_loss        | 0.0352    |\n",
      "|    value_loss         | 0.00749   |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 46300  |\n",
      "|    time_elapsed    | 2573   |\n",
      "|    total_timesteps | 231500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=232000, episode_reward=279.44 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 279       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 232000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.383    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 46399     |\n",
      "|    policy_loss        | 2.13      |\n",
      "|    value_loss         | 216       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 46400  |\n",
      "|    time_elapsed    | 2579   |\n",
      "|    total_timesteps | 232000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=232500, episode_reward=281.23 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 281      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 232500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.458   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46499    |\n",
      "|    policy_loss        | -0.453   |\n",
      "|    value_loss         | 1.01     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 46500  |\n",
      "|    time_elapsed    | 2585   |\n",
      "|    total_timesteps | 232500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=233000, episode_reward=291.79 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 292      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 233000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.475   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46599    |\n",
      "|    policy_loss        | 0.504    |\n",
      "|    value_loss         | 3.72     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 46600  |\n",
      "|    time_elapsed    | 2590   |\n",
      "|    total_timesteps | 233000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=233500, episode_reward=289.59 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 290      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 233500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.445   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46699    |\n",
      "|    policy_loss        | -0.183   |\n",
      "|    value_loss         | 0.619    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 46700  |\n",
      "|    time_elapsed    | 2596   |\n",
      "|    total_timesteps | 233500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=234000, episode_reward=291.15 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 291      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 234000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.517   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46799    |\n",
      "|    policy_loss        | -0.316   |\n",
      "|    value_loss         | 0.757    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 46800  |\n",
      "|    time_elapsed    | 2601   |\n",
      "|    total_timesteps | 234000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=234500, episode_reward=291.15 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 291      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 234500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.522   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46899    |\n",
      "|    policy_loss        | 5.48     |\n",
      "|    value_loss         | 102      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 46900  |\n",
      "|    time_elapsed    | 2607   |\n",
      "|    total_timesteps | 234500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=235000, episode_reward=299.76 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 300      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 235000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.571   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46999    |\n",
      "|    policy_loss        | -0.317   |\n",
      "|    value_loss         | 0.625    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 47000  |\n",
      "|    time_elapsed    | 2613   |\n",
      "|    total_timesteps | 235000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=235500, episode_reward=97.16 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 97.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 235500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.366   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47099    |\n",
      "|    policy_loss        | -0.113   |\n",
      "|    value_loss         | 0.635    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 47100  |\n",
      "|    time_elapsed    | 2618   |\n",
      "|    total_timesteps | 235500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=236000, episode_reward=87.68 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 87.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 236000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.452   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47199    |\n",
      "|    policy_loss        | -0.0089  |\n",
      "|    value_loss         | 0.00697  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 47200  |\n",
      "|    time_elapsed    | 2624   |\n",
      "|    total_timesteps | 236000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=236500, episode_reward=67.93 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 67.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 236500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.369   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47299    |\n",
      "|    policy_loss        | -0.0618  |\n",
      "|    value_loss         | 0.0209   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 47300  |\n",
      "|    time_elapsed    | 2630   |\n",
      "|    total_timesteps | 236500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=237000, episode_reward=91.36 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 91.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 237000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.469   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47399    |\n",
      "|    policy_loss        | -0.745   |\n",
      "|    value_loss         | 1.98     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 47400  |\n",
      "|    time_elapsed    | 2635   |\n",
      "|    total_timesteps | 237000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=237500, episode_reward=67.93 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 67.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 237500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.463   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47499    |\n",
      "|    policy_loss        | -0.698   |\n",
      "|    value_loss         | 2.68     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 47500  |\n",
      "|    time_elapsed    | 2641   |\n",
      "|    total_timesteps | 237500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=238000, episode_reward=287.57 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 288      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 238000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.488   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47599    |\n",
      "|    policy_loss        | -0.236   |\n",
      "|    value_loss         | 0.148    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 47600  |\n",
      "|    time_elapsed    | 2646   |\n",
      "|    total_timesteps | 238000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=238500, episode_reward=67.93 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 67.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 238500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.395   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47699    |\n",
      "|    policy_loss        | 0.0383   |\n",
      "|    value_loss         | 0.601    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 47700  |\n",
      "|    time_elapsed    | 2652   |\n",
      "|    total_timesteps | 238500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=239000, episode_reward=287.36 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 287       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 239000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.446    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 47799     |\n",
      "|    policy_loss        | 0.718     |\n",
      "|    value_loss         | 3.79      |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 47800  |\n",
      "|    time_elapsed    | 2658   |\n",
      "|    total_timesteps | 239000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=239500, episode_reward=87.13 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 87.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 239500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.438   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47899    |\n",
      "|    policy_loss        | -0.0879  |\n",
      "|    value_loss         | 0.0823   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 47900  |\n",
      "|    time_elapsed    | 2663   |\n",
      "|    total_timesteps | 239500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=70.79 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 70.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 240000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.311   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47999    |\n",
      "|    policy_loss        | 0.031    |\n",
      "|    value_loss         | 0.00496  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 48000  |\n",
      "|    time_elapsed    | 2669   |\n",
      "|    total_timesteps | 240000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=240500, episode_reward=292.34 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 292      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 240500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.474   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48099    |\n",
      "|    policy_loss        | -0.168   |\n",
      "|    value_loss         | 0.0969   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 48100  |\n",
      "|    time_elapsed    | 2674   |\n",
      "|    total_timesteps | 240500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=241000, episode_reward=96.63 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 96.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 241000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.446   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48199    |\n",
      "|    policy_loss        | 4.61     |\n",
      "|    value_loss         | 112      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 48200  |\n",
      "|    time_elapsed    | 2680   |\n",
      "|    total_timesteps | 241000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=241500, episode_reward=273.94 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 274      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 241500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.35    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48299    |\n",
      "|    policy_loss        | 9.03     |\n",
      "|    value_loss         | 109      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 48300  |\n",
      "|    time_elapsed    | 2686   |\n",
      "|    total_timesteps | 241500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=242000, episode_reward=288.06 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 288      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 242000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.386   |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48399    |\n",
      "|    policy_loss        | 8.06     |\n",
      "|    value_loss         | 150      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 48400  |\n",
      "|    time_elapsed    | 2691   |\n",
      "|    total_timesteps | 242000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=242500, episode_reward=280.98 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 281      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 242500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.419   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48499    |\n",
      "|    policy_loss        | -0.695   |\n",
      "|    value_loss         | 1.99     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 48500  |\n",
      "|    time_elapsed    | 2697   |\n",
      "|    total_timesteps | 242500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=243000, episode_reward=86.56 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 86.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 243000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.435   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48599    |\n",
      "|    policy_loss        | 0.0344   |\n",
      "|    value_loss         | 0.00749  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 48600  |\n",
      "|    time_elapsed    | 2703   |\n",
      "|    total_timesteps | 243000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=243500, episode_reward=87.46 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 87.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 243500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.419   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48699    |\n",
      "|    policy_loss        | 0.00298  |\n",
      "|    value_loss         | 0.00576  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 48700  |\n",
      "|    time_elapsed    | 2708   |\n",
      "|    total_timesteps | 243500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=244000, episode_reward=72.53 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 72.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 244000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.439   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48799    |\n",
      "|    policy_loss        | -0.273   |\n",
      "|    value_loss         | 0.281    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 48800  |\n",
      "|    time_elapsed    | 2714   |\n",
      "|    total_timesteps | 244000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=244500, episode_reward=278.53 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 279       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 244500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.403    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 48899     |\n",
      "|    policy_loss        | -0.205    |\n",
      "|    value_loss         | 0.0929    |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 48900  |\n",
      "|    time_elapsed    | 2719   |\n",
      "|    total_timesteps | 244500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=245000, episode_reward=278.99 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 279      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 245000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.415   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48999    |\n",
      "|    policy_loss        | 0.346    |\n",
      "|    value_loss         | 2.51     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 49000  |\n",
      "|    time_elapsed    | 2725   |\n",
      "|    total_timesteps | 245000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=245500, episode_reward=86.61 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 86.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 245500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.452   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49099    |\n",
      "|    policy_loss        | -0.0138  |\n",
      "|    value_loss         | 0.00129  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 49100  |\n",
      "|    time_elapsed    | 2731   |\n",
      "|    total_timesteps | 245500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=246000, episode_reward=78.89 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 78.9      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 246000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.421    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 49199     |\n",
      "|    policy_loss        | -0.0169   |\n",
      "|    value_loss         | 0.00252   |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 49200  |\n",
      "|    time_elapsed    | 2736   |\n",
      "|    total_timesteps | 246000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=246500, episode_reward=98.05 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 98.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 246500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.473   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49299    |\n",
      "|    policy_loss        | -0.0729  |\n",
      "|    value_loss         | 0.0495   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 49300  |\n",
      "|    time_elapsed    | 2742   |\n",
      "|    total_timesteps | 246500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=247000, episode_reward=68.32 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 68.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 247000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.479   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49399    |\n",
      "|    policy_loss        | 0.135    |\n",
      "|    value_loss         | 0.582    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 49400  |\n",
      "|    time_elapsed    | 2748   |\n",
      "|    total_timesteps | 247000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=247500, episode_reward=281.73 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 282      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 247500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.463   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49499    |\n",
      "|    policy_loss        | 1.75     |\n",
      "|    value_loss         | 12.4     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 49500  |\n",
      "|    time_elapsed    | 2753   |\n",
      "|    total_timesteps | 247500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=248000, episode_reward=246.58 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 247       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 248000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.471    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 49599     |\n",
      "|    policy_loss        | -0.11     |\n",
      "|    value_loss         | 0.0679    |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 49600  |\n",
      "|    time_elapsed    | 2759   |\n",
      "|    total_timesteps | 248000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=248500, episode_reward=270.61 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 271      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 248500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.421   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49699    |\n",
      "|    policy_loss        | 0.32     |\n",
      "|    value_loss         | 2.46     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 49700  |\n",
      "|    time_elapsed    | 2765   |\n",
      "|    total_timesteps | 248500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=249000, episode_reward=93.93 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 93.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 249000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.454   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49799    |\n",
      "|    policy_loss        | -0.667   |\n",
      "|    value_loss         | 3.01     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 49800  |\n",
      "|    time_elapsed    | 2770   |\n",
      "|    total_timesteps | 249000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=249500, episode_reward=262.99 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 263      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 249500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.49    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49899    |\n",
      "|    policy_loss        | -0.521   |\n",
      "|    value_loss         | 2.79     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 49900  |\n",
      "|    time_elapsed    | 2776   |\n",
      "|    total_timesteps | 249500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=37.16 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 37.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 250000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.447   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49999    |\n",
      "|    policy_loss        | -0.209   |\n",
      "|    value_loss         | 0.186    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 50000  |\n",
      "|    time_elapsed    | 2782   |\n",
      "|    total_timesteps | 250000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=250500, episode_reward=308.54 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 309       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 250500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.41     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 50099     |\n",
      "|    policy_loss        | 2.16      |\n",
      "|    value_loss         | 214       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 50100  |\n",
      "|    time_elapsed    | 2787   |\n",
      "|    total_timesteps | 250500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=251000, episode_reward=290.37 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 290      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 251000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.329   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50199    |\n",
      "|    policy_loss        | 1.12     |\n",
      "|    value_loss         | 134      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 50200  |\n",
      "|    time_elapsed    | 2793   |\n",
      "|    total_timesteps | 251000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=251500, episode_reward=287.45 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 287       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 251500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.3      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 50299     |\n",
      "|    policy_loss        | 1.59      |\n",
      "|    value_loss         | 213       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 50300  |\n",
      "|    time_elapsed    | 2799   |\n",
      "|    total_timesteps | 251500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=252000, episode_reward=68.01 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 68       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 252000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.429   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50399    |\n",
      "|    policy_loss        | -0.509   |\n",
      "|    value_loss         | 1.19     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 50400  |\n",
      "|    time_elapsed    | 2804   |\n",
      "|    total_timesteps | 252000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=252500, episode_reward=252.39 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 252      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 252500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.415   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50499    |\n",
      "|    policy_loss        | -0.218   |\n",
      "|    value_loss         | 0.62     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 50500  |\n",
      "|    time_elapsed    | 2810   |\n",
      "|    total_timesteps | 252500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=253000, episode_reward=277.28 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 277      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 253000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.437   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50599    |\n",
      "|    policy_loss        | 3.97     |\n",
      "|    value_loss         | 71.2     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 50600  |\n",
      "|    time_elapsed    | 2816   |\n",
      "|    total_timesteps | 253000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=253500, episode_reward=73.93 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 73.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 253500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.404   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50699    |\n",
      "|    policy_loss        | -0.171   |\n",
      "|    value_loss         | 0.662    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 50700  |\n",
      "|    time_elapsed    | 2821   |\n",
      "|    total_timesteps | 253500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=254000, episode_reward=290.00 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 290      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 254000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.381   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50799    |\n",
      "|    policy_loss        | -0.63    |\n",
      "|    value_loss         | 1.04     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 50800  |\n",
      "|    time_elapsed    | 2827   |\n",
      "|    total_timesteps | 254000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=254500, episode_reward=278.83 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 279      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 254500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.378   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50899    |\n",
      "|    policy_loss        | -0.225   |\n",
      "|    value_loss         | 0.0807   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 50900  |\n",
      "|    time_elapsed    | 2833   |\n",
      "|    total_timesteps | 254500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=255000, episode_reward=288.78 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 289      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 255000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.339   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50999    |\n",
      "|    policy_loss        | 1.99     |\n",
      "|    value_loss         | 214      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 51000  |\n",
      "|    time_elapsed    | 2838   |\n",
      "|    total_timesteps | 255000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=255500, episode_reward=277.42 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 277      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 255500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.263   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51099    |\n",
      "|    policy_loss        | 9.81     |\n",
      "|    value_loss         | 372      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 51100  |\n",
      "|    time_elapsed    | 2844   |\n",
      "|    total_timesteps | 255500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=256000, episode_reward=71.29 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 71.3      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 256000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.405    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 51199     |\n",
      "|    policy_loss        | -0.000762 |\n",
      "|    value_loss         | 0.00507   |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 51200  |\n",
      "|    time_elapsed    | 2850   |\n",
      "|    total_timesteps | 256000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=256500, episode_reward=287.79 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 288      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 256500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.342   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51299    |\n",
      "|    policy_loss        | -0.259   |\n",
      "|    value_loss         | 0.223    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 51300  |\n",
      "|    time_elapsed    | 2855   |\n",
      "|    total_timesteps | 256500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=257000, episode_reward=297.61 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 298      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 257000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.268   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51399    |\n",
      "|    policy_loss        | 8        |\n",
      "|    value_loss         | 54.2     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 51400  |\n",
      "|    time_elapsed    | 2861   |\n",
      "|    total_timesteps | 257000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=257500, episode_reward=291.78 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 292      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 257500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.377   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51499    |\n",
      "|    policy_loss        | 0.619    |\n",
      "|    value_loss         | 1.42     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 51500  |\n",
      "|    time_elapsed    | 2867   |\n",
      "|    total_timesteps | 257500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=258000, episode_reward=68.13 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 68.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 258000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.409   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51599    |\n",
      "|    policy_loss        | -0.165   |\n",
      "|    value_loss         | 0.112    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 51600  |\n",
      "|    time_elapsed    | 2872   |\n",
      "|    total_timesteps | 258000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=258500, episode_reward=286.46 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 286      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 258500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.401   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51699    |\n",
      "|    policy_loss        | -0.448   |\n",
      "|    value_loss         | 3.09     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 51700  |\n",
      "|    time_elapsed    | 2878   |\n",
      "|    total_timesteps | 258500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=259000, episode_reward=96.47 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 96.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 259000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.425   |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51799    |\n",
      "|    policy_loss        | 0.676    |\n",
      "|    value_loss         | 1.76     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 51800  |\n",
      "|    time_elapsed    | 2883   |\n",
      "|    total_timesteps | 259000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=259500, episode_reward=259.91 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 260      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 259500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.423   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51899    |\n",
      "|    policy_loss        | -0.778   |\n",
      "|    value_loss         | 2.33     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 51900  |\n",
      "|    time_elapsed    | 2889   |\n",
      "|    total_timesteps | 259500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=32.60 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 32.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 260000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.24     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 51999     |\n",
      "|    policy_loss        | -0.026    |\n",
      "|    value_loss         | 0.106     |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 52000  |\n",
      "|    time_elapsed    | 2895   |\n",
      "|    total_timesteps | 260000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=260500, episode_reward=6.32 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 6.32     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 260500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.366   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52099    |\n",
      "|    policy_loss        | -0.248   |\n",
      "|    value_loss         | 0.917    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 52100  |\n",
      "|    time_elapsed    | 2900   |\n",
      "|    total_timesteps | 260500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=261000, episode_reward=40.08 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 40.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 261000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.431   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52199    |\n",
      "|    policy_loss        | -0.101   |\n",
      "|    value_loss         | 0.0417   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 52200  |\n",
      "|    time_elapsed    | 2906   |\n",
      "|    total_timesteps | 261000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=261500, episode_reward=317.17 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 317      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 261500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.318   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52299    |\n",
      "|    policy_loss        | 1.31     |\n",
      "|    value_loss         | 215      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 52300  |\n",
      "|    time_elapsed    | 2912   |\n",
      "|    total_timesteps | 261500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=262000, episode_reward=311.41 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 311      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 262000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.381   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52399    |\n",
      "|    policy_loss        | 6.39     |\n",
      "|    value_loss         | 59       |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 52400  |\n",
      "|    time_elapsed    | 2917   |\n",
      "|    total_timesteps | 262000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=262500, episode_reward=259.57 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 260      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 262500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.388   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52499    |\n",
      "|    policy_loss        | -0.613   |\n",
      "|    value_loss         | 0.983    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 52500  |\n",
      "|    time_elapsed    | 2923   |\n",
      "|    total_timesteps | 262500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=263000, episode_reward=276.50 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 277      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 263000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.32    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52599    |\n",
      "|    policy_loss        | -0.215   |\n",
      "|    value_loss         | 2.83     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 52600  |\n",
      "|    time_elapsed    | 2929   |\n",
      "|    total_timesteps | 263000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=263500, episode_reward=286.53 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 287       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 263500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.384    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 52699     |\n",
      "|    policy_loss        | -0.271    |\n",
      "|    value_loss         | 0.278     |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 52700  |\n",
      "|    time_elapsed    | 2934   |\n",
      "|    total_timesteps | 263500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=264000, episode_reward=274.66 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 275      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 264000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.289   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52799    |\n",
      "|    policy_loss        | -0.0317  |\n",
      "|    value_loss         | 0.115    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 52800  |\n",
      "|    time_elapsed    | 2940   |\n",
      "|    total_timesteps | 264000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=264500, episode_reward=268.52 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 269      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 264500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.423   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52899    |\n",
      "|    policy_loss        | 6.65     |\n",
      "|    value_loss         | 180      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 52900  |\n",
      "|    time_elapsed    | 2945   |\n",
      "|    total_timesteps | 264500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=265000, episode_reward=301.67 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 302      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 265000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.391   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52999    |\n",
      "|    policy_loss        | -0.618   |\n",
      "|    value_loss         | 1.22     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 53000  |\n",
      "|    time_elapsed    | 2951   |\n",
      "|    total_timesteps | 265000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=265500, episode_reward=88.46 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 88.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 265500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.425   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53099    |\n",
      "|    policy_loss        | -0.253   |\n",
      "|    value_loss         | 0.323    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 53100  |\n",
      "|    time_elapsed    | 2957   |\n",
      "|    total_timesteps | 265500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=266000, episode_reward=100.61 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 101      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 266000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.44    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53199    |\n",
      "|    policy_loss        | -0.0414  |\n",
      "|    value_loss         | 0.0134   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 53200  |\n",
      "|    time_elapsed    | 2962   |\n",
      "|    total_timesteps | 266000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=266500, episode_reward=274.04 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 274       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 266500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.471    |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 53299     |\n",
      "|    policy_loss        | 5.44      |\n",
      "|    value_loss         | 113       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 53300  |\n",
      "|    time_elapsed    | 2968   |\n",
      "|    total_timesteps | 266500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=267000, episode_reward=275.37 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 275      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 267000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.477   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53399    |\n",
      "|    policy_loss        | 4.12     |\n",
      "|    value_loss         | 214      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 53400  |\n",
      "|    time_elapsed    | 2974   |\n",
      "|    total_timesteps | 267000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=267500, episode_reward=302.80 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 303       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 267500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.473    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 53499     |\n",
      "|    policy_loss        | -0.474    |\n",
      "|    value_loss         | 1.07      |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 53500  |\n",
      "|    time_elapsed    | 2979   |\n",
      "|    total_timesteps | 267500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=268000, episode_reward=293.18 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 293      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 268000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.476   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53599    |\n",
      "|    policy_loss        | -0.371   |\n",
      "|    value_loss         | 0.921    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 53600  |\n",
      "|    time_elapsed    | 2985   |\n",
      "|    total_timesteps | 268000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=268500, episode_reward=296.04 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 296      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 268500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.436   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53699    |\n",
      "|    policy_loss        | -0.674   |\n",
      "|    value_loss         | 2.91     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 53700  |\n",
      "|    time_elapsed    | 2991   |\n",
      "|    total_timesteps | 268500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=269000, episode_reward=255.41 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 255      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 269000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.425   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53799    |\n",
      "|    policy_loss        | -0.283   |\n",
      "|    value_loss         | 0.755    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 53800  |\n",
      "|    time_elapsed    | 2996   |\n",
      "|    total_timesteps | 269000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=269500, episode_reward=290.18 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 290      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 269500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.325   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53899    |\n",
      "|    policy_loss        | 6.87     |\n",
      "|    value_loss         | 113      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 53900  |\n",
      "|    time_elapsed    | 3002   |\n",
      "|    total_timesteps | 269500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=273.43 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 273      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 270000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.388   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53999    |\n",
      "|    policy_loss        | -0.249   |\n",
      "|    value_loss         | 3.84     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 54000  |\n",
      "|    time_elapsed    | 3008   |\n",
      "|    total_timesteps | 270000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=270500, episode_reward=275.05 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 275      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 270500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.375   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54099    |\n",
      "|    policy_loss        | 1.62     |\n",
      "|    value_loss         | 197      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 54100  |\n",
      "|    time_elapsed    | 3013   |\n",
      "|    total_timesteps | 270500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=271000, episode_reward=285.59 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 286      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 271000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.279   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54199    |\n",
      "|    policy_loss        | 0.799    |\n",
      "|    value_loss         | 137      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 54200  |\n",
      "|    time_elapsed    | 3019   |\n",
      "|    total_timesteps | 271000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=271500, episode_reward=289.77 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 290      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 271500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.409   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54299    |\n",
      "|    policy_loss        | 3.17     |\n",
      "|    value_loss         | 53.5     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 54300  |\n",
      "|    time_elapsed    | 3025   |\n",
      "|    total_timesteps | 271500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=272000, episode_reward=70.58 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 70.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 272000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.415   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54399    |\n",
      "|    policy_loss        | -0.102   |\n",
      "|    value_loss         | 0.0312   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 54400  |\n",
      "|    time_elapsed    | 3030   |\n",
      "|    total_timesteps | 272000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=272500, episode_reward=28.53 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 28.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 272500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.229   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54499    |\n",
      "|    policy_loss        | -0.267   |\n",
      "|    value_loss         | 1.03     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 54500  |\n",
      "|    time_elapsed    | 3036   |\n",
      "|    total_timesteps | 272500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=273000, episode_reward=99.16 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 99.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 273000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.421   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54599    |\n",
      "|    policy_loss        | -0.13    |\n",
      "|    value_loss         | 0.0925   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 54600  |\n",
      "|    time_elapsed    | 3042   |\n",
      "|    total_timesteps | 273000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=273500, episode_reward=310.94 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 311      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 273500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.401   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54699    |\n",
      "|    policy_loss        | -0.155   |\n",
      "|    value_loss         | 0.563    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 54700  |\n",
      "|    time_elapsed    | 3047   |\n",
      "|    total_timesteps | 273500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=274000, episode_reward=275.49 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 275       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 274000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.44     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 54799     |\n",
      "|    policy_loss        | -0.282    |\n",
      "|    value_loss         | 3.32      |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 54800  |\n",
      "|    time_elapsed    | 3053   |\n",
      "|    total_timesteps | 274000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=274500, episode_reward=274.93 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 275       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 274500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.41     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 54899     |\n",
      "|    policy_loss        | -0.0698   |\n",
      "|    value_loss         | 0.0644    |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 54900  |\n",
      "|    time_elapsed    | 3058   |\n",
      "|    total_timesteps | 274500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=275000, episode_reward=274.12 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 274      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 275000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.331   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54999    |\n",
      "|    policy_loss        | 1.74     |\n",
      "|    value_loss         | 196      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 55000  |\n",
      "|    time_elapsed    | 3064   |\n",
      "|    total_timesteps | 275000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=275500, episode_reward=273.01 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 273      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 275500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.419   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55099    |\n",
      "|    policy_loss        | -0.567   |\n",
      "|    value_loss         | 3.28     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 55100  |\n",
      "|    time_elapsed    | 3070   |\n",
      "|    total_timesteps | 275500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=276000, episode_reward=277.75 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 278      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 276000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.392   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55199    |\n",
      "|    policy_loss        | -0.384   |\n",
      "|    value_loss         | 0.637    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 55200  |\n",
      "|    time_elapsed    | 3075   |\n",
      "|    total_timesteps | 276000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=276500, episode_reward=234.57 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 235      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 276500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.423   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55299    |\n",
      "|    policy_loss        | -0.734   |\n",
      "|    value_loss         | 2.1      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 55300  |\n",
      "|    time_elapsed    | 3081   |\n",
      "|    total_timesteps | 276500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=277000, episode_reward=294.86 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 295      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 277000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.286   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55399    |\n",
      "|    policy_loss        | 1        |\n",
      "|    value_loss         | 132      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 55400  |\n",
      "|    time_elapsed    | 3086   |\n",
      "|    total_timesteps | 277000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=277500, episode_reward=71.94 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 71.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 277500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.394   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55499    |\n",
      "|    policy_loss        | -0.226   |\n",
      "|    value_loss         | 0.156    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 55500  |\n",
      "|    time_elapsed    | 3092   |\n",
      "|    total_timesteps | 277500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=278000, episode_reward=235.15 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 235      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 278000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.421   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55599    |\n",
      "|    policy_loss        | -0.332   |\n",
      "|    value_loss         | 0.405    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 55600  |\n",
      "|    time_elapsed    | 3098   |\n",
      "|    total_timesteps | 278000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=278500, episode_reward=282.26 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 282      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 278500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.278   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55699    |\n",
      "|    policy_loss        | 0.11     |\n",
      "|    value_loss         | 2.15     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 55700  |\n",
      "|    time_elapsed    | 3103   |\n",
      "|    total_timesteps | 278500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=279000, episode_reward=277.22 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 277       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 279000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.322    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 55799     |\n",
      "|    policy_loss        | 1.45      |\n",
      "|    value_loss         | 232       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 55800  |\n",
      "|    time_elapsed    | 3109   |\n",
      "|    total_timesteps | 279000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=279500, episode_reward=258.74 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 259      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 279500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.402   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55899    |\n",
      "|    policy_loss        | 20.2     |\n",
      "|    value_loss         | 342      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 55900  |\n",
      "|    time_elapsed    | 3115   |\n",
      "|    total_timesteps | 279500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=256.12 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 256      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 280000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.427   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55999    |\n",
      "|    policy_loss        | -0.159   |\n",
      "|    value_loss         | 0.095    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 56000  |\n",
      "|    time_elapsed    | 3120   |\n",
      "|    total_timesteps | 280000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=280500, episode_reward=275.70 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 276      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 280500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.421   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56099    |\n",
      "|    policy_loss        | -0.0312  |\n",
      "|    value_loss         | 0.0131   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 56100  |\n",
      "|    time_elapsed    | 3126   |\n",
      "|    total_timesteps | 280500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=281000, episode_reward=259.76 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 260       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 281000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.399    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 56199     |\n",
      "|    policy_loss        | -0.238    |\n",
      "|    value_loss         | 0.772     |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 56200  |\n",
      "|    time_elapsed    | 3131   |\n",
      "|    total_timesteps | 281000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=281500, episode_reward=277.56 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 278       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 281500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.27     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 56299     |\n",
      "|    policy_loss        | 1.01      |\n",
      "|    value_loss         | 186       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 56300  |\n",
      "|    time_elapsed    | 3137   |\n",
      "|    total_timesteps | 281500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=282000, episode_reward=269.04 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 269      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 282000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.424   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56399    |\n",
      "|    policy_loss        | 0.578    |\n",
      "|    value_loss         | 1.88     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 56400  |\n",
      "|    time_elapsed    | 3143   |\n",
      "|    total_timesteps | 282000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=282500, episode_reward=275.77 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 276       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 282500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.36     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 56499     |\n",
      "|    policy_loss        | -0.171    |\n",
      "|    value_loss         | 0.766     |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 56500  |\n",
      "|    time_elapsed    | 3148   |\n",
      "|    total_timesteps | 282500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=283000, episode_reward=262.32 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 262      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 283000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.406   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56599    |\n",
      "|    policy_loss        | 3.57     |\n",
      "|    value_loss         | 214      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 56600  |\n",
      "|    time_elapsed    | 3154   |\n",
      "|    total_timesteps | 283000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=283500, episode_reward=276.80 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 277      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 283500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.41    |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56699    |\n",
      "|    policy_loss        | 2.66     |\n",
      "|    value_loss         | 219      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 56700  |\n",
      "|    time_elapsed    | 3160   |\n",
      "|    total_timesteps | 283500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=284000, episode_reward=270.85 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 271      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 284000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.473   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56799    |\n",
      "|    policy_loss        | 5.16     |\n",
      "|    value_loss         | 54.6     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 56800  |\n",
      "|    time_elapsed    | 3165   |\n",
      "|    total_timesteps | 284000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=284500, episode_reward=271.27 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 271      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 284500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.42    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56899    |\n",
      "|    policy_loss        | -0.0828  |\n",
      "|    value_loss         | 0.129    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 56900  |\n",
      "|    time_elapsed    | 3171   |\n",
      "|    total_timesteps | 284500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=285000, episode_reward=282.13 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 282      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 285000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.328   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56999    |\n",
      "|    policy_loss        | 9.68     |\n",
      "|    value_loss         | 230      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 57000  |\n",
      "|    time_elapsed    | 3177   |\n",
      "|    total_timesteps | 285000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=285500, episode_reward=278.42 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 278      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 285500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.382   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57099    |\n",
      "|    policy_loss        | -0.126   |\n",
      "|    value_loss         | 0.606    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 57100  |\n",
      "|    time_elapsed    | 3182   |\n",
      "|    total_timesteps | 285500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=286000, episode_reward=240.74 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 241      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 286000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.424   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57199    |\n",
      "|    policy_loss        | -0.0772  |\n",
      "|    value_loss         | 0.0348   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 57200  |\n",
      "|    time_elapsed    | 3188   |\n",
      "|    total_timesteps | 286000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=286500, episode_reward=278.42 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 278      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 286500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.365   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57299    |\n",
      "|    policy_loss        | 6.07     |\n",
      "|    value_loss         | 110      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 57300  |\n",
      "|    time_elapsed    | 3194   |\n",
      "|    total_timesteps | 286500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=287000, episode_reward=283.75 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 284      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 287000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.45    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57399    |\n",
      "|    policy_loss        | 5.29     |\n",
      "|    value_loss         | 72.1     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 57400  |\n",
      "|    time_elapsed    | 3199   |\n",
      "|    total_timesteps | 287000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=287500, episode_reward=281.81 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 282      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 287500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.412   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57499    |\n",
      "|    policy_loss        | 16.4     |\n",
      "|    value_loss         | 339      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 57500  |\n",
      "|    time_elapsed    | 3205   |\n",
      "|    total_timesteps | 287500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=288000, episode_reward=254.04 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 254       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 288000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.369    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 57599     |\n",
      "|    policy_loss        | -0.0605   |\n",
      "|    value_loss         | 0.0982    |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 57600  |\n",
      "|    time_elapsed    | 3211   |\n",
      "|    total_timesteps | 288000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=288500, episode_reward=283.16 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 283      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 288500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.372   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57699    |\n",
      "|    policy_loss        | 7.57     |\n",
      "|    value_loss         | 39.7     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 57700  |\n",
      "|    time_elapsed    | 3216   |\n",
      "|    total_timesteps | 288500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=289000, episode_reward=237.60 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 238      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 289000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.345   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57799    |\n",
      "|    policy_loss        | -0.656   |\n",
      "|    value_loss         | 1.38     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 57800  |\n",
      "|    time_elapsed    | 3222   |\n",
      "|    total_timesteps | 289000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=289500, episode_reward=238.60 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 239      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 289500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.342   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57899    |\n",
      "|    policy_loss        | 5.2      |\n",
      "|    value_loss         | 53.5     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 57900  |\n",
      "|    time_elapsed    | 3227   |\n",
      "|    total_timesteps | 289500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=40.35 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 40.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 290000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.428   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57999    |\n",
      "|    policy_loss        | -0.0944  |\n",
      "|    value_loss         | 0.0325   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 58000  |\n",
      "|    time_elapsed    | 3233   |\n",
      "|    total_timesteps | 290000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=290500, episode_reward=245.99 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 246      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 290500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.357   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58099    |\n",
      "|    policy_loss        | 6.76     |\n",
      "|    value_loss         | 163      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 58100  |\n",
      "|    time_elapsed    | 3239   |\n",
      "|    total_timesteps | 290500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=291000, episode_reward=259.05 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 259       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 291000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.4      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 58199     |\n",
      "|    policy_loss        | 0.0284    |\n",
      "|    value_loss         | 0.00556   |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 58200  |\n",
      "|    time_elapsed    | 3244   |\n",
      "|    total_timesteps | 291000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=291500, episode_reward=282.99 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 283       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 291500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.46     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 58299     |\n",
      "|    policy_loss        | -0.132    |\n",
      "|    value_loss         | 0.0778    |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 58300  |\n",
      "|    time_elapsed    | 3250   |\n",
      "|    total_timesteps | 291500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=292000, episode_reward=283.20 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 283      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 292000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.522   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58399    |\n",
      "|    policy_loss        | 2.75     |\n",
      "|    value_loss         | 155      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 58400  |\n",
      "|    time_elapsed    | 3255   |\n",
      "|    total_timesteps | 292000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=292500, episode_reward=287.77 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 288      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 292500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.487   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58499    |\n",
      "|    policy_loss        | 0.114    |\n",
      "|    value_loss         | 0.0309   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 58500  |\n",
      "|    time_elapsed    | 3261   |\n",
      "|    total_timesteps | 292500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=293000, episode_reward=286.60 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 287      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 293000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.499   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58599    |\n",
      "|    policy_loss        | 4.67     |\n",
      "|    value_loss         | 199      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 58600  |\n",
      "|    time_elapsed    | 3267   |\n",
      "|    total_timesteps | 293000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=293500, episode_reward=285.16 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 285       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 293500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.405    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 58699     |\n",
      "|    policy_loss        | 1.83      |\n",
      "|    value_loss         | 164       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 58700  |\n",
      "|    time_elapsed    | 3272   |\n",
      "|    total_timesteps | 293500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=294000, episode_reward=273.93 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 274      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 294000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.473   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58799    |\n",
      "|    policy_loss        | 2.03     |\n",
      "|    value_loss         | 141      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 58800  |\n",
      "|    time_elapsed    | 3278   |\n",
      "|    total_timesteps | 294000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=294500, episode_reward=278.11 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 278       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 294500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.349    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 58899     |\n",
      "|    policy_loss        | -0.0231   |\n",
      "|    value_loss         | 0.00921   |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 58900  |\n",
      "|    time_elapsed    | 3284   |\n",
      "|    total_timesteps | 294500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=295000, episode_reward=437.82 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 438      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 295000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.39    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58999    |\n",
      "|    policy_loss        | -0.127   |\n",
      "|    value_loss         | 0.575    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 59000  |\n",
      "|    time_elapsed    | 3289   |\n",
      "|    total_timesteps | 295000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=295500, episode_reward=406.74 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 407      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 295500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.409   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59099    |\n",
      "|    policy_loss        | 1.52     |\n",
      "|    value_loss         | 51.1     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 59100  |\n",
      "|    time_elapsed    | 3295   |\n",
      "|    total_timesteps | 295500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=296000, episode_reward=404.46 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 404      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 296000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.345   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59199    |\n",
      "|    policy_loss        | -0.177   |\n",
      "|    value_loss         | 0.125    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 59200  |\n",
      "|    time_elapsed    | 3300   |\n",
      "|    total_timesteps | 296000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=296500, episode_reward=284.63 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 285       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 296500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.328    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 59299     |\n",
      "|    policy_loss        | 1.46      |\n",
      "|    value_loss         | 138       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 59300  |\n",
      "|    time_elapsed    | 3306   |\n",
      "|    total_timesteps | 296500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=297000, episode_reward=425.28 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 425      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 297000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.429   |\n",
      "|    explained_variance | 2.38e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59399    |\n",
      "|    policy_loss        | 1.64     |\n",
      "|    value_loss         | 35.4     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 59400  |\n",
      "|    time_elapsed    | 3312   |\n",
      "|    total_timesteps | 297000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=297500, episode_reward=444.43 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 444      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 297500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.443   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59499    |\n",
      "|    policy_loss        | -0.871   |\n",
      "|    value_loss         | 3.66     |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 59500  |\n",
      "|    time_elapsed    | 3317   |\n",
      "|    total_timesteps | 297500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=298000, episode_reward=274.65 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 275      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 298000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.441   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59599    |\n",
      "|    policy_loss        | -0.602   |\n",
      "|    value_loss         | 3.35     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 59600  |\n",
      "|    time_elapsed    | 3323   |\n",
      "|    total_timesteps | 298000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=298500, episode_reward=285.16 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 285      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 298500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.352   |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59699    |\n",
      "|    policy_loss        | 1.8      |\n",
      "|    value_loss         | 146      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 59700  |\n",
      "|    time_elapsed    | 3329   |\n",
      "|    total_timesteps | 298500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=299000, episode_reward=275.67 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 276       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 299000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.347    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 59799     |\n",
      "|    policy_loss        | 9.11      |\n",
      "|    value_loss         | 201       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 59800  |\n",
      "|    time_elapsed    | 3334   |\n",
      "|    total_timesteps | 299000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=299500, episode_reward=267.88 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 268      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 299500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.36    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59899    |\n",
      "|    policy_loss        | 1.86     |\n",
      "|    value_loss         | 134      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 59900  |\n",
      "|    time_elapsed    | 3340   |\n",
      "|    total_timesteps | 299500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=286.42 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 286      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 300000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.322   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59999    |\n",
      "|    policy_loss        | 13.2     |\n",
      "|    value_loss         | 231      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 60000  |\n",
      "|    time_elapsed    | 3346   |\n",
      "|    total_timesteps | 300000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=300500, episode_reward=260.96 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 261      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 300500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.295   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60099    |\n",
      "|    policy_loss        | 7.18     |\n",
      "|    value_loss         | 128      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 60100  |\n",
      "|    time_elapsed    | 3351   |\n",
      "|    total_timesteps | 300500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=301000, episode_reward=280.23 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 280       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 301000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.621    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 60199     |\n",
      "|    policy_loss        | 5.1       |\n",
      "|    value_loss         | 106       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 60200  |\n",
      "|    time_elapsed    | 3357   |\n",
      "|    total_timesteps | 301000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=301500, episode_reward=284.36 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 284      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 301500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.569   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60299    |\n",
      "|    policy_loss        | 5.9      |\n",
      "|    value_loss         | 49.6     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 60300  |\n",
      "|    time_elapsed    | 3363   |\n",
      "|    total_timesteps | 301500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=302000, episode_reward=275.33 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 275      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 302000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.655   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60399    |\n",
      "|    policy_loss        | 6.6      |\n",
      "|    value_loss         | 167      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 60400  |\n",
      "|    time_elapsed    | 3369   |\n",
      "|    total_timesteps | 302000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=302500, episode_reward=274.90 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 275       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 302500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.559    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 60499     |\n",
      "|    policy_loss        | 3.98      |\n",
      "|    value_loss         | 31.5      |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 60500  |\n",
      "|    time_elapsed    | 3374   |\n",
      "|    total_timesteps | 302500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=303000, episode_reward=270.53 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 271      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 303000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.463   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60599    |\n",
      "|    policy_loss        | 4.67     |\n",
      "|    value_loss         | 218      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 60600  |\n",
      "|    time_elapsed    | 3380   |\n",
      "|    total_timesteps | 303000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=303500, episode_reward=267.72 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 268      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 303500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.424   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60699    |\n",
      "|    policy_loss        | -0.421   |\n",
      "|    value_loss         | 1.16     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 60700  |\n",
      "|    time_elapsed    | 3385   |\n",
      "|    total_timesteps | 303500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=304000, episode_reward=264.14 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 264       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 304000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.414    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 60799     |\n",
      "|    policy_loss        | -0.146    |\n",
      "|    value_loss         | 0.0864    |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 60800  |\n",
      "|    time_elapsed    | 3391   |\n",
      "|    total_timesteps | 304000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=304500, episode_reward=287.29 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 287      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 304500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.341   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60899    |\n",
      "|    policy_loss        | -0.839   |\n",
      "|    value_loss         | 2        |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 60900  |\n",
      "|    time_elapsed    | 3397   |\n",
      "|    total_timesteps | 304500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=305000, episode_reward=75.73 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 75.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 305000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.424   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60999    |\n",
      "|    policy_loss        | -0.738   |\n",
      "|    value_loss         | 1.9      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 61000  |\n",
      "|    time_elapsed    | 3402   |\n",
      "|    total_timesteps | 305000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=305500, episode_reward=224.97 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 225      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 305500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.395   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61099    |\n",
      "|    policy_loss        | -0.211   |\n",
      "|    value_loss         | 0.645    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 61100  |\n",
      "|    time_elapsed    | 3408   |\n",
      "|    total_timesteps | 305500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=306000, episode_reward=283.28 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 283      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 306000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.351   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61199    |\n",
      "|    policy_loss        | 8.17     |\n",
      "|    value_loss         | 179      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 61200  |\n",
      "|    time_elapsed    | 3414   |\n",
      "|    total_timesteps | 306000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=306500, episode_reward=281.19 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 281      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 306500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.398   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61299    |\n",
      "|    policy_loss        | -0.743   |\n",
      "|    value_loss         | 2.2      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 61300  |\n",
      "|    time_elapsed    | 3419   |\n",
      "|    total_timesteps | 306500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=307000, episode_reward=259.89 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 260       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 307000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.431    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 61399     |\n",
      "|    policy_loss        | -0.035    |\n",
      "|    value_loss         | 0.0127    |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 61400  |\n",
      "|    time_elapsed    | 3425   |\n",
      "|    total_timesteps | 307000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=307500, episode_reward=65.85 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 65.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 307500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.418   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61499    |\n",
      "|    policy_loss        | -0.113   |\n",
      "|    value_loss         | 0.0398   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 61500  |\n",
      "|    time_elapsed    | 3431   |\n",
      "|    total_timesteps | 307500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=308000, episode_reward=282.85 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 283      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 308000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.302   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61599    |\n",
      "|    policy_loss        | 1.77     |\n",
      "|    value_loss         | 205      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 61600  |\n",
      "|    time_elapsed    | 3436   |\n",
      "|    total_timesteps | 308000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=308500, episode_reward=226.38 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 226      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 308500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.422   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61699    |\n",
      "|    policy_loss        | -0.149   |\n",
      "|    value_loss         | 0.067    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 61700  |\n",
      "|    time_elapsed    | 3442   |\n",
      "|    total_timesteps | 308500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=309000, episode_reward=273.35 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 273      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 309000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.383   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61799    |\n",
      "|    policy_loss        | -0.0389  |\n",
      "|    value_loss         | 0.0294   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 61800  |\n",
      "|    time_elapsed    | 3448   |\n",
      "|    total_timesteps | 309000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=309500, episode_reward=281.77 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 282      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 309500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.448   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61899    |\n",
      "|    policy_loss        | 5.36     |\n",
      "|    value_loss         | 108      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 61900  |\n",
      "|    time_elapsed    | 3453   |\n",
      "|    total_timesteps | 309500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=439.52 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 440       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 310000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.421    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 61999     |\n",
      "|    policy_loss        | 2.9       |\n",
      "|    value_loss         | 51        |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 62000  |\n",
      "|    time_elapsed    | 3459   |\n",
      "|    total_timesteps | 310000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=310500, episode_reward=397.76 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 398      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 310500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.395   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62099    |\n",
      "|    policy_loss        | -0.502   |\n",
      "|    value_loss         | 1.1      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 62100  |\n",
      "|    time_elapsed    | 3464   |\n",
      "|    total_timesteps | 310500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=311000, episode_reward=273.64 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 274      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 311000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.419   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62199    |\n",
      "|    policy_loss        | -0.307   |\n",
      "|    value_loss         | 0.358    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 62200  |\n",
      "|    time_elapsed    | 3470   |\n",
      "|    total_timesteps | 311000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=311500, episode_reward=265.19 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 265       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 311500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.423    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 62299     |\n",
      "|    policy_loss        | 3.67      |\n",
      "|    value_loss         | 131       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 62300  |\n",
      "|    time_elapsed    | 3476   |\n",
      "|    total_timesteps | 311500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=312000, episode_reward=282.31 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 282      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 312000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.423   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62399    |\n",
      "|    policy_loss        | -0.0396  |\n",
      "|    value_loss         | 0.0252   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 62400  |\n",
      "|    time_elapsed    | 3481   |\n",
      "|    total_timesteps | 312000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=312500, episode_reward=271.09 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 271      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 312500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.375   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62499    |\n",
      "|    policy_loss        | -0.606   |\n",
      "|    value_loss         | 0.768    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 62500  |\n",
      "|    time_elapsed    | 3487   |\n",
      "|    total_timesteps | 312500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=313000, episode_reward=270.70 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 271      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 313000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.443   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62599    |\n",
      "|    policy_loss        | -0.0411  |\n",
      "|    value_loss         | 0.0109   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 62600  |\n",
      "|    time_elapsed    | 3492   |\n",
      "|    total_timesteps | 313000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=313500, episode_reward=282.92 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 283      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 313500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.442   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62699    |\n",
      "|    policy_loss        | -0.0876  |\n",
      "|    value_loss         | 0.0284   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 62700  |\n",
      "|    time_elapsed    | 3498   |\n",
      "|    total_timesteps | 313500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=314000, episode_reward=416.03 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 416      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 314000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.369   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62799    |\n",
      "|    policy_loss        | -0.111   |\n",
      "|    value_loss         | 0.593    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 62800  |\n",
      "|    time_elapsed    | 3504   |\n",
      "|    total_timesteps | 314000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=314500, episode_reward=273.67 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 274       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 314500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.44     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 62899     |\n",
      "|    policy_loss        | -0.26     |\n",
      "|    value_loss         | 0.155     |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 62900  |\n",
      "|    time_elapsed    | 3509   |\n",
      "|    total_timesteps | 314500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=315000, episode_reward=278.48 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 278      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 315000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.434   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62999    |\n",
      "|    policy_loss        | -0.0623  |\n",
      "|    value_loss         | 0.018    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 63000  |\n",
      "|    time_elapsed    | 3515   |\n",
      "|    total_timesteps | 315000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=315500, episode_reward=285.79 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 286      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 315500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.433   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63099    |\n",
      "|    policy_loss        | 3.52     |\n",
      "|    value_loss         | 146      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 63100  |\n",
      "|    time_elapsed    | 3520   |\n",
      "|    total_timesteps | 315500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=316000, episode_reward=239.20 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 239       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 316000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.409    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 63199     |\n",
      "|    policy_loss        | -0.123    |\n",
      "|    value_loss         | 0.546     |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 63200  |\n",
      "|    time_elapsed    | 3526   |\n",
      "|    total_timesteps | 316000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=316500, episode_reward=409.83 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 410      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 316500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.435   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63299    |\n",
      "|    policy_loss        | -0.787   |\n",
      "|    value_loss         | 2.76     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 63300  |\n",
      "|    time_elapsed    | 3532   |\n",
      "|    total_timesteps | 316500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=317000, episode_reward=52.42 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 52.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 317000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.256   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63399    |\n",
      "|    policy_loss        | -0.00352 |\n",
      "|    value_loss         | 0.00302  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 63400  |\n",
      "|    time_elapsed    | 3537   |\n",
      "|    total_timesteps | 317000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=317500, episode_reward=49.05 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 49.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 317500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.322   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63499    |\n",
      "|    policy_loss        | -0.00318 |\n",
      "|    value_loss         | 0.00439  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 63500  |\n",
      "|    time_elapsed    | 3543   |\n",
      "|    total_timesteps | 317500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=318000, episode_reward=271.28 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 271       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 318000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.397    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 63599     |\n",
      "|    policy_loss        | 2.98      |\n",
      "|    value_loss         | 217       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 63600  |\n",
      "|    time_elapsed    | 3549   |\n",
      "|    total_timesteps | 318000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=318500, episode_reward=271.24 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 271      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 318500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.458   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63699    |\n",
      "|    policy_loss        | -0.118   |\n",
      "|    value_loss         | 0.0342   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 63700  |\n",
      "|    time_elapsed    | 3554   |\n",
      "|    total_timesteps | 318500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=319000, episode_reward=279.06 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 279      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 319000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.415   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63799    |\n",
      "|    policy_loss        | 2.16     |\n",
      "|    value_loss         | 218      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 63800  |\n",
      "|    time_elapsed    | 3560   |\n",
      "|    total_timesteps | 319000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=319500, episode_reward=276.91 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 277      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 319500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.493   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63899    |\n",
      "|    policy_loss        | -0.0452  |\n",
      "|    value_loss         | 0.0233   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 63900  |\n",
      "|    time_elapsed    | 3566   |\n",
      "|    total_timesteps | 319500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=45.99 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 46       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 320000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.367   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63999    |\n",
      "|    policy_loss        | -0.0142  |\n",
      "|    value_loss         | 0.00498  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 64000  |\n",
      "|    time_elapsed    | 3571   |\n",
      "|    total_timesteps | 320000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=320500, episode_reward=222.40 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 222      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 320500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.429   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64099    |\n",
      "|    policy_loss        | -0.223   |\n",
      "|    value_loss         | 0.617    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 64100  |\n",
      "|    time_elapsed    | 3577   |\n",
      "|    total_timesteps | 320500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=321000, episode_reward=273.26 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 273      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 321000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.4     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64199    |\n",
      "|    policy_loss        | 1.65     |\n",
      "|    value_loss         | 175      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 64200  |\n",
      "|    time_elapsed    | 3582   |\n",
      "|    total_timesteps | 321000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=321500, episode_reward=278.17 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 278      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 321500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.414   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64299    |\n",
      "|    policy_loss        | 4.53     |\n",
      "|    value_loss         | 54.5     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 64300  |\n",
      "|    time_elapsed    | 3588   |\n",
      "|    total_timesteps | 321500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=322000, episode_reward=43.25 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 43.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 322000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.379   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64399    |\n",
      "|    policy_loss        | 0.101    |\n",
      "|    value_loss         | 0.0687   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 64400  |\n",
      "|    time_elapsed    | 3594   |\n",
      "|    total_timesteps | 322000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=322500, episode_reward=299.76 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 300       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 322500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.427    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 64499     |\n",
      "|    policy_loss        | 5.48      |\n",
      "|    value_loss         | 114       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 64500  |\n",
      "|    time_elapsed    | 3600   |\n",
      "|    total_timesteps | 322500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=323000, episode_reward=238.38 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 238      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 323000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.417   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64599    |\n",
      "|    policy_loss        | -0.0548  |\n",
      "|    value_loss         | 0.0155   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 64600  |\n",
      "|    time_elapsed    | 3605   |\n",
      "|    total_timesteps | 323000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=323500, episode_reward=265.47 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 265      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 323500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.421   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64699    |\n",
      "|    policy_loss        | -0.586   |\n",
      "|    value_loss         | 3.02     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 64700  |\n",
      "|    time_elapsed    | 3611   |\n",
      "|    total_timesteps | 323500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=324000, episode_reward=272.52 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 273      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 324000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.449   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64799    |\n",
      "|    policy_loss        | -0.44    |\n",
      "|    value_loss         | 0.947    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 64800  |\n",
      "|    time_elapsed    | 3617   |\n",
      "|    total_timesteps | 324000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=324500, episode_reward=271.95 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 272      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 324500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.377   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64899    |\n",
      "|    policy_loss        | 1.29     |\n",
      "|    value_loss         | 3.9      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 64900  |\n",
      "|    time_elapsed    | 3622   |\n",
      "|    total_timesteps | 324500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=325000, episode_reward=279.75 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 280      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 325000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.36    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64999    |\n",
      "|    policy_loss        | 7.98     |\n",
      "|    value_loss         | 57.6     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 65000  |\n",
      "|    time_elapsed    | 3628   |\n",
      "|    total_timesteps | 325000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=325500, episode_reward=264.53 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 265       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 325500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.441    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 65099     |\n",
      "|    policy_loss        | 1.49      |\n",
      "|    value_loss         | 9.04      |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 65100  |\n",
      "|    time_elapsed    | 3633   |\n",
      "|    total_timesteps | 325500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=326000, episode_reward=271.65 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 272      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 326000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.432   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65199    |\n",
      "|    policy_loss        | -0.077   |\n",
      "|    value_loss         | 0.0782   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 65200  |\n",
      "|    time_elapsed    | 3639   |\n",
      "|    total_timesteps | 326000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=326500, episode_reward=276.58 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 277      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 326500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.444   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65299    |\n",
      "|    policy_loss        | -0.0948  |\n",
      "|    value_loss         | 0.0395   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 65300  |\n",
      "|    time_elapsed    | 3645   |\n",
      "|    total_timesteps | 326500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=327000, episode_reward=273.46 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 273      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 327000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.465   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65399    |\n",
      "|    policy_loss        | -0.61    |\n",
      "|    value_loss         | 1.01     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 65400  |\n",
      "|    time_elapsed    | 3651   |\n",
      "|    total_timesteps | 327000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=327500, episode_reward=308.30 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 308      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 327500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.43    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65499    |\n",
      "|    policy_loss        | 3.81     |\n",
      "|    value_loss         | 218      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 65500  |\n",
      "|    time_elapsed    | 3656   |\n",
      "|    total_timesteps | 327500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=328000, episode_reward=273.72 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 274      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 328000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.422   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65599    |\n",
      "|    policy_loss        | 0.787    |\n",
      "|    value_loss         | 14.9     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 65600  |\n",
      "|    time_elapsed    | 3662   |\n",
      "|    total_timesteps | 328000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=328500, episode_reward=245.26 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 245      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 328500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.432   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65699    |\n",
      "|    policy_loss        | -0.0138  |\n",
      "|    value_loss         | 0.0062   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 65700  |\n",
      "|    time_elapsed    | 3668   |\n",
      "|    total_timesteps | 328500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=329000, episode_reward=307.53 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 308      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 329000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.425   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65799    |\n",
      "|    policy_loss        | 3.67     |\n",
      "|    value_loss         | 14.3     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 65800  |\n",
      "|    time_elapsed    | 3673   |\n",
      "|    total_timesteps | 329000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=329500, episode_reward=272.31 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 272      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 329500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.402   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65899    |\n",
      "|    policy_loss        | -0.105   |\n",
      "|    value_loss         | 0.0545   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 65900  |\n",
      "|    time_elapsed    | 3679   |\n",
      "|    total_timesteps | 329500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=52.42 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 52.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 330000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.352   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65999    |\n",
      "|    policy_loss        | 0.0557   |\n",
      "|    value_loss         | 0.0207   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 66000  |\n",
      "|    time_elapsed    | 3684   |\n",
      "|    total_timesteps | 330000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=330500, episode_reward=271.46 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 271      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 330500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.379   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66099    |\n",
      "|    policy_loss        | -0.198   |\n",
      "|    value_loss         | 0.161    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 66100  |\n",
      "|    time_elapsed    | 3690   |\n",
      "|    total_timesteps | 330500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=331000, episode_reward=273.21 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 273      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 331000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.401   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66199    |\n",
      "|    policy_loss        | -0.197   |\n",
      "|    value_loss         | 0.541    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 66200  |\n",
      "|    time_elapsed    | 3696   |\n",
      "|    total_timesteps | 331000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=331500, episode_reward=66.06 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 66.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 331500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.388   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66299    |\n",
      "|    policy_loss        | -0.0701  |\n",
      "|    value_loss         | 0.0269   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 66300  |\n",
      "|    time_elapsed    | 3701   |\n",
      "|    total_timesteps | 331500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=332000, episode_reward=67.32 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 67.3      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 332000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.42     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 66399     |\n",
      "|    policy_loss        | -0.148    |\n",
      "|    value_loss         | 0.0788    |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 66400  |\n",
      "|    time_elapsed    | 3707   |\n",
      "|    total_timesteps | 332000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=332500, episode_reward=53.85 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 53.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 332500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.413   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66499    |\n",
      "|    policy_loss        | -0.0346  |\n",
      "|    value_loss         | 0.0124   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 66500  |\n",
      "|    time_elapsed    | 3713   |\n",
      "|    total_timesteps | 332500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=333000, episode_reward=261.09 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 261      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 333000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.367   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66599    |\n",
      "|    policy_loss        | -0.0419  |\n",
      "|    value_loss         | 0.0344   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 66600  |\n",
      "|    time_elapsed    | 3718   |\n",
      "|    total_timesteps | 333000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=333500, episode_reward=244.71 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 245      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 333500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.384   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66699    |\n",
      "|    policy_loss        | -0.44    |\n",
      "|    value_loss         | 2.79     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 66700  |\n",
      "|    time_elapsed    | 3724   |\n",
      "|    total_timesteps | 333500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=334000, episode_reward=252.18 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 252      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 334000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.359   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66799    |\n",
      "|    policy_loss        | -0.0554  |\n",
      "|    value_loss         | 0.0589   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 66800  |\n",
      "|    time_elapsed    | 3730   |\n",
      "|    total_timesteps | 334000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=334500, episode_reward=248.49 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 248      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 334500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.278   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66899    |\n",
      "|    policy_loss        | 6.81     |\n",
      "|    value_loss         | 113      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 66900  |\n",
      "|    time_elapsed    | 3735   |\n",
      "|    total_timesteps | 334500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=335000, episode_reward=247.56 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 248      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 335000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.303   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66999    |\n",
      "|    policy_loss        | 5.61     |\n",
      "|    value_loss         | 53.1     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 67000  |\n",
      "|    time_elapsed    | 3741   |\n",
      "|    total_timesteps | 335000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=335500, episode_reward=224.82 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 225       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 335500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.397    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 67099     |\n",
      "|    policy_loss        | -0.114    |\n",
      "|    value_loss         | 0.128     |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 67100  |\n",
      "|    time_elapsed    | 3747   |\n",
      "|    total_timesteps | 335500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=336000, episode_reward=238.92 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 239      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 336000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.407   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67199    |\n",
      "|    policy_loss        | 4.77     |\n",
      "|    value_loss         | 201      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 67200  |\n",
      "|    time_elapsed    | 3752   |\n",
      "|    total_timesteps | 336000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=336500, episode_reward=256.02 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 256      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 336500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.403   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67299    |\n",
      "|    policy_loss        | 0.0255   |\n",
      "|    value_loss         | 0.00586  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 67300  |\n",
      "|    time_elapsed    | 3758   |\n",
      "|    total_timesteps | 336500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=337000, episode_reward=283.27 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 283      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 337000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.383   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67399    |\n",
      "|    policy_loss        | 1.55     |\n",
      "|    value_loss         | 128      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 67400  |\n",
      "|    time_elapsed    | 3763   |\n",
      "|    total_timesteps | 337000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=337500, episode_reward=442.11 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 442      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 337500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.53    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67499    |\n",
      "|    policy_loss        | 3.3      |\n",
      "|    value_loss         | 62.6     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 67500  |\n",
      "|    time_elapsed    | 3769   |\n",
      "|    total_timesteps | 337500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=338000, episode_reward=285.74 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 286      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 338000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.369   |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67599    |\n",
      "|    policy_loss        | 2.17     |\n",
      "|    value_loss         | 248      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 67600  |\n",
      "|    time_elapsed    | 3775   |\n",
      "|    total_timesteps | 338000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=338500, episode_reward=265.99 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 266      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 338500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.44    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67699    |\n",
      "|    policy_loss        | 5.82     |\n",
      "|    value_loss         | 211      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 67700  |\n",
      "|    time_elapsed    | 3780   |\n",
      "|    total_timesteps | 338500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=339000, episode_reward=435.22 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 435      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 339000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.372   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67799    |\n",
      "|    policy_loss        | -0.0138  |\n",
      "|    value_loss         | 0.00394  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 67800  |\n",
      "|    time_elapsed    | 3786   |\n",
      "|    total_timesteps | 339000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=339500, episode_reward=62.57 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 62.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 339500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.183   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67899    |\n",
      "|    policy_loss        | -0.00242 |\n",
      "|    value_loss         | 0.00274  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 67900  |\n",
      "|    time_elapsed    | 3792   |\n",
      "|    total_timesteps | 339500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=68.00 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 68       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 340000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.386   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67999    |\n",
      "|    policy_loss        | -0.17    |\n",
      "|    value_loss         | 0.0703   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 68000  |\n",
      "|    time_elapsed    | 3797   |\n",
      "|    total_timesteps | 340000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=340500, episode_reward=394.39 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 394      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 340500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.411   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68099    |\n",
      "|    policy_loss        | -0.0759  |\n",
      "|    value_loss         | 0.0412   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 68100  |\n",
      "|    time_elapsed    | 3803   |\n",
      "|    total_timesteps | 340500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=341000, episode_reward=223.90 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 224      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 341000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.416   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68199    |\n",
      "|    policy_loss        | -0.516   |\n",
      "|    value_loss         | 1.25     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 68200  |\n",
      "|    time_elapsed    | 3809   |\n",
      "|    total_timesteps | 341000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=341500, episode_reward=221.38 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 221      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 341500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.346   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68299    |\n",
      "|    policy_loss        | -0.0378  |\n",
      "|    value_loss         | 0.0329   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 68300  |\n",
      "|    time_elapsed    | 3814   |\n",
      "|    total_timesteps | 341500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=342000, episode_reward=400.01 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 400      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 342000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.417   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68399    |\n",
      "|    policy_loss        | -0.199   |\n",
      "|    value_loss         | 0.156    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 68400  |\n",
      "|    time_elapsed    | 3820   |\n",
      "|    total_timesteps | 342000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=342500, episode_reward=251.66 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 252      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 342500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.419   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68499    |\n",
      "|    policy_loss        | -0.468   |\n",
      "|    value_loss         | 1.07     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 68500  |\n",
      "|    time_elapsed    | 3826   |\n",
      "|    total_timesteps | 342500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=343000, episode_reward=37.08 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 37.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 343000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.341   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68599    |\n",
      "|    policy_loss        | -0.069   |\n",
      "|    value_loss         | 0.534    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 68600  |\n",
      "|    time_elapsed    | 3831   |\n",
      "|    total_timesteps | 343000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=343500, episode_reward=246.43 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 246      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 343500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.387   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68699    |\n",
      "|    policy_loss        | -0.186   |\n",
      "|    value_loss         | 0.164    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 68700  |\n",
      "|    time_elapsed    | 3837   |\n",
      "|    total_timesteps | 343500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=344000, episode_reward=71.86 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 71.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 344000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.351   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68799    |\n",
      "|    policy_loss        | -0.182   |\n",
      "|    value_loss         | 0.116    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 68800  |\n",
      "|    time_elapsed    | 3843   |\n",
      "|    total_timesteps | 344000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=344500, episode_reward=396.03 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 396       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 344500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.419    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 68899     |\n",
      "|    policy_loss        | 0.0274    |\n",
      "|    value_loss         | 0.00574   |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 68900  |\n",
      "|    time_elapsed    | 3848   |\n",
      "|    total_timesteps | 344500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=345000, episode_reward=246.44 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 246      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 345000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.421   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68999    |\n",
      "|    policy_loss        | -0.237   |\n",
      "|    value_loss         | 0.612    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 69000  |\n",
      "|    time_elapsed    | 3854   |\n",
      "|    total_timesteps | 345000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=345500, episode_reward=53.48 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 53.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 345500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.409   |\n",
      "|    explained_variance | 2.38e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69099    |\n",
      "|    policy_loss        | -0.0482  |\n",
      "|    value_loss         | 0.0904   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 69100  |\n",
      "|    time_elapsed    | 3860   |\n",
      "|    total_timesteps | 345500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=346000, episode_reward=64.36 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 64.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 346000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.391   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69199    |\n",
      "|    policy_loss        | 0.359    |\n",
      "|    value_loss         | 0.645    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 69200  |\n",
      "|    time_elapsed    | 3865   |\n",
      "|    total_timesteps | 346000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=346500, episode_reward=246.95 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 247      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 346500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.414   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69299    |\n",
      "|    policy_loss        | -0.4     |\n",
      "|    value_loss         | 0.685    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 69300  |\n",
      "|    time_elapsed    | 3871   |\n",
      "|    total_timesteps | 346500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=347000, episode_reward=273.28 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 273      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 347000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.426   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69399    |\n",
      "|    policy_loss        | -0.0693  |\n",
      "|    value_loss         | 0.0415   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 69400  |\n",
      "|    time_elapsed    | 3876   |\n",
      "|    total_timesteps | 347000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=347500, episode_reward=250.20 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 250      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 347500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.423   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69499    |\n",
      "|    policy_loss        | 0.11     |\n",
      "|    value_loss         | 0.14     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 69500  |\n",
      "|    time_elapsed    | 3882   |\n",
      "|    total_timesteps | 347500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=348000, episode_reward=282.62 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 283      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 348000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.39    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69599    |\n",
      "|    policy_loss        | 2.31     |\n",
      "|    value_loss         | 151      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 69600  |\n",
      "|    time_elapsed    | 3888   |\n",
      "|    total_timesteps | 348000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=348500, episode_reward=260.70 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 261      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 348500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.467   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69699    |\n",
      "|    policy_loss        | 2.61     |\n",
      "|    value_loss         | 5        |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 69700  |\n",
      "|    time_elapsed    | 3893   |\n",
      "|    total_timesteps | 348500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=349000, episode_reward=410.93 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 411      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 349000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.416   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69799    |\n",
      "|    policy_loss        | 0.0175   |\n",
      "|    value_loss         | 0.0201   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 69800  |\n",
      "|    time_elapsed    | 3899   |\n",
      "|    total_timesteps | 349000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=349500, episode_reward=66.33 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 66.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 349500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.414   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69899    |\n",
      "|    policy_loss        | -0.123   |\n",
      "|    value_loss         | 0.0627   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 69900  |\n",
      "|    time_elapsed    | 3905   |\n",
      "|    total_timesteps | 349500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=45.59 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 45.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 350000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.379    |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 69999     |\n",
      "|    policy_loss        | -0.126    |\n",
      "|    value_loss         | 0.593     |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 70000  |\n",
      "|    time_elapsed    | 3910   |\n",
      "|    total_timesteps | 350000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=350500, episode_reward=64.60 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 64.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 350500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.419   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70099    |\n",
      "|    policy_loss        | 0.056    |\n",
      "|    value_loss         | 0.0492   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 70100  |\n",
      "|    time_elapsed    | 3916   |\n",
      "|    total_timesteps | 350500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=351000, episode_reward=69.66 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 69.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 351000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.424   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70199    |\n",
      "|    policy_loss        | 0.0944   |\n",
      "|    value_loss         | 0.0838   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 70200  |\n",
      "|    time_elapsed    | 3921   |\n",
      "|    total_timesteps | 351000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=351500, episode_reward=270.95 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 271      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 351500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.418   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70299    |\n",
      "|    policy_loss        | -0.357   |\n",
      "|    value_loss         | 0.753    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 70300  |\n",
      "|    time_elapsed    | 3927   |\n",
      "|    total_timesteps | 351500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=352000, episode_reward=280.69 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 281       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 352000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.439    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 70399     |\n",
      "|    policy_loss        | 2.81      |\n",
      "|    value_loss         | 212       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 70400  |\n",
      "|    time_elapsed    | 3933   |\n",
      "|    total_timesteps | 352000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=352500, episode_reward=281.90 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 282       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 352500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.464    |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 70499     |\n",
      "|    policy_loss        | 3.57      |\n",
      "|    value_loss         | 48.3      |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 70500  |\n",
      "|    time_elapsed    | 3938   |\n",
      "|    total_timesteps | 352500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=353000, episode_reward=272.18 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 272      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 353000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.517   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70599    |\n",
      "|    policy_loss        | 3.55     |\n",
      "|    value_loss         | 34.6     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 70600  |\n",
      "|    time_elapsed    | 3944   |\n",
      "|    total_timesteps | 353000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=353500, episode_reward=285.47 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 285      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 353500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.552   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70699    |\n",
      "|    policy_loss        | 0.0297   |\n",
      "|    value_loss         | 0.0493   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 70700  |\n",
      "|    time_elapsed    | 3950   |\n",
      "|    total_timesteps | 353500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=354000, episode_reward=272.54 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 273       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 354000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.504    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 70799     |\n",
      "|    policy_loss        | -0.386    |\n",
      "|    value_loss         | 2.23      |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 70800  |\n",
      "|    time_elapsed    | 3955   |\n",
      "|    total_timesteps | 354000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=354500, episode_reward=435.12 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 435      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 354500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.424   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70899    |\n",
      "|    policy_loss        | 0.492    |\n",
      "|    value_loss         | 5        |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 70900  |\n",
      "|    time_elapsed    | 3961   |\n",
      "|    total_timesteps | 354500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=355000, episode_reward=435.38 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 435      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 355000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.44    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70999    |\n",
      "|    policy_loss        | 0.0538   |\n",
      "|    value_loss         | 0.0395   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 71000  |\n",
      "|    time_elapsed    | 3967   |\n",
      "|    total_timesteps | 355000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=355500, episode_reward=435.80 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 436      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 355500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.387   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71099    |\n",
      "|    policy_loss        | -0.461   |\n",
      "|    value_loss         | 0.891    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 71100  |\n",
      "|    time_elapsed    | 3972   |\n",
      "|    total_timesteps | 355500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=356000, episode_reward=439.33 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 439      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 356000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.338   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71199    |\n",
      "|    policy_loss        | -0.17    |\n",
      "|    value_loss         | 0.765    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 71200  |\n",
      "|    time_elapsed    | 3978   |\n",
      "|    total_timesteps | 356000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=356500, episode_reward=282.19 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 282      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 356500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.419   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71299    |\n",
      "|    policy_loss        | 0.0642   |\n",
      "|    value_loss         | 0.027    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 71300  |\n",
      "|    time_elapsed    | 3984   |\n",
      "|    total_timesteps | 356500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=357000, episode_reward=255.45 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 255      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 357000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.411   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71399    |\n",
      "|    policy_loss        | 3.8      |\n",
      "|    value_loss         | 136      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 71400  |\n",
      "|    time_elapsed    | 3989   |\n",
      "|    total_timesteps | 357000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=357500, episode_reward=253.65 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 254      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 357500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.261   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71499    |\n",
      "|    policy_loss        | 7.37     |\n",
      "|    value_loss         | 109      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 71500  |\n",
      "|    time_elapsed    | 3995   |\n",
      "|    total_timesteps | 357500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=358000, episode_reward=274.42 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 274      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 358000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.308   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71599    |\n",
      "|    policy_loss        | 1.14     |\n",
      "|    value_loss         | 142      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 71600  |\n",
      "|    time_elapsed    | 4001   |\n",
      "|    total_timesteps | 358000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=358500, episode_reward=275.24 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 275       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 358500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.386    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 71699     |\n",
      "|    policy_loss        | -0.225    |\n",
      "|    value_loss         | 0.146     |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 71700  |\n",
      "|    time_elapsed    | 4006   |\n",
      "|    total_timesteps | 358500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=359000, episode_reward=277.64 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 278      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 359000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.327   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71799    |\n",
      "|    policy_loss        | 1.7      |\n",
      "|    value_loss         | 139      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 71800  |\n",
      "|    time_elapsed    | 4012   |\n",
      "|    total_timesteps | 359000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=359500, episode_reward=266.10 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 266       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 359500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.461    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 71899     |\n",
      "|    policy_loss        | 4.73      |\n",
      "|    value_loss         | 115       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 71900  |\n",
      "|    time_elapsed    | 4018   |\n",
      "|    total_timesteps | 359500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=271.14 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 271       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 360000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.459    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 71999     |\n",
      "|    policy_loss        | 5.25      |\n",
      "|    value_loss         | 109       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 72000  |\n",
      "|    time_elapsed    | 4023   |\n",
      "|    total_timesteps | 360000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=360500, episode_reward=438.20 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 438      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 360500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.443   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72099    |\n",
      "|    policy_loss        | 0.0486   |\n",
      "|    value_loss         | 0.033    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 72100  |\n",
      "|    time_elapsed    | 4029   |\n",
      "|    total_timesteps | 360500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=361000, episode_reward=272.29 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 272      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 361000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.458   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72199    |\n",
      "|    policy_loss        | 0.00653  |\n",
      "|    value_loss         | 0.0269   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 72200  |\n",
      "|    time_elapsed    | 4035   |\n",
      "|    total_timesteps | 361000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=361500, episode_reward=273.24 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 273      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 361500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.428   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72299    |\n",
      "|    policy_loss        | 3.77     |\n",
      "|    value_loss         | 72.8     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 72300  |\n",
      "|    time_elapsed    | 4040   |\n",
      "|    total_timesteps | 361500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=362000, episode_reward=272.46 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 272      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 362000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.407   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72399    |\n",
      "|    policy_loss        | 0.0164   |\n",
      "|    value_loss         | 0.022    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 72400  |\n",
      "|    time_elapsed    | 4046   |\n",
      "|    total_timesteps | 362000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=362500, episode_reward=269.06 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 269      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 362500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.362   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72499    |\n",
      "|    policy_loss        | -0.0449  |\n",
      "|    value_loss         | 0.0813   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 72500  |\n",
      "|    time_elapsed    | 4051   |\n",
      "|    total_timesteps | 362500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=363000, episode_reward=273.72 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 274       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 363000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.331    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 72599     |\n",
      "|    policy_loss        | -0.323    |\n",
      "|    value_loss         | 3.39      |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 72600  |\n",
      "|    time_elapsed    | 4057   |\n",
      "|    total_timesteps | 363000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=363500, episode_reward=225.37 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 225      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 363500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.331   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72699    |\n",
      "|    policy_loss        | -0.174   |\n",
      "|    value_loss         | 0.791    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 72700  |\n",
      "|    time_elapsed    | 4063   |\n",
      "|    total_timesteps | 363500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=364000, episode_reward=252.32 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 252      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 364000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.313   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72799    |\n",
      "|    policy_loss        | -0.26    |\n",
      "|    value_loss         | 2.89     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 72800  |\n",
      "|    time_elapsed    | 4068   |\n",
      "|    total_timesteps | 364000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=364500, episode_reward=246.44 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 246      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 364500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.404   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72899    |\n",
      "|    policy_loss        | -0.492   |\n",
      "|    value_loss         | 1.12     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 72900  |\n",
      "|    time_elapsed    | 4074   |\n",
      "|    total_timesteps | 364500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=365000, episode_reward=220.14 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 220      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 365000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.418   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72999    |\n",
      "|    policy_loss        | -0.387   |\n",
      "|    value_loss         | 0.615    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 73000  |\n",
      "|    time_elapsed    | 4080   |\n",
      "|    total_timesteps | 365000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=365500, episode_reward=232.39 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 232      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 365500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.406   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73099    |\n",
      "|    policy_loss        | 3.81     |\n",
      "|    value_loss         | 70.7     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 73100  |\n",
      "|    time_elapsed    | 4086   |\n",
      "|    total_timesteps | 365500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=366000, episode_reward=59.17 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 59.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 366000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.417   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73199    |\n",
      "|    policy_loss        | -0.174   |\n",
      "|    value_loss         | 0.125    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 73200  |\n",
      "|    time_elapsed    | 4092   |\n",
      "|    total_timesteps | 366000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=366500, episode_reward=54.32 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 54.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 366500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.406   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73299    |\n",
      "|    policy_loss        | 0.0324   |\n",
      "|    value_loss         | 0.0112   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 73300  |\n",
      "|    time_elapsed    | 4098   |\n",
      "|    total_timesteps | 366500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=367000, episode_reward=55.23 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 55.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 367000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.288   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73399    |\n",
      "|    policy_loss        | 0.00782  |\n",
      "|    value_loss         | 0.012    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 73400  |\n",
      "|    time_elapsed    | 4105   |\n",
      "|    total_timesteps | 367000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=367500, episode_reward=247.70 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 248       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 367500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.384    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 73499     |\n",
      "|    policy_loss        | -0.121    |\n",
      "|    value_loss         | 0.0758    |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 73500  |\n",
      "|    time_elapsed    | 4111   |\n",
      "|    total_timesteps | 367500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=368000, episode_reward=245.73 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 246      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 368000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.245   |\n",
      "|    explained_variance | 2.38e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73599    |\n",
      "|    policy_loss        | 2.28     |\n",
      "|    value_loss         | 9.41     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 73600  |\n",
      "|    time_elapsed    | 4117   |\n",
      "|    total_timesteps | 368000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=368500, episode_reward=250.65 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 251       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 368500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.39     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 73699     |\n",
      "|    policy_loss        | 3.01      |\n",
      "|    value_loss         | 183       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 73700  |\n",
      "|    time_elapsed    | 4124   |\n",
      "|    total_timesteps | 368500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=369000, episode_reward=407.79 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 408      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 369000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.426   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73799    |\n",
      "|    policy_loss        | -0.0407  |\n",
      "|    value_loss         | 0.0182   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 73800  |\n",
      "|    time_elapsed    | 4130   |\n",
      "|    total_timesteps | 369000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=369500, episode_reward=406.05 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 406      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 369500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.429   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73899    |\n",
      "|    policy_loss        | 0.0778   |\n",
      "|    value_loss         | 0.0501   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 73900  |\n",
      "|    time_elapsed    | 4136   |\n",
      "|    total_timesteps | 369500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=246.48 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 246      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 370000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.418   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73999    |\n",
      "|    policy_loss        | -0.615   |\n",
      "|    value_loss         | 1.54     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 74000  |\n",
      "|    time_elapsed    | 4142   |\n",
      "|    total_timesteps | 370000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=370500, episode_reward=215.27 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 215      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 370500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.405   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74099    |\n",
      "|    policy_loss        | -0.29    |\n",
      "|    value_loss         | 0.832    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 74100  |\n",
      "|    time_elapsed    | 4149   |\n",
      "|    total_timesteps | 370500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=371000, episode_reward=245.96 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 246      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 371000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.372   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74199    |\n",
      "|    policy_loss        | -0.242   |\n",
      "|    value_loss         | 0.177    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 74200  |\n",
      "|    time_elapsed    | 4155   |\n",
      "|    total_timesteps | 371000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=371500, episode_reward=246.18 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 246      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 371500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.438   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74299    |\n",
      "|    policy_loss        | 4.76     |\n",
      "|    value_loss         | 102      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 74300  |\n",
      "|    time_elapsed    | 4161   |\n",
      "|    total_timesteps | 371500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=372000, episode_reward=247.24 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 247      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 372000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.401   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74399    |\n",
      "|    policy_loss        | 5.18     |\n",
      "|    value_loss         | 108      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 74400  |\n",
      "|    time_elapsed    | 4167   |\n",
      "|    total_timesteps | 372000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=372500, episode_reward=44.30 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 44.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 372500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.387   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74499    |\n",
      "|    policy_loss        | -0.0244  |\n",
      "|    value_loss         | 0.0507   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 74500  |\n",
      "|    time_elapsed    | 4174   |\n",
      "|    total_timesteps | 372500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=373000, episode_reward=244.78 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 245      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 373000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.377   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74599    |\n",
      "|    policy_loss        | -0.495   |\n",
      "|    value_loss         | 1.01     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 74600  |\n",
      "|    time_elapsed    | 4180   |\n",
      "|    total_timesteps | 373000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=373500, episode_reward=253.15 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 253      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 373500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.305   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74699    |\n",
      "|    policy_loss        | 1.51     |\n",
      "|    value_loss         | 178      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 74700  |\n",
      "|    time_elapsed    | 4186   |\n",
      "|    total_timesteps | 373500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=374000, episode_reward=248.89 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 249      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 374000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.354   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74799    |\n",
      "|    policy_loss        | -0.101   |\n",
      "|    value_loss         | 0.0736   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 74800  |\n",
      "|    time_elapsed    | 4193   |\n",
      "|    total_timesteps | 374000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=374500, episode_reward=245.54 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 246      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 374500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.318   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74899    |\n",
      "|    policy_loss        | -0.205   |\n",
      "|    value_loss         | 0.129    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 74900  |\n",
      "|    time_elapsed    | 4199   |\n",
      "|    total_timesteps | 374500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=375000, episode_reward=259.50 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 260       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 375000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.365    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 74999     |\n",
      "|    policy_loss        | 4.89      |\n",
      "|    value_loss         | 34.1      |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 75000  |\n",
      "|    time_elapsed    | 4205   |\n",
      "|    total_timesteps | 375000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=375500, episode_reward=244.29 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 244      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 375500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.401   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75099    |\n",
      "|    policy_loss        | -0.646   |\n",
      "|    value_loss         | 1.77     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 75100  |\n",
      "|    time_elapsed    | 4212   |\n",
      "|    total_timesteps | 375500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=376000, episode_reward=262.17 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 262       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 376000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.452    |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 75199     |\n",
      "|    policy_loss        | 7.35      |\n",
      "|    value_loss         | 55.5      |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 75200  |\n",
      "|    time_elapsed    | 4218   |\n",
      "|    total_timesteps | 376000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=376500, episode_reward=242.53 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 243      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 376500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.405   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75299    |\n",
      "|    policy_loss        | 2.76     |\n",
      "|    value_loss         | 200      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 75300  |\n",
      "|    time_elapsed    | 4224   |\n",
      "|    total_timesteps | 376500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=377000, episode_reward=245.42 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 245       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 377000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.298    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 75399     |\n",
      "|    policy_loss        | 0.447     |\n",
      "|    value_loss         | 18.1      |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 75400  |\n",
      "|    time_elapsed    | 4231   |\n",
      "|    total_timesteps | 377000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=377500, episode_reward=39.17 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 39.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 377500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.412    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 75499     |\n",
      "|    policy_loss        | -0.147    |\n",
      "|    value_loss         | 0.189     |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 75500  |\n",
      "|    time_elapsed    | 4237   |\n",
      "|    total_timesteps | 377500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=378000, episode_reward=243.02 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 243       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 378000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.341    |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 75599     |\n",
      "|    policy_loss        | -0.0382   |\n",
      "|    value_loss         | 0.0421    |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 75600  |\n",
      "|    time_elapsed    | 4243   |\n",
      "|    total_timesteps | 378000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=378500, episode_reward=244.77 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 245      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 378500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.359   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75699    |\n",
      "|    policy_loss        | -0.117   |\n",
      "|    value_loss         | 0.0642   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 75700  |\n",
      "|    time_elapsed    | 4250   |\n",
      "|    total_timesteps | 378500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=379000, episode_reward=274.79 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 275      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 379000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.394   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75799    |\n",
      "|    policy_loss        | 1.98     |\n",
      "|    value_loss         | 145      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 75800  |\n",
      "|    time_elapsed    | 4256   |\n",
      "|    total_timesteps | 379000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=379500, episode_reward=288.71 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 289      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 379500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.336   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75899    |\n",
      "|    policy_loss        | 0.00573  |\n",
      "|    value_loss         | 0.0106   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 75900  |\n",
      "|    time_elapsed    | 4262   |\n",
      "|    total_timesteps | 379500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=417.33 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 417      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 380000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.445   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75999    |\n",
      "|    policy_loss        | -0.645   |\n",
      "|    value_loss         | 2.34     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 76000  |\n",
      "|    time_elapsed    | 4268   |\n",
      "|    total_timesteps | 380000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=380500, episode_reward=274.29 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 274      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 380500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.346   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76099    |\n",
      "|    policy_loss        | 5.71     |\n",
      "|    value_loss         | 111      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 89     |\n",
      "|    iterations      | 76100  |\n",
      "|    time_elapsed    | 4275   |\n",
      "|    total_timesteps | 380500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=381000, episode_reward=269.89 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 270       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 381000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.327    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 76199     |\n",
      "|    policy_loss        | 6.23      |\n",
      "|    value_loss         | 135       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 76200  |\n",
      "|    time_elapsed    | 4281   |\n",
      "|    total_timesteps | 381000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=381500, episode_reward=273.26 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 273      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 381500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.423   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76299    |\n",
      "|    policy_loss        | 0.00374  |\n",
      "|    value_loss         | 0.0252   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 76300  |\n",
      "|    time_elapsed    | 4287   |\n",
      "|    total_timesteps | 381500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=382000, episode_reward=419.09 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 419      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 382000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.417   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76399    |\n",
      "|    policy_loss        | -0.119   |\n",
      "|    value_loss         | 0.0318   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 76400  |\n",
      "|    time_elapsed    | 4293   |\n",
      "|    total_timesteps | 382000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=382500, episode_reward=261.43 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 261      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 382500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.444   |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76499    |\n",
      "|    policy_loss        | 3.75     |\n",
      "|    value_loss         | 74.1     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 76500  |\n",
      "|    time_elapsed    | 4300   |\n",
      "|    total_timesteps | 382500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=383000, episode_reward=431.06 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 431      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 383000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.444   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76599    |\n",
      "|    policy_loss        | 2.52     |\n",
      "|    value_loss         | 34.7     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 76600  |\n",
      "|    time_elapsed    | 4306   |\n",
      "|    total_timesteps | 383000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=383500, episode_reward=267.88 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 268      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 383500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.5     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76699    |\n",
      "|    policy_loss        | -0.0499  |\n",
      "|    value_loss         | 0.0187   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 76700  |\n",
      "|    time_elapsed    | 4312   |\n",
      "|    total_timesteps | 383500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=384000, episode_reward=431.06 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 431       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 384000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.408    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 76799     |\n",
      "|    policy_loss        | 1.19      |\n",
      "|    value_loss         | 37.5      |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 76800  |\n",
      "|    time_elapsed    | 4319   |\n",
      "|    total_timesteps | 384000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=384500, episode_reward=409.38 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 409       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 384500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.261    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 76899     |\n",
      "|    policy_loss        | -0.0487   |\n",
      "|    value_loss         | 0.545     |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 76900  |\n",
      "|    time_elapsed    | 4325   |\n",
      "|    total_timesteps | 384500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=385000, episode_reward=260.72 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 261      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 385000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.454   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76999    |\n",
      "|    policy_loss        | 5.54     |\n",
      "|    value_loss         | 110      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 77000  |\n",
      "|    time_elapsed    | 4332   |\n",
      "|    total_timesteps | 385000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=385500, episode_reward=260.82 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 261      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 385500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.411   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77099    |\n",
      "|    policy_loss        | -0.0989  |\n",
      "|    value_loss         | 0.0364   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 77100  |\n",
      "|    time_elapsed    | 4338   |\n",
      "|    total_timesteps | 385500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=386000, episode_reward=279.89 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 280      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 386000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.302   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77199    |\n",
      "|    policy_loss        | 1.42     |\n",
      "|    value_loss         | 145      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 77200  |\n",
      "|    time_elapsed    | 4344   |\n",
      "|    total_timesteps | 386000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=386500, episode_reward=241.56 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 242      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 386500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.385   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77299    |\n",
      "|    policy_loss        | -0.473   |\n",
      "|    value_loss         | 0.745    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 77300  |\n",
      "|    time_elapsed    | 4350   |\n",
      "|    total_timesteps | 386500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=387000, episode_reward=63.59 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 63.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 387000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.418   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77399    |\n",
      "|    policy_loss        | -0.791   |\n",
      "|    value_loss         | 2.58     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 77400  |\n",
      "|    time_elapsed    | 4357   |\n",
      "|    total_timesteps | 387000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=387500, episode_reward=225.25 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 225      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 387500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.367   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77499    |\n",
      "|    policy_loss        | -0.123   |\n",
      "|    value_loss         | 0.312    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 77500  |\n",
      "|    time_elapsed    | 4363   |\n",
      "|    total_timesteps | 387500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=388000, episode_reward=254.86 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 255      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 388000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.392   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77599    |\n",
      "|    policy_loss        | -0.0468  |\n",
      "|    value_loss         | 0.00757  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 77600  |\n",
      "|    time_elapsed    | 4369   |\n",
      "|    total_timesteps | 388000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=388500, episode_reward=274.49 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 274      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 388500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.31    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77699    |\n",
      "|    policy_loss        | 1.24     |\n",
      "|    value_loss         | 143      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 77700  |\n",
      "|    time_elapsed    | 4375   |\n",
      "|    total_timesteps | 388500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=389000, episode_reward=267.28 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 267       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 389000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.379    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 77799     |\n",
      "|    policy_loss        | 2.72      |\n",
      "|    value_loss         | 143       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 77800  |\n",
      "|    time_elapsed    | 4382   |\n",
      "|    total_timesteps | 389000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=389500, episode_reward=430.57 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 431      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 389500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.461   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77899    |\n",
      "|    policy_loss        | -0.0446  |\n",
      "|    value_loss         | 0.0184   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 77900  |\n",
      "|    time_elapsed    | 4388   |\n",
      "|    total_timesteps | 389500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=268.68 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 269      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 390000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.378   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77999    |\n",
      "|    policy_loss        | 5.41     |\n",
      "|    value_loss         | 109      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 78000  |\n",
      "|    time_elapsed    | 4394   |\n",
      "|    total_timesteps | 390000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=390500, episode_reward=271.97 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 272      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 390500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.435   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78099    |\n",
      "|    policy_loss        | -0.137   |\n",
      "|    value_loss         | 0.0711   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 78100  |\n",
      "|    time_elapsed    | 4401   |\n",
      "|    total_timesteps | 390500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=391000, episode_reward=423.72 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 424       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 391000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.421    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 78199     |\n",
      "|    policy_loss        | 2.78      |\n",
      "|    value_loss         | 34.9      |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 78200  |\n",
      "|    time_elapsed    | 4407   |\n",
      "|    total_timesteps | 391000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=391500, episode_reward=421.67 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 422      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 391500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.35    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78299    |\n",
      "|    policy_loss        | -0.259   |\n",
      "|    value_loss         | 0.993    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 78300  |\n",
      "|    time_elapsed    | 4413   |\n",
      "|    total_timesteps | 391500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=392000, episode_reward=262.84 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 263      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 392000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.367   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78399    |\n",
      "|    policy_loss        | 1.97     |\n",
      "|    value_loss         | 139      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 78400  |\n",
      "|    time_elapsed    | 4419   |\n",
      "|    total_timesteps | 392000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=392500, episode_reward=263.31 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 263      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 392500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.274   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78499    |\n",
      "|    policy_loss        | 1.09     |\n",
      "|    value_loss         | 141      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 78500  |\n",
      "|    time_elapsed    | 4426   |\n",
      "|    total_timesteps | 392500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=393000, episode_reward=281.57 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 282      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 393000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.278   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78599    |\n",
      "|    policy_loss        | 1.33     |\n",
      "|    value_loss         | 210      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 78600  |\n",
      "|    time_elapsed    | 4432   |\n",
      "|    total_timesteps | 393000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=393500, episode_reward=287.10 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 287      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 393500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.247   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78699    |\n",
      "|    policy_loss        | 0.718    |\n",
      "|    value_loss         | 146      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 78700  |\n",
      "|    time_elapsed    | 4438   |\n",
      "|    total_timesteps | 393500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=394000, episode_reward=442.12 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 442      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 394000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.396   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78799    |\n",
      "|    policy_loss        | 0.0988   |\n",
      "|    value_loss         | 0.0977   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 78800  |\n",
      "|    time_elapsed    | 4444   |\n",
      "|    total_timesteps | 394000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=394500, episode_reward=275.86 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 276      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 394500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.182   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78899    |\n",
      "|    policy_loss        | 0.57     |\n",
      "|    value_loss         | 146      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 78900  |\n",
      "|    time_elapsed    | 4450   |\n",
      "|    total_timesteps | 394500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=395000, episode_reward=275.77 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 276      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 395000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.28    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78999    |\n",
      "|    policy_loss        | 0.00261  |\n",
      "|    value_loss         | 0.0238   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 79000  |\n",
      "|    time_elapsed    | 4457   |\n",
      "|    total_timesteps | 395000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=395500, episode_reward=275.41 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 275      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 395500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.33    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79099    |\n",
      "|    policy_loss        | -0.0835  |\n",
      "|    value_loss         | 0.0458   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 79100  |\n",
      "|    time_elapsed    | 4463   |\n",
      "|    total_timesteps | 395500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=396000, episode_reward=258.27 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 258      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 396000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.249   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79199    |\n",
      "|    policy_loss        | 1.16     |\n",
      "|    value_loss         | 160      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 79200  |\n",
      "|    time_elapsed    | 4469   |\n",
      "|    total_timesteps | 396000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=396500, episode_reward=259.81 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 260      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 396500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.337   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79299    |\n",
      "|    policy_loss        | -0.477   |\n",
      "|    value_loss         | 0.82     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 79300  |\n",
      "|    time_elapsed    | 4476   |\n",
      "|    total_timesteps | 396500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=397000, episode_reward=260.60 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 261      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 397000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.239   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79399    |\n",
      "|    policy_loss        | 0.975    |\n",
      "|    value_loss         | 144      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 79400  |\n",
      "|    time_elapsed    | 4482   |\n",
      "|    total_timesteps | 397000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=397500, episode_reward=238.06 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 238      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 397500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.418   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79499    |\n",
      "|    policy_loss        | 0.0387   |\n",
      "|    value_loss         | 0.0413   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 79500  |\n",
      "|    time_elapsed    | 4488   |\n",
      "|    total_timesteps | 397500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=398000, episode_reward=270.72 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 271      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 398000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.445   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79599    |\n",
      "|    policy_loss        | 0.0187   |\n",
      "|    value_loss         | 0.0234   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 79600  |\n",
      "|    time_elapsed    | 4495   |\n",
      "|    total_timesteps | 398000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=398500, episode_reward=441.80 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 442       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 398500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.417    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 79699     |\n",
      "|    policy_loss        | 0.0618    |\n",
      "|    value_loss         | 0.0258    |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 79700  |\n",
      "|    time_elapsed    | 4502   |\n",
      "|    total_timesteps | 398500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=399000, episode_reward=272.77 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 273      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 399000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.392   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79799    |\n",
      "|    policy_loss        | 8.56     |\n",
      "|    value_loss         | 140      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 79800  |\n",
      "|    time_elapsed    | 4508   |\n",
      "|    total_timesteps | 399000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=399500, episode_reward=267.00 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 267      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 399500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.438   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79899    |\n",
      "|    policy_loss        | -0.0938  |\n",
      "|    value_loss         | 0.0415   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 79900  |\n",
      "|    time_elapsed    | 4516   |\n",
      "|    total_timesteps | 399500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=433.95 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 434      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 400000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.459   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79999    |\n",
      "|    policy_loss        | 1.73     |\n",
      "|    value_loss         | 35.7     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 80000  |\n",
      "|    time_elapsed    | 4523   |\n",
      "|    total_timesteps | 400000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=400500, episode_reward=265.09 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 265       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 400500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.354    |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 80099     |\n",
      "|    policy_loss        | 1.7       |\n",
      "|    value_loss         | 143       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 80100  |\n",
      "|    time_elapsed    | 4529   |\n",
      "|    total_timesteps | 400500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=401000, episode_reward=271.71 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 272      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 401000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.363   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80199    |\n",
      "|    policy_loss        | 1.48     |\n",
      "|    value_loss         | 142      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 80200  |\n",
      "|    time_elapsed    | 4536   |\n",
      "|    total_timesteps | 401000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=401500, episode_reward=439.45 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 439       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 401500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.312    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 80299     |\n",
      "|    policy_loss        | 0.0304    |\n",
      "|    value_loss         | 0.00592   |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 80300  |\n",
      "|    time_elapsed    | 4543   |\n",
      "|    total_timesteps | 401500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=402000, episode_reward=419.62 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 420      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 402000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.328   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80399    |\n",
      "|    policy_loss        | 0.00442  |\n",
      "|    value_loss         | 0.00953  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 80400  |\n",
      "|    time_elapsed    | 4549   |\n",
      "|    total_timesteps | 402000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=402500, episode_reward=421.53 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 422      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 402500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.478   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80499    |\n",
      "|    policy_loss        | -0.438   |\n",
      "|    value_loss         | 0.972    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 80500  |\n",
      "|    time_elapsed    | 4556   |\n",
      "|    total_timesteps | 402500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=403000, episode_reward=434.58 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 435       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 403000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.463    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 80599     |\n",
      "|    policy_loss        | 3.84      |\n",
      "|    value_loss         | 76.1      |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 80600  |\n",
      "|    time_elapsed    | 4563   |\n",
      "|    total_timesteps | 403000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=403500, episode_reward=40.42 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 40.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 403500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0969  |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80699    |\n",
      "|    policy_loss        | -0.00505 |\n",
      "|    value_loss         | 0.0356   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 80700  |\n",
      "|    time_elapsed    | 4570   |\n",
      "|    total_timesteps | 403500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=404000, episode_reward=380.13 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 380      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 404000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.372   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80799    |\n",
      "|    policy_loss        | -0.0293  |\n",
      "|    value_loss         | 0.00741  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 80800  |\n",
      "|    time_elapsed    | 4577   |\n",
      "|    total_timesteps | 404000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=404500, episode_reward=245.52 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 246      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 404500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.425   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80899    |\n",
      "|    policy_loss        | 0.772    |\n",
      "|    value_loss         | 2.99     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 80900  |\n",
      "|    time_elapsed    | 4583   |\n",
      "|    total_timesteps | 404500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=405000, episode_reward=258.91 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 259      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 405000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.382   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80999    |\n",
      "|    policy_loss        | -0.555   |\n",
      "|    value_loss         | 0.994    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 81000  |\n",
      "|    time_elapsed    | 4590   |\n",
      "|    total_timesteps | 405000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=405500, episode_reward=248.22 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 248      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 405500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.238   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81099    |\n",
      "|    policy_loss        | 8.1      |\n",
      "|    value_loss         | 159      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 81100  |\n",
      "|    time_elapsed    | 4597   |\n",
      "|    total_timesteps | 405500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=406000, episode_reward=239.67 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 240       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 406000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.314    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 81199     |\n",
      "|    policy_loss        | -0.126    |\n",
      "|    value_loss         | 0.0679    |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 81200  |\n",
      "|    time_elapsed    | 4603   |\n",
      "|    total_timesteps | 406000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=406500, episode_reward=257.64 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 258       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 406500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.217    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 81299     |\n",
      "|    policy_loss        | 0.827     |\n",
      "|    value_loss         | 130       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 81300  |\n",
      "|    time_elapsed    | 4610   |\n",
      "|    total_timesteps | 406500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=407000, episode_reward=277.14 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 277      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 407000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.345   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81399    |\n",
      "|    policy_loss        | 11.4     |\n",
      "|    value_loss         | 45.7     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 81400  |\n",
      "|    time_elapsed    | 4617   |\n",
      "|    total_timesteps | 407000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=407500, episode_reward=262.36 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 262      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 407500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.474   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81499    |\n",
      "|    policy_loss        | 3.13     |\n",
      "|    value_loss         | 33.8     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 81500  |\n",
      "|    time_elapsed    | 4624   |\n",
      "|    total_timesteps | 407500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=408000, episode_reward=431.74 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 432      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 408000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.503   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81599    |\n",
      "|    policy_loss        | 1.64     |\n",
      "|    value_loss         | 36.8     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 81600  |\n",
      "|    time_elapsed    | 4630   |\n",
      "|    total_timesteps | 408000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=408500, episode_reward=264.33 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 264      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 408500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.468   |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81699    |\n",
      "|    policy_loss        | -0.39    |\n",
      "|    value_loss         | 1        |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 81700  |\n",
      "|    time_elapsed    | 4637   |\n",
      "|    total_timesteps | 408500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=409000, episode_reward=418.89 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 419      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 409000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.396   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81799    |\n",
      "|    policy_loss        | 0.0516   |\n",
      "|    value_loss         | 0.0624   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 81800  |\n",
      "|    time_elapsed    | 4644   |\n",
      "|    total_timesteps | 409000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=409500, episode_reward=269.67 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 270      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 409500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.523   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81899    |\n",
      "|    policy_loss        | -0.352   |\n",
      "|    value_loss         | 0.807    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 81900  |\n",
      "|    time_elapsed    | 4650   |\n",
      "|    total_timesteps | 409500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=277.79 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 278      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 410000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.656   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81999    |\n",
      "|    policy_loss        | -0.233   |\n",
      "|    value_loss         | 0.734    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 82000  |\n",
      "|    time_elapsed    | 4657   |\n",
      "|    total_timesteps | 410000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=410500, episode_reward=271.63 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 272      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 410500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.682   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82099    |\n",
      "|    policy_loss        | -0.0335  |\n",
      "|    value_loss         | 0.0453   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 88     |\n",
      "|    iterations      | 82100  |\n",
      "|    time_elapsed    | 4664   |\n",
      "|    total_timesteps | 410500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=411000, episode_reward=282.32 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 282      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 411000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.457   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82199    |\n",
      "|    policy_loss        | 0.0681   |\n",
      "|    value_loss         | 0.127    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 82200  |\n",
      "|    time_elapsed    | 4670   |\n",
      "|    total_timesteps | 411000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=411500, episode_reward=264.33 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 264      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 411500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.595   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82299    |\n",
      "|    policy_loss        | -0.28    |\n",
      "|    value_loss         | 0.691    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 82300  |\n",
      "|    time_elapsed    | 4677   |\n",
      "|    total_timesteps | 411500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=412000, episode_reward=424.15 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 424      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 412000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.304   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82399    |\n",
      "|    policy_loss        | 0.0384   |\n",
      "|    value_loss         | 0.131    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 82400  |\n",
      "|    time_elapsed    | 4684   |\n",
      "|    total_timesteps | 412000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=412500, episode_reward=417.74 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 418      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 412500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.514   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82499    |\n",
      "|    policy_loss        | 3.81     |\n",
      "|    value_loss         | 73.6     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 82500  |\n",
      "|    time_elapsed    | 4691   |\n",
      "|    total_timesteps | 412500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=413000, episode_reward=422.74 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 423      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 413000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.358   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82599    |\n",
      "|    policy_loss        | 0.022    |\n",
      "|    value_loss         | 0.0259   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 82600  |\n",
      "|    time_elapsed    | 4697   |\n",
      "|    total_timesteps | 413000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=413500, episode_reward=427.95 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 428      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 413500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.454   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82699    |\n",
      "|    policy_loss        | 0.0908   |\n",
      "|    value_loss         | 0.0352   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 82700  |\n",
      "|    time_elapsed    | 4704   |\n",
      "|    total_timesteps | 413500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=414000, episode_reward=428.31 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 428      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 414000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.438   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82799    |\n",
      "|    policy_loss        | -0.106   |\n",
      "|    value_loss         | 0.593    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 82800  |\n",
      "|    time_elapsed    | 4710   |\n",
      "|    total_timesteps | 414000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=414500, episode_reward=427.01 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 427      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 414500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.466   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82899    |\n",
      "|    policy_loss        | -0.0625  |\n",
      "|    value_loss         | 0.0338   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 82900  |\n",
      "|    time_elapsed    | 4716   |\n",
      "|    total_timesteps | 414500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=415000, episode_reward=254.81 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 255      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 415000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.443   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82999    |\n",
      "|    policy_loss        | 2.25     |\n",
      "|    value_loss         | 5.22     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 83000  |\n",
      "|    time_elapsed    | 4723   |\n",
      "|    total_timesteps | 415000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=415500, episode_reward=420.45 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 420      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 415500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.424   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83099    |\n",
      "|    policy_loss        | 0.11     |\n",
      "|    value_loss         | 0.0657   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 83100  |\n",
      "|    time_elapsed    | 4729   |\n",
      "|    total_timesteps | 415500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=416000, episode_reward=412.05 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 412      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 416000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.397   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83199    |\n",
      "|    policy_loss        | 0.128    |\n",
      "|    value_loss         | 0.079    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 83200  |\n",
      "|    time_elapsed    | 4735   |\n",
      "|    total_timesteps | 416000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=416500, episode_reward=204.40 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 204      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 416500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.342   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83299    |\n",
      "|    policy_loss        | -0.147   |\n",
      "|    value_loss         | 0.708    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 83300  |\n",
      "|    time_elapsed    | 4741   |\n",
      "|    total_timesteps | 416500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=417000, episode_reward=407.69 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 408      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 417000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.443   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83399    |\n",
      "|    policy_loss        | 2.1      |\n",
      "|    value_loss         | 36.9     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 83400  |\n",
      "|    time_elapsed    | 4748   |\n",
      "|    total_timesteps | 417000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=417500, episode_reward=423.60 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 424      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 417500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.313   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83499    |\n",
      "|    policy_loss        | -0.0197  |\n",
      "|    value_loss         | 0.508    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 83500  |\n",
      "|    time_elapsed    | 4754   |\n",
      "|    total_timesteps | 417500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=418000, episode_reward=421.35 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 421      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 418000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.533   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83599    |\n",
      "|    policy_loss        | 0.0681   |\n",
      "|    value_loss         | 0.0671   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 83600  |\n",
      "|    time_elapsed    | 4760   |\n",
      "|    total_timesteps | 418000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=418500, episode_reward=430.61 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 431       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 418500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.548    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 83699     |\n",
      "|    policy_loss        | 0.0903    |\n",
      "|    value_loss         | 0.0566    |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 83700  |\n",
      "|    time_elapsed    | 4766   |\n",
      "|    total_timesteps | 418500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=419000, episode_reward=398.47 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 398      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 419000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.421   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83799    |\n",
      "|    policy_loss        | -0.0489  |\n",
      "|    value_loss         | 0.0146   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 83800  |\n",
      "|    time_elapsed    | 4773   |\n",
      "|    total_timesteps | 419000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=419500, episode_reward=430.96 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 431      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 419500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.506   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83899    |\n",
      "|    policy_loss        | 0.0931   |\n",
      "|    value_loss         | 0.0818   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 83900  |\n",
      "|    time_elapsed    | 4779   |\n",
      "|    total_timesteps | 419500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=260.88 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 261      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 420000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.48    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83999    |\n",
      "|    policy_loss        | 4.37     |\n",
      "|    value_loss         | 80.2     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 84000  |\n",
      "|    time_elapsed    | 4785   |\n",
      "|    total_timesteps | 420000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=420500, episode_reward=258.50 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 258       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 420500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.423    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 84099     |\n",
      "|    policy_loss        | -0.269    |\n",
      "|    value_loss         | 0.615     |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 84100  |\n",
      "|    time_elapsed    | 4792   |\n",
      "|    total_timesteps | 420500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=421000, episode_reward=260.63 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 261       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 421000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.35     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 84199     |\n",
      "|    policy_loss        | 1.71      |\n",
      "|    value_loss         | 188       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 84200  |\n",
      "|    time_elapsed    | 4798   |\n",
      "|    total_timesteps | 421000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=421500, episode_reward=260.79 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 261      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 421500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.429   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84299    |\n",
      "|    policy_loss        | 3.3      |\n",
      "|    value_loss         | 32.2     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 84300  |\n",
      "|    time_elapsed    | 4804   |\n",
      "|    total_timesteps | 421500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=422000, episode_reward=232.37 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 232      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 422000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.341   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84399    |\n",
      "|    policy_loss        | 0.0186   |\n",
      "|    value_loss         | 0.0274   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 84400  |\n",
      "|    time_elapsed    | 4811   |\n",
      "|    total_timesteps | 422000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=422500, episode_reward=27.47 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 27.5      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 422500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.176    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 84499     |\n",
      "|    policy_loss        | 0.00044   |\n",
      "|    value_loss         | 0.00735   |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 84500  |\n",
      "|    time_elapsed    | 4817   |\n",
      "|    total_timesteps | 422500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=423000, episode_reward=401.64 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 402      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 423000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.417   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84599    |\n",
      "|    policy_loss        | -0.0351  |\n",
      "|    value_loss         | 0.0107   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 84600  |\n",
      "|    time_elapsed    | 4824   |\n",
      "|    total_timesteps | 423000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=423500, episode_reward=226.31 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 226      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 423500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.355   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84699    |\n",
      "|    policy_loss        | 0.047    |\n",
      "|    value_loss         | 0.0937   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 84700  |\n",
      "|    time_elapsed    | 4831   |\n",
      "|    total_timesteps | 423500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=424000, episode_reward=419.83 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 420      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 424000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.432   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84799    |\n",
      "|    policy_loss        | -0.661   |\n",
      "|    value_loss         | 1.48     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 84800  |\n",
      "|    time_elapsed    | 4837   |\n",
      "|    total_timesteps | 424000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=424500, episode_reward=220.50 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 220       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 424500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.267    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 84899     |\n",
      "|    policy_loss        | 0.0267    |\n",
      "|    value_loss         | 0.0939    |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 84900  |\n",
      "|    time_elapsed    | 4844   |\n",
      "|    total_timesteps | 424500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=425000, episode_reward=243.53 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 244      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 425000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.281   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84999    |\n",
      "|    policy_loss        | 0.0262   |\n",
      "|    value_loss         | 0.0817   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 85000  |\n",
      "|    time_elapsed    | 4850   |\n",
      "|    total_timesteps | 425000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=425500, episode_reward=435.82 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 436      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 425500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.443   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85099    |\n",
      "|    policy_loss        | 3.24     |\n",
      "|    value_loss         | 7.14     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 85100  |\n",
      "|    time_elapsed    | 4857   |\n",
      "|    total_timesteps | 425500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=426000, episode_reward=436.95 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 437      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 426000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.412   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85199    |\n",
      "|    policy_loss        | 0.026    |\n",
      "|    value_loss         | 0.0116   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 85200  |\n",
      "|    time_elapsed    | 4863   |\n",
      "|    total_timesteps | 426000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=426500, episode_reward=266.38 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 266      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 426500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.465   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85299    |\n",
      "|    policy_loss        | -0.0598  |\n",
      "|    value_loss         | 0.0757   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 85300  |\n",
      "|    time_elapsed    | 4869   |\n",
      "|    total_timesteps | 426500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=427000, episode_reward=270.11 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 270       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 427000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.466    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 85399     |\n",
      "|    policy_loss        | 0.0785    |\n",
      "|    value_loss         | 0.0797    |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 85400  |\n",
      "|    time_elapsed    | 4876   |\n",
      "|    total_timesteps | 427000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=427500, episode_reward=274.47 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 274       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 427500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.406    |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 85499     |\n",
      "|    policy_loss        | 2.88      |\n",
      "|    value_loss         | 162       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 85500  |\n",
      "|    time_elapsed    | 4882   |\n",
      "|    total_timesteps | 427500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=428000, episode_reward=266.82 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 267      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 428000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.45    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85599    |\n",
      "|    policy_loss        | -0.347   |\n",
      "|    value_loss         | 0.694    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 85600  |\n",
      "|    time_elapsed    | 4888   |\n",
      "|    total_timesteps | 428000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=428500, episode_reward=269.74 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 270      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 428500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.43    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85699    |\n",
      "|    policy_loss        | -0.376   |\n",
      "|    value_loss         | 0.765    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 85700  |\n",
      "|    time_elapsed    | 4894   |\n",
      "|    total_timesteps | 428500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=429000, episode_reward=242.87 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 243      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 429000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.328   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85799    |\n",
      "|    policy_loss        | 0.0393   |\n",
      "|    value_loss         | 0.0896   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 85800  |\n",
      "|    time_elapsed    | 4901   |\n",
      "|    total_timesteps | 429000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=429500, episode_reward=50.07 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 50.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 429500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.303   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85899    |\n",
      "|    policy_loss        | 0.0468   |\n",
      "|    value_loss         | 0.158    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 85900  |\n",
      "|    time_elapsed    | 4907   |\n",
      "|    total_timesteps | 429500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=68.39 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 68.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 430000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.402   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85999    |\n",
      "|    policy_loss        | -0.05    |\n",
      "|    value_loss         | 0.0391   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 86000  |\n",
      "|    time_elapsed    | 4913   |\n",
      "|    total_timesteps | 430000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=430500, episode_reward=268.74 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 269      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 430500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.362   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86099    |\n",
      "|    policy_loss        | 0.0677   |\n",
      "|    value_loss         | 0.0465   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 86100  |\n",
      "|    time_elapsed    | 4919   |\n",
      "|    total_timesteps | 430500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=431000, episode_reward=277.57 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 278      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 431000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.448   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86199    |\n",
      "|    policy_loss        | 0.505    |\n",
      "|    value_loss         | 0.551    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 86200  |\n",
      "|    time_elapsed    | 4926   |\n",
      "|    total_timesteps | 431000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=431500, episode_reward=264.75 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 265      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 431500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.41    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86299    |\n",
      "|    policy_loss        | 3        |\n",
      "|    value_loss         | 224      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 86300  |\n",
      "|    time_elapsed    | 4933   |\n",
      "|    total_timesteps | 431500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=432000, episode_reward=268.08 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 268      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 432000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.297   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86399    |\n",
      "|    policy_loss        | 6.07     |\n",
      "|    value_loss         | 46       |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 86400  |\n",
      "|    time_elapsed    | 4939   |\n",
      "|    total_timesteps | 432000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=432500, episode_reward=275.81 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 276      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 432500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.451   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86499    |\n",
      "|    policy_loss        | 0.181    |\n",
      "|    value_loss         | 0.123    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 86500  |\n",
      "|    time_elapsed    | 4946   |\n",
      "|    total_timesteps | 432500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=433000, episode_reward=254.99 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 255       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 433000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.27     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 86599     |\n",
      "|    policy_loss        | 1.47      |\n",
      "|    value_loss         | 170       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 86600  |\n",
      "|    time_elapsed    | 4953   |\n",
      "|    total_timesteps | 433000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=433500, episode_reward=258.50 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 259      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 433500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.198   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86699    |\n",
      "|    policy_loss        | 0.732    |\n",
      "|    value_loss         | 167      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 86700  |\n",
      "|    time_elapsed    | 4959   |\n",
      "|    total_timesteps | 433500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=434000, episode_reward=265.98 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 266      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 434000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.245   |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86799    |\n",
      "|    policy_loss        | 0.763    |\n",
      "|    value_loss         | 145      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 86800  |\n",
      "|    time_elapsed    | 4966   |\n",
      "|    total_timesteps | 434000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=434500, episode_reward=249.19 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 249      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 434500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.234   |\n",
      "|    explained_variance | 2.38e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86899    |\n",
      "|    policy_loss        | 1.26     |\n",
      "|    value_loss         | 251      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 86900  |\n",
      "|    time_elapsed    | 4973   |\n",
      "|    total_timesteps | 434500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=435000, episode_reward=401.74 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 402      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 435000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.42    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86999    |\n",
      "|    policy_loss        | 0.0266   |\n",
      "|    value_loss         | 0.0112   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 87000  |\n",
      "|    time_elapsed    | 4979   |\n",
      "|    total_timesteps | 435000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=435500, episode_reward=227.98 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 228      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 435500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.257   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87099    |\n",
      "|    policy_loss        | 7.11     |\n",
      "|    value_loss         | 116      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 87100  |\n",
      "|    time_elapsed    | 4985   |\n",
      "|    total_timesteps | 435500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=436000, episode_reward=251.61 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 252      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 436000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.275   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87199    |\n",
      "|    policy_loss        | -0.656   |\n",
      "|    value_loss         | 0.689    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 87200  |\n",
      "|    time_elapsed    | 4992   |\n",
      "|    total_timesteps | 436000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=436500, episode_reward=261.82 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 262      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 436500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.239   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87299    |\n",
      "|    policy_loss        | -0.0322  |\n",
      "|    value_loss         | 0.0537   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 87300  |\n",
      "|    time_elapsed    | 4998   |\n",
      "|    total_timesteps | 436500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=437000, episode_reward=259.91 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 260       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 437000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.209    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 87399     |\n",
      "|    policy_loss        | 6.95      |\n",
      "|    value_loss         | 112       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 87400  |\n",
      "|    time_elapsed    | 5005   |\n",
      "|    total_timesteps | 437000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=437500, episode_reward=254.64 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 255       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 437500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.351    |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 87499     |\n",
      "|    policy_loss        | 4.2       |\n",
      "|    value_loss         | 37.6      |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 87500  |\n",
      "|    time_elapsed    | 5011   |\n",
      "|    total_timesteps | 437500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=438000, episode_reward=249.98 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 250      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 438000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.46    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87599    |\n",
      "|    policy_loss        | 5.98     |\n",
      "|    value_loss         | 219      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 87600  |\n",
      "|    time_elapsed    | 5017   |\n",
      "|    total_timesteps | 438000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=438500, episode_reward=273.49 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 273      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 438500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.427   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87699    |\n",
      "|    policy_loss        | 1.9      |\n",
      "|    value_loss         | 151      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 87700  |\n",
      "|    time_elapsed    | 5024   |\n",
      "|    total_timesteps | 438500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=439000, episode_reward=253.38 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 253      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 439000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.426   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87799    |\n",
      "|    policy_loss        | 0.162    |\n",
      "|    value_loss         | 0.183    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 87800  |\n",
      "|    time_elapsed    | 5030   |\n",
      "|    total_timesteps | 439000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=439500, episode_reward=261.40 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 261      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 439500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.414   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87899    |\n",
      "|    policy_loss        | 2.26     |\n",
      "|    value_loss         | 144      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 87900  |\n",
      "|    time_elapsed    | 5037   |\n",
      "|    total_timesteps | 439500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=259.00 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 259      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 440000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.433   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87999    |\n",
      "|    policy_loss        | 1.87     |\n",
      "|    value_loss         | 14.1     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 88000  |\n",
      "|    time_elapsed    | 5043   |\n",
      "|    total_timesteps | 440000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=440500, episode_reward=431.43 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 431      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 440500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.333   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88099    |\n",
      "|    policy_loss        | 0.126    |\n",
      "|    value_loss         | 0.099    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 88100  |\n",
      "|    time_elapsed    | 5050   |\n",
      "|    total_timesteps | 440500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=441000, episode_reward=261.80 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 262      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 441000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.425   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88199    |\n",
      "|    policy_loss        | 0.0282   |\n",
      "|    value_loss         | 0.00736  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 88200  |\n",
      "|    time_elapsed    | 5056   |\n",
      "|    total_timesteps | 441000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=441500, episode_reward=424.24 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 424      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 441500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.395   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88299    |\n",
      "|    policy_loss        | 0.00401  |\n",
      "|    value_loss         | 0.0277   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 88300  |\n",
      "|    time_elapsed    | 5062   |\n",
      "|    total_timesteps | 441500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=442000, episode_reward=431.63 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 432      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 442000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.525   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88399    |\n",
      "|    policy_loss        | -0.0823  |\n",
      "|    value_loss         | 0.0524   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 88400  |\n",
      "|    time_elapsed    | 5069   |\n",
      "|    total_timesteps | 442000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=442500, episode_reward=219.42 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 219      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 442500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0591  |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88499    |\n",
      "|    policy_loss        | -0.0011  |\n",
      "|    value_loss         | 0.00635  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 88500  |\n",
      "|    time_elapsed    | 5075   |\n",
      "|    total_timesteps | 442500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=443000, episode_reward=264.87 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 265       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 443000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.428    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 88599     |\n",
      "|    policy_loss        | -0.331    |\n",
      "|    value_loss         | 0.759     |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 88600  |\n",
      "|    time_elapsed    | 5082   |\n",
      "|    total_timesteps | 443000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=443500, episode_reward=256.44 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 256       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 443500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.419    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 88699     |\n",
      "|    policy_loss        | -0.366    |\n",
      "|    value_loss         | 0.708     |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 88700  |\n",
      "|    time_elapsed    | 5088   |\n",
      "|    total_timesteps | 443500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=444000, episode_reward=250.05 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 250      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 444000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.42    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88799    |\n",
      "|    policy_loss        | -0.0137  |\n",
      "|    value_loss         | 0.0203   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 88800  |\n",
      "|    time_elapsed    | 5094   |\n",
      "|    total_timesteps | 444000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=444500, episode_reward=250.05 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 250      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 444500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.403   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88899    |\n",
      "|    policy_loss        | -0.0178  |\n",
      "|    value_loss         | 0.0705   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 88900  |\n",
      "|    time_elapsed    | 5101   |\n",
      "|    total_timesteps | 444500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=445000, episode_reward=245.34 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 245       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 445000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.403    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 88999     |\n",
      "|    policy_loss        | 0.911     |\n",
      "|    value_loss         | 4.38      |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 89000  |\n",
      "|    time_elapsed    | 5107   |\n",
      "|    total_timesteps | 445000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=445500, episode_reward=414.05 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 414      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 445500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.417   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89099    |\n",
      "|    policy_loss        | -0.312   |\n",
      "|    value_loss         | 0.35     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 89100  |\n",
      "|    time_elapsed    | 5113   |\n",
      "|    total_timesteps | 445500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=446000, episode_reward=430.95 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 431      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 446000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.409   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89199    |\n",
      "|    policy_loss        | -0.146   |\n",
      "|    value_loss         | 0.549    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 89200  |\n",
      "|    time_elapsed    | 5120   |\n",
      "|    total_timesteps | 446000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=446500, episode_reward=262.10 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 262      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 446500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.34    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89299    |\n",
      "|    policy_loss        | 1.86     |\n",
      "|    value_loss         | 145      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 89300  |\n",
      "|    time_elapsed    | 5126   |\n",
      "|    total_timesteps | 446500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=447000, episode_reward=424.16 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 424       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 447000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.422    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 89399     |\n",
      "|    policy_loss        | 2.06      |\n",
      "|    value_loss         | 56.1      |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 89400  |\n",
      "|    time_elapsed    | 5132   |\n",
      "|    total_timesteps | 447000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=447500, episode_reward=429.85 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 430      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 447500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.354   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89499    |\n",
      "|    policy_loss        | 0.0301   |\n",
      "|    value_loss         | 0.0462   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 89500  |\n",
      "|    time_elapsed    | 5139   |\n",
      "|    total_timesteps | 447500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=448000, episode_reward=263.93 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 264      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 448000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.415   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89599    |\n",
      "|    policy_loss        | -0.415   |\n",
      "|    value_loss         | 0.898    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 89600  |\n",
      "|    time_elapsed    | 5145   |\n",
      "|    total_timesteps | 448000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=448500, episode_reward=264.45 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 264      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 448500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.419   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89699    |\n",
      "|    policy_loss        | -0.61    |\n",
      "|    value_loss         | 1.5      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 89700  |\n",
      "|    time_elapsed    | 5151   |\n",
      "|    total_timesteps | 448500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=449000, episode_reward=262.62 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 263      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 449000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.42    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89799    |\n",
      "|    policy_loss        | -0.297   |\n",
      "|    value_loss         | 0.6      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 89800  |\n",
      "|    time_elapsed    | 5158   |\n",
      "|    total_timesteps | 449000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=449500, episode_reward=263.85 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 264       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 449500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.398    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 89899     |\n",
      "|    policy_loss        | 0.028     |\n",
      "|    value_loss         | 0.061     |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 89900  |\n",
      "|    time_elapsed    | 5164   |\n",
      "|    total_timesteps | 449500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=424.78 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 425      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 450000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.416   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89999    |\n",
      "|    policy_loss        | 0.243    |\n",
      "|    value_loss         | 0.29     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 90000  |\n",
      "|    time_elapsed    | 5171   |\n",
      "|    total_timesteps | 450000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=450500, episode_reward=261.70 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 262      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 450500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.388   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90099    |\n",
      "|    policy_loss        | -0.0252  |\n",
      "|    value_loss         | 0.0189   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 87     |\n",
      "|    iterations      | 90100  |\n",
      "|    time_elapsed    | 5177   |\n",
      "|    total_timesteps | 450500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=451000, episode_reward=419.29 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 419       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 451000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.407    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 90199     |\n",
      "|    policy_loss        | 1.35      |\n",
      "|    value_loss         | 38.7      |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 90200  |\n",
      "|    time_elapsed    | 5184   |\n",
      "|    total_timesteps | 451000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=451500, episode_reward=431.96 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 432      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 451500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.323   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90299    |\n",
      "|    policy_loss        | -0.00243 |\n",
      "|    value_loss         | 0.00468  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 90300  |\n",
      "|    time_elapsed    | 5190   |\n",
      "|    total_timesteps | 451500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=452000, episode_reward=246.41 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 246      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 452000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.412   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90399    |\n",
      "|    policy_loss        | -0.108   |\n",
      "|    value_loss         | 0.524    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 90400  |\n",
      "|    time_elapsed    | 5197   |\n",
      "|    total_timesteps | 452000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=452500, episode_reward=253.00 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 253      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 452500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.411   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90499    |\n",
      "|    policy_loss        | 2.74     |\n",
      "|    value_loss         | 38.9     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 90500  |\n",
      "|    time_elapsed    | 5203   |\n",
      "|    total_timesteps | 452500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=453000, episode_reward=429.99 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 430      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 453000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.432   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90599    |\n",
      "|    policy_loss        | 0.0964   |\n",
      "|    value_loss         | 0.0986   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 90600  |\n",
      "|    time_elapsed    | 5210   |\n",
      "|    total_timesteps | 453000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=453500, episode_reward=221.19 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 221       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 453500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.232    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 90699     |\n",
      "|    policy_loss        | 1         |\n",
      "|    value_loss         | 2.17      |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 90700  |\n",
      "|    time_elapsed    | 5216   |\n",
      "|    total_timesteps | 453500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=454000, episode_reward=262.02 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 262      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 454000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.37    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90799    |\n",
      "|    policy_loss        | -0.0122  |\n",
      "|    value_loss         | 0.0185   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 90800  |\n",
      "|    time_elapsed    | 5223   |\n",
      "|    total_timesteps | 454000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=454500, episode_reward=254.53 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 255      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 454500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.413   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90899    |\n",
      "|    policy_loss        | -0.116   |\n",
      "|    value_loss         | 0.0874   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 90900  |\n",
      "|    time_elapsed    | 5230   |\n",
      "|    total_timesteps | 454500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=455000, episode_reward=254.02 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 254      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 455000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.407   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90999    |\n",
      "|    policy_loss        | 0.0918   |\n",
      "|    value_loss         | 0.0605   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 91000  |\n",
      "|    time_elapsed    | 5236   |\n",
      "|    total_timesteps | 455000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=455500, episode_reward=267.43 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 267      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 455500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.353   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91099    |\n",
      "|    policy_loss        | 4.62     |\n",
      "|    value_loss         | 76.3     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 91100  |\n",
      "|    time_elapsed    | 5242   |\n",
      "|    total_timesteps | 455500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=456000, episode_reward=255.07 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 255      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 456000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.372   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91199    |\n",
      "|    policy_loss        | 0.925    |\n",
      "|    value_loss         | 17.1     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 91200  |\n",
      "|    time_elapsed    | 5249   |\n",
      "|    total_timesteps | 456000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=456500, episode_reward=255.37 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 255      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 456500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.316   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91299    |\n",
      "|    policy_loss        | 6.49     |\n",
      "|    value_loss         | 140      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 91300  |\n",
      "|    time_elapsed    | 5255   |\n",
      "|    total_timesteps | 456500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=457000, episode_reward=250.56 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 251      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 457000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.397   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91399    |\n",
      "|    policy_loss        | 0.0503   |\n",
      "|    value_loss         | 0.0985   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 91400  |\n",
      "|    time_elapsed    | 5261   |\n",
      "|    total_timesteps | 457000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=457500, episode_reward=272.48 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 272      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 457500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.423   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91499    |\n",
      "|    policy_loss        | -0.0436  |\n",
      "|    value_loss         | 0.0459   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 91500  |\n",
      "|    time_elapsed    | 5268   |\n",
      "|    total_timesteps | 457500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=458000, episode_reward=278.01 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 278       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 458000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.419    |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 91599     |\n",
      "|    policy_loss        | 4.53      |\n",
      "|    value_loss         | 104       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 91600  |\n",
      "|    time_elapsed    | 5274   |\n",
      "|    total_timesteps | 458000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=458500, episode_reward=268.44 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 268      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 458500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.394   |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91699    |\n",
      "|    policy_loss        | 6.3      |\n",
      "|    value_loss         | 161      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 91700  |\n",
      "|    time_elapsed    | 5281   |\n",
      "|    total_timesteps | 458500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=459000, episode_reward=446.07 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 446       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 459000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.427    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 91799     |\n",
      "|    policy_loss        | 0.0324    |\n",
      "|    value_loss         | 0.0345    |\n",
      "-------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 91800  |\n",
      "|    time_elapsed    | 5287   |\n",
      "|    total_timesteps | 459000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=459500, episode_reward=270.09 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 270       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 459500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.398    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 91899     |\n",
      "|    policy_loss        | 2.86      |\n",
      "|    value_loss         | 147       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 91900  |\n",
      "|    time_elapsed    | 5293   |\n",
      "|    total_timesteps | 459500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=228.76 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 229      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 460000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.309   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91999    |\n",
      "|    policy_loss        | -0.154   |\n",
      "|    value_loss         | 0.76     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 92000  |\n",
      "|    time_elapsed    | 5300   |\n",
      "|    total_timesteps | 460000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=460500, episode_reward=51.78 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 51.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 460500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.249   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92099    |\n",
      "|    policy_loss        | 0.134    |\n",
      "|    value_loss         | 0.0862   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 92100  |\n",
      "|    time_elapsed    | 5306   |\n",
      "|    total_timesteps | 460500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=461000, episode_reward=268.29 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 268      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 461000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.439   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92199    |\n",
      "|    policy_loss        | -0.0224  |\n",
      "|    value_loss         | 0.0194   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 92200  |\n",
      "|    time_elapsed    | 5313   |\n",
      "|    total_timesteps | 461000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=461500, episode_reward=268.44 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 268      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 461500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.434   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92299    |\n",
      "|    policy_loss        | 5.3      |\n",
      "|    value_loss         | 215      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 92300  |\n",
      "|    time_elapsed    | 5319   |\n",
      "|    total_timesteps | 461500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=462000, episode_reward=86.46 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 86.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 462000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.239   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92399    |\n",
      "|    policy_loss        | -0.289   |\n",
      "|    value_loss         | 0.103    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 92400  |\n",
      "|    time_elapsed    | 5326   |\n",
      "|    total_timesteps | 462000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=462500, episode_reward=268.44 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 268      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 462500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.418   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92499    |\n",
      "|    policy_loss        | 0.0385   |\n",
      "|    value_loss         | 0.0983   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 92500  |\n",
      "|    time_elapsed    | 5332   |\n",
      "|    total_timesteps | 462500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=463000, episode_reward=267.25 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 267      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 463000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.406   |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92599    |\n",
      "|    policy_loss        | 3.52     |\n",
      "|    value_loss         | 48.8     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 92600  |\n",
      "|    time_elapsed    | 5339   |\n",
      "|    total_timesteps | 463000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=463500, episode_reward=429.53 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 430      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 463500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.327   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92699    |\n",
      "|    policy_loss        | -0.191   |\n",
      "|    value_loss         | 0.0762   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 92700  |\n",
      "|    time_elapsed    | 5345   |\n",
      "|    total_timesteps | 463500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=464000, episode_reward=261.97 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 262      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 464000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.47    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92799    |\n",
      "|    policy_loss        | -0.429   |\n",
      "|    value_loss         | 0.799    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 92800  |\n",
      "|    time_elapsed    | 5351   |\n",
      "|    total_timesteps | 464000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=464500, episode_reward=260.49 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 260      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 464500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.506   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92899    |\n",
      "|    policy_loss        | 5.4      |\n",
      "|    value_loss         | 54.6     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 92900  |\n",
      "|    time_elapsed    | 5357   |\n",
      "|    total_timesteps | 464500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=465000, episode_reward=437.56 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 438      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 465000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.444   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92999    |\n",
      "|    policy_loss        | -0.013   |\n",
      "|    value_loss         | 1.64     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 93000  |\n",
      "|    time_elapsed    | 5364   |\n",
      "|    total_timesteps | 465000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=465500, episode_reward=268.69 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 269      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 465500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.354   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93099    |\n",
      "|    policy_loss        | 5.73     |\n",
      "|    value_loss         | 107      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 93100  |\n",
      "|    time_elapsed    | 5370   |\n",
      "|    total_timesteps | 465500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=466000, episode_reward=281.15 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 281      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 466000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.421   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93199    |\n",
      "|    policy_loss        | 6.66     |\n",
      "|    value_loss         | 176      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 93200  |\n",
      "|    time_elapsed    | 5376   |\n",
      "|    total_timesteps | 466000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=466500, episode_reward=271.90 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 272      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 466500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.4     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93299    |\n",
      "|    policy_loss        | 0.9      |\n",
      "|    value_loss         | 3.38     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 93300  |\n",
      "|    time_elapsed    | 5383   |\n",
      "|    total_timesteps | 466500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=467000, episode_reward=267.57 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 268      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 467000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.384   |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93399    |\n",
      "|    policy_loss        | 4.07     |\n",
      "|    value_loss         | 57.2     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 93400  |\n",
      "|    time_elapsed    | 5389   |\n",
      "|    total_timesteps | 467000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=467500, episode_reward=436.95 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 437      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 467500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.424   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93499    |\n",
      "|    policy_loss        | 0.238    |\n",
      "|    value_loss         | 0.323    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 93500  |\n",
      "|    time_elapsed    | 5396   |\n",
      "|    total_timesteps | 467500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=468000, episode_reward=268.44 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 268      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 468000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.409   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93599    |\n",
      "|    policy_loss        | 3.98     |\n",
      "|    value_loss         | 153      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 93600  |\n",
      "|    time_elapsed    | 5402   |\n",
      "|    total_timesteps | 468000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=468500, episode_reward=268.72 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 269       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 468500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.383    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 93699     |\n",
      "|    policy_loss        | 0.229     |\n",
      "|    value_loss         | 0.462     |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 93700  |\n",
      "|    time_elapsed    | 5408   |\n",
      "|    total_timesteps | 468500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=469000, episode_reward=268.72 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 269      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 469000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.398   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93799    |\n",
      "|    policy_loss        | 6.32     |\n",
      "|    value_loss         | 163      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 93800  |\n",
      "|    time_elapsed    | 5414   |\n",
      "|    total_timesteps | 469000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=469500, episode_reward=269.80 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 270      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 469500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.354   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93899    |\n",
      "|    policy_loss        | 1.91     |\n",
      "|    value_loss         | 151      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 93900  |\n",
      "|    time_elapsed    | 5421   |\n",
      "|    total_timesteps | 469500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=268.38 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 268      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 470000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.319   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93999    |\n",
      "|    policy_loss        | 0.0147   |\n",
      "|    value_loss         | 0.061    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 94000  |\n",
      "|    time_elapsed    | 5427   |\n",
      "|    total_timesteps | 470000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=470500, episode_reward=436.84 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 437      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 470500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.413   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94099    |\n",
      "|    policy_loss        | 0.134    |\n",
      "|    value_loss         | 0.117    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 94100  |\n",
      "|    time_elapsed    | 5433   |\n",
      "|    total_timesteps | 470500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=471000, episode_reward=443.35 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 443      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 471000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.416   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94199    |\n",
      "|    policy_loss        | 0.18     |\n",
      "|    value_loss         | 0.218    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 94200  |\n",
      "|    time_elapsed    | 5439   |\n",
      "|    total_timesteps | 471000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=471500, episode_reward=268.38 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 268      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 471500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.398   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94299    |\n",
      "|    policy_loss        | 0.197    |\n",
      "|    value_loss         | 0.277    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 94300  |\n",
      "|    time_elapsed    | 5446   |\n",
      "|    total_timesteps | 471500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=472000, episode_reward=268.72 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 269      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 472000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.408   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94399    |\n",
      "|    policy_loss        | -0.0295  |\n",
      "|    value_loss         | 0.0531   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 94400  |\n",
      "|    time_elapsed    | 5452   |\n",
      "|    total_timesteps | 472000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=472500, episode_reward=266.88 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 267       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 472500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.329    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 94499     |\n",
      "|    policy_loss        | 0.679     |\n",
      "|    value_loss         | 19.8      |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 94500  |\n",
      "|    time_elapsed    | 5458   |\n",
      "|    total_timesteps | 472500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=473000, episode_reward=264.21 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 264       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 473000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.344    |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 94599     |\n",
      "|    policy_loss        | 1.97      |\n",
      "|    value_loss         | 151       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 94600  |\n",
      "|    time_elapsed    | 5465   |\n",
      "|    total_timesteps | 473000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=473500, episode_reward=267.75 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 268       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 473500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.245    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 94699     |\n",
      "|    policy_loss        | 6.76      |\n",
      "|    value_loss         | 115       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 94700  |\n",
      "|    time_elapsed    | 5471   |\n",
      "|    total_timesteps | 473500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=474000, episode_reward=429.29 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 429      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 474000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.415   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94799    |\n",
      "|    policy_loss        | 2.58     |\n",
      "|    value_loss         | 38.7     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 94800  |\n",
      "|    time_elapsed    | 5477   |\n",
      "|    total_timesteps | 474000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=474500, episode_reward=269.68 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 270      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 474500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.422   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94899    |\n",
      "|    policy_loss        | -0.138   |\n",
      "|    value_loss         | 0.0695   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 94900  |\n",
      "|    time_elapsed    | 5484   |\n",
      "|    total_timesteps | 474500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=475000, episode_reward=437.23 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 437       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 475000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.41     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 94999     |\n",
      "|    policy_loss        | 2.45      |\n",
      "|    value_loss         | 35        |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 95000  |\n",
      "|    time_elapsed    | 5491   |\n",
      "|    total_timesteps | 475000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=475500, episode_reward=264.32 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 264      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 475500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.288   |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95099    |\n",
      "|    policy_loss        | 1.21     |\n",
      "|    value_loss         | 147      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 95100  |\n",
      "|    time_elapsed    | 5497   |\n",
      "|    total_timesteps | 475500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=476000, episode_reward=414.93 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 415      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 476000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.384   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95199    |\n",
      "|    policy_loss        | 0.0188   |\n",
      "|    value_loss         | 0.0208   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 95200  |\n",
      "|    time_elapsed    | 5503   |\n",
      "|    total_timesteps | 476000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=476500, episode_reward=265.45 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 265       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 476500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.441    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 95299     |\n",
      "|    policy_loss        | 4.56      |\n",
      "|    value_loss         | 96.2      |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 95300  |\n",
      "|    time_elapsed    | 5509   |\n",
      "|    total_timesteps | 476500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=477000, episode_reward=267.17 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 267      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 477000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.403   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95399    |\n",
      "|    policy_loss        | -0.0877  |\n",
      "|    value_loss         | 0.0523   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 95400  |\n",
      "|    time_elapsed    | 5515   |\n",
      "|    total_timesteps | 477000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=477500, episode_reward=266.31 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 266      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 477500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.416   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95499    |\n",
      "|    policy_loss        | -0.0104  |\n",
      "|    value_loss         | 0.0192   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 95500  |\n",
      "|    time_elapsed    | 5521   |\n",
      "|    total_timesteps | 477500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=478000, episode_reward=269.24 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 269      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 478000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.447   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95599    |\n",
      "|    policy_loss        | 0.148    |\n",
      "|    value_loss         | 0.146    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 95600  |\n",
      "|    time_elapsed    | 5527   |\n",
      "|    total_timesteps | 478000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=478500, episode_reward=277.41 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 277      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 478500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.426   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95699    |\n",
      "|    policy_loss        | 0.0068   |\n",
      "|    value_loss         | 0.0169   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 95700  |\n",
      "|    time_elapsed    | 5533   |\n",
      "|    total_timesteps | 478500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=479000, episode_reward=279.55 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 280      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 479000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.401   |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95799    |\n",
      "|    policy_loss        | -0.357   |\n",
      "|    value_loss         | 0.679    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 95800  |\n",
      "|    time_elapsed    | 5539   |\n",
      "|    total_timesteps | 479000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=479500, episode_reward=278.90 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 279       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 479500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.422    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 95899     |\n",
      "|    policy_loss        | 7.8       |\n",
      "|    value_loss         | 229       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 95900  |\n",
      "|    time_elapsed    | 5545   |\n",
      "|    total_timesteps | 479500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=266.65 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 267      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 480000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.413   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95999    |\n",
      "|    policy_loss        | 2.79     |\n",
      "|    value_loss         | 36.2     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 96000  |\n",
      "|    time_elapsed    | 5551   |\n",
      "|    total_timesteps | 480000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=480500, episode_reward=416.07 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 416      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 480500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.409   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96099    |\n",
      "|    policy_loss        | 0.133    |\n",
      "|    value_loss         | 0.256    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 96100  |\n",
      "|    time_elapsed    | 5557   |\n",
      "|    total_timesteps | 480500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=481000, episode_reward=267.29 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 267      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 481000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.422   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96199    |\n",
      "|    policy_loss        | 0.15     |\n",
      "|    value_loss         | 0.154    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 96200  |\n",
      "|    time_elapsed    | 5563   |\n",
      "|    total_timesteps | 481000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=481500, episode_reward=254.56 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 255       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 481500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.34     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 96299     |\n",
      "|    policy_loss        | 2.24      |\n",
      "|    value_loss         | 147       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 96300  |\n",
      "|    time_elapsed    | 5569   |\n",
      "|    total_timesteps | 481500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=482000, episode_reward=394.42 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 394      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 482000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.414   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96399    |\n",
      "|    policy_loss        | 0.17     |\n",
      "|    value_loss         | 0.178    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 96400  |\n",
      "|    time_elapsed    | 5576   |\n",
      "|    total_timesteps | 482000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=482500, episode_reward=229.64 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 230      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 482500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.399   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96499    |\n",
      "|    policy_loss        | 0.125    |\n",
      "|    value_loss         | 0.114    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 96500  |\n",
      "|    time_elapsed    | 5582   |\n",
      "|    total_timesteps | 482500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=483000, episode_reward=267.80 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 268      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 483000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.367   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96599    |\n",
      "|    policy_loss        | 4.96     |\n",
      "|    value_loss         | 96.7     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 96600  |\n",
      "|    time_elapsed    | 5588   |\n",
      "|    total_timesteps | 483000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=483500, episode_reward=401.48 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 401      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 483500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.414   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96699    |\n",
      "|    policy_loss        | 0.114    |\n",
      "|    value_loss         | 0.154    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 96700  |\n",
      "|    time_elapsed    | 5594   |\n",
      "|    total_timesteps | 483500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=484000, episode_reward=269.53 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 270      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 484000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.399   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96799    |\n",
      "|    policy_loss        | 5.25     |\n",
      "|    value_loss         | 112      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 96800  |\n",
      "|    time_elapsed    | 5600   |\n",
      "|    total_timesteps | 484000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=484500, episode_reward=258.30 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 258      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 484500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.419   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96899    |\n",
      "|    policy_loss        | 0.00481  |\n",
      "|    value_loss         | 0.0121   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 96900  |\n",
      "|    time_elapsed    | 5606   |\n",
      "|    total_timesteps | 484500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=485000, episode_reward=281.66 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 282       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 485000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.419    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 96999     |\n",
      "|    policy_loss        | 5.73      |\n",
      "|    value_loss         | 138       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 97000  |\n",
      "|    time_elapsed    | 5612   |\n",
      "|    total_timesteps | 485000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=485500, episode_reward=270.05 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 270       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 485500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.329    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 97099     |\n",
      "|    policy_loss        | 5.19      |\n",
      "|    value_loss         | 75        |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 97100  |\n",
      "|    time_elapsed    | 5618   |\n",
      "|    total_timesteps | 485500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=486000, episode_reward=279.23 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 279      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 486000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.419   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97199    |\n",
      "|    policy_loss        | -0.00605 |\n",
      "|    value_loss         | 0.0331   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 97200  |\n",
      "|    time_elapsed    | 5624   |\n",
      "|    total_timesteps | 486000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=486500, episode_reward=270.83 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 271       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 486500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.403    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 97299     |\n",
      "|    policy_loss        | 2.91      |\n",
      "|    value_loss         | 154       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 97300  |\n",
      "|    time_elapsed    | 5630   |\n",
      "|    total_timesteps | 486500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=487000, episode_reward=268.61 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 269      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 487000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.427   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97399    |\n",
      "|    policy_loss        | 0.28     |\n",
      "|    value_loss         | 0.43     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 97400  |\n",
      "|    time_elapsed    | 5635   |\n",
      "|    total_timesteps | 487000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=487500, episode_reward=268.61 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 269      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 487500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.42    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97499    |\n",
      "|    policy_loss        | 0.0683   |\n",
      "|    value_loss         | 0.0462   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 97500  |\n",
      "|    time_elapsed    | 5641   |\n",
      "|    total_timesteps | 487500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=488000, episode_reward=260.59 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 261      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 488000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.424   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97599    |\n",
      "|    policy_loss        | 4.38     |\n",
      "|    value_loss         | 145      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 97600  |\n",
      "|    time_elapsed    | 5647   |\n",
      "|    total_timesteps | 488000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=488500, episode_reward=267.04 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 267      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 488500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.442   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97699    |\n",
      "|    policy_loss        | -0.501   |\n",
      "|    value_loss         | 2.73     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 97700  |\n",
      "|    time_elapsed    | 5653   |\n",
      "|    total_timesteps | 488500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=489000, episode_reward=255.47 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 255      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 489000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.486   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97799    |\n",
      "|    policy_loss        | -0.107   |\n",
      "|    value_loss         | 0.049    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 97800  |\n",
      "|    time_elapsed    | 5659   |\n",
      "|    total_timesteps | 489000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=489500, episode_reward=266.53 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 267      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 489500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.413   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97899    |\n",
      "|    policy_loss        | 5.19     |\n",
      "|    value_loss         | 109      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 97900  |\n",
      "|    time_elapsed    | 5665   |\n",
      "|    total_timesteps | 489500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=267.55 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 268      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 490000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.376   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97999    |\n",
      "|    policy_loss        | -0.543   |\n",
      "|    value_loss         | 0.881    |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 98000  |\n",
      "|    time_elapsed    | 5671   |\n",
      "|    total_timesteps | 490000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=490500, episode_reward=269.27 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 269      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 490500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.325   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98099    |\n",
      "|    policy_loss        | 4.93     |\n",
      "|    value_loss         | 76.9     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 98100  |\n",
      "|    time_elapsed    | 5678   |\n",
      "|    total_timesteps | 490500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=491000, episode_reward=273.23 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 273      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 491000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.324   |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98199    |\n",
      "|    policy_loss        | 1.7      |\n",
      "|    value_loss         | 146      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 98200  |\n",
      "|    time_elapsed    | 5684   |\n",
      "|    total_timesteps | 491000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=491500, episode_reward=428.97 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 429      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 491500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.311   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98299    |\n",
      "|    policy_loss        | 0.0929   |\n",
      "|    value_loss         | 0.0705   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 98300  |\n",
      "|    time_elapsed    | 5690   |\n",
      "|    total_timesteps | 491500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=492000, episode_reward=441.61 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 442      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 492000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.353   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98399    |\n",
      "|    policy_loss        | 0.122    |\n",
      "|    value_loss         | 0.0905   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 98400  |\n",
      "|    time_elapsed    | 5696   |\n",
      "|    total_timesteps | 492000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=492500, episode_reward=276.45 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 276       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 492500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.411    |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 98499     |\n",
      "|    policy_loss        | 5.9       |\n",
      "|    value_loss         | 142       |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 98500  |\n",
      "|    time_elapsed    | 5702   |\n",
      "|    total_timesteps | 492500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=493000, episode_reward=266.96 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 267      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 493000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.417   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98599    |\n",
      "|    policy_loss        | 5.32     |\n",
      "|    value_loss         | 115      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 98600  |\n",
      "|    time_elapsed    | 5708   |\n",
      "|    total_timesteps | 493000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=493500, episode_reward=442.25 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 442      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 493500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.422   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98699    |\n",
      "|    policy_loss        | -0.749   |\n",
      "|    value_loss         | 2.6      |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 98700  |\n",
      "|    time_elapsed    | 5714   |\n",
      "|    total_timesteps | 493500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=494000, episode_reward=442.25 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 442      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 494000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.414   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98799    |\n",
      "|    policy_loss        | 0.0567   |\n",
      "|    value_loss         | 0.0535   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 98800  |\n",
      "|    time_elapsed    | 5720   |\n",
      "|    total_timesteps | 494000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=494500, episode_reward=437.95 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 438      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 494500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.266   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98899    |\n",
      "|    policy_loss        | -0.119   |\n",
      "|    value_loss         | 0.0514   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 98900  |\n",
      "|    time_elapsed    | 5726   |\n",
      "|    total_timesteps | 494500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=495000, episode_reward=272.58 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 273      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 495000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.418   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98999    |\n",
      "|    policy_loss        | -0.124   |\n",
      "|    value_loss         | 0.0591   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 99000  |\n",
      "|    time_elapsed    | 5732   |\n",
      "|    total_timesteps | 495000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=495500, episode_reward=267.80 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 268       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 495500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.379    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99099     |\n",
      "|    policy_loss        | -0.705    |\n",
      "|    value_loss         | 2.05      |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 99100  |\n",
      "|    time_elapsed    | 5738   |\n",
      "|    total_timesteps | 495500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=496000, episode_reward=268.60 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 269      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 496000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.354   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99199    |\n",
      "|    policy_loss        | -0.13    |\n",
      "|    value_loss         | 0.0779   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 99200  |\n",
      "|    time_elapsed    | 5744   |\n",
      "|    total_timesteps | 496000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=496500, episode_reward=260.14 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.74e+03  |\n",
      "|    mean_reward        | 260       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 496500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.402    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99299     |\n",
      "|    policy_loss        | -0.418    |\n",
      "|    value_loss         | 0.788     |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 99300  |\n",
      "|    time_elapsed    | 5750   |\n",
      "|    total_timesteps | 496500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=497000, episode_reward=268.08 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 268      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 497000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.372   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99399    |\n",
      "|    policy_loss        | 4.32     |\n",
      "|    value_loss         | 75.6     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 99400  |\n",
      "|    time_elapsed    | 5756   |\n",
      "|    total_timesteps | 497000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=497500, episode_reward=268.60 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 269      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 497500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.329   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99499    |\n",
      "|    policy_loss        | 0.0571   |\n",
      "|    value_loss         | 0.0524   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 99500  |\n",
      "|    time_elapsed    | 5762   |\n",
      "|    total_timesteps | 497500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=498000, episode_reward=268.43 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 268      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 498000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.375   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99599    |\n",
      "|    policy_loss        | 3.36     |\n",
      "|    value_loss         | 36.5     |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 99600  |\n",
      "|    time_elapsed    | 5768   |\n",
      "|    total_timesteps | 498000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=498500, episode_reward=436.95 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 437      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 498500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.427   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99699    |\n",
      "|    policy_loss        | -0.0973  |\n",
      "|    value_loss         | 0.0314   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 99700  |\n",
      "|    time_elapsed    | 5774   |\n",
      "|    total_timesteps | 498500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=499000, episode_reward=430.20 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 430      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 499000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.25    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99799    |\n",
      "|    policy_loss        | -0.0044  |\n",
      "|    value_loss         | 0.00674  |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 99800  |\n",
      "|    time_elapsed    | 5780   |\n",
      "|    total_timesteps | 499000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=499500, episode_reward=429.43 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 429      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 499500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.281   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99899    |\n",
      "|    policy_loss        | 0.00842  |\n",
      "|    value_loss         | 0.0192   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 99900  |\n",
      "|    time_elapsed    | 5786   |\n",
      "|    total_timesteps | 499500 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=436.34 +/- 0.00\n",
      "Episode length: 2737.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.74e+03 |\n",
      "|    mean_reward        | 436      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 500000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.323   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99999    |\n",
      "|    policy_loss        | 0.00721  |\n",
      "|    value_loss         | 0.0135   |\n",
      "------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 86     |\n",
      "|    iterations      | 100000 |\n",
      "|    time_elapsed    | 5792   |\n",
      "|    total_timesteps | 500000 |\n",
      "-------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x784e79ddbbb0>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 103
  },
  {
   "cell_type": "code",
   "source": "model.save('RL_del_gran_pisky_model_callback_2.h5')",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T04:41:54.403575Z",
     "start_time": "2024-04-18T04:41:53.867167Z"
    }
   },
   "id": "40ee6249f9dedbc8",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[104], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mRL_del_gran_pisky_model_callback_2.h5\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:841\u001B[0m, in \u001B[0;36mBaseAlgorithm.save\u001B[0;34m(self, path, exclude, include)\u001B[0m\n\u001B[1;32m    838\u001B[0m \u001B[38;5;66;03m# Build dict of state_dicts\u001B[39;00m\n\u001B[1;32m    839\u001B[0m params_to_save \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_parameters()\n\u001B[0;32m--> 841\u001B[0m \u001B[43msave_to_zip_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams_to_save\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpytorch_variables\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpytorch_variables\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:328\u001B[0m, in \u001B[0;36msave_to_zip_file\u001B[0;34m(save_path, data, params, pytorch_variables, verbose)\u001B[0m\n\u001B[1;32m    326\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m file_name, dict_ \u001B[38;5;129;01min\u001B[39;00m params\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m archive\u001B[38;5;241m.\u001B[39mopen(file_name \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m, force_zip64\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m param_file:\n\u001B[0;32m--> 328\u001B[0m             \u001B[43mth\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdict_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam_file\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    329\u001B[0m \u001B[38;5;66;03m# Save metadata: library version when file was saved\u001B[39;00m\n\u001B[1;32m    330\u001B[0m archive\u001B[38;5;241m.\u001B[39mwritestr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_stable_baselines3_version\u001B[39m\u001B[38;5;124m\"\u001B[39m, sb3\u001B[38;5;241m.\u001B[39m__version__)\n",
      "File \u001B[0;32m~/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/torch/serialization.py:629\u001B[0m, in \u001B[0;36msave\u001B[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001B[0m\n\u001B[1;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _use_new_zipfile_serialization:\n\u001B[1;32m    628\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m _open_zipfile_writer(f) \u001B[38;5;28;01mas\u001B[39;00m opened_zipfile:\n\u001B[0;32m--> 629\u001B[0m         \u001B[43m_save\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopened_zipfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpickle_module\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpickle_protocol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_disable_byteorder_record\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    630\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/torch/serialization.py:860\u001B[0m, in \u001B[0;36m_save\u001B[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001B[0m\n\u001B[1;32m    856\u001B[0m \u001B[38;5;66;03m# given that we copy things around anyway, we might use storage.cpu()\u001B[39;00m\n\u001B[1;32m    857\u001B[0m \u001B[38;5;66;03m# this means to that to get tensors serialized, you need to implement\u001B[39;00m\n\u001B[1;32m    858\u001B[0m \u001B[38;5;66;03m# .cpu() on the underlying Storage\u001B[39;00m\n\u001B[1;32m    859\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m storage\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m--> 860\u001B[0m     storage \u001B[38;5;241m=\u001B[39m \u001B[43mstorage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    861\u001B[0m \u001B[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001B[39;00m\n\u001B[1;32m    862\u001B[0m num_bytes \u001B[38;5;241m=\u001B[39m storage\u001B[38;5;241m.\u001B[39mnbytes()\n",
      "File \u001B[0;32m~/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/torch/storage.py:135\u001B[0m, in \u001B[0;36m_StorageBase.cpu\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Return a CPU copy of this storage if it's not already on the CPU.\"\"\"\u001B[39;00m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m--> 135\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mUntypedStorage\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    136\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    137\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39b34dbe4dbeaf7a"
  },
  {
   "cell_type": "code",
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "#load model\n",
    "from stable_baselines3 import A2C\n",
    "env_test = DummyVecEnv([lambda: env])\n",
    "model = A2C.load('./logs_v2/best_model.zip')\n",
    "mean_reward_trained = evaluate_policy(model, env_test, n_eval_episodes=10)\n",
    "mean_reward_trained"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T07:30:34.729027Z",
     "start_time": "2024-04-18T07:30:23.600686Z"
    }
   },
   "id": "602239b2e2c64da3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(446.07014799999996, 5.684341886080802e-14)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T07:30:59.674166Z",
     "start_time": "2024-04-18T07:30:48.644385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Crear el entorno de entrenamiento\n",
    "env_train = DummyVecEnv([lambda: env])\n",
    "model_untrained = A2C('MlpPolicy', env_train, verbose=1)\n",
    "mean_reward_untrained = evaluate_policy(model_untrained, env_test, n_eval_episodes=10)\n",
    "mean_reward_untrained"
   ],
   "id": "450586c409f23a27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(348.077097, 0.0)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 108
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "data = yf.download('NFLX', start='2012-01-01', end= '2024-01-01',interval=\"1d\")\n",
    "prices = np.array(data['Adj Close'].values) #Adj Close prices\n",
    "dates = np.array(data.index.values).astype('datetime64[ns]') #Dates\n",
    "data['rsi'] = TA.RSI(data,14,column='adj close') #Relative Strength Index (RSI)\n",
    "data['macd'] = TA.MACD(data,column='adj close')['MACD'] #MACD Line\n",
    "data['macd_signal'] = TA.MACD(data, column='adj close')['SIGNAL'] #MACD Signal Line\n",
    "# data['bb_bbm'] = TA.BBANDS(data, column='adj close')['BB_MIDDLE'] #Bollinger Bands (BB) middle band (BBM)\n",
    "data['bb_bbu'] = TA.BBANDS(data, column='adj close')['BB_UPPER'] #Bollinger Bands (BB) upper band (BBU)\n",
    "data['bb_bbl'] = TA.BBANDS(data,column='adj close')['BB_LOWER'] #Bollinger Bands (BB) lower band (BBL)\n",
    "# data['bb_width'] = TA.BBWIDTH(data,column='adj close') #Bollinger Bands (BB) width\n",
    "data['obv'] = TA.OBV(data,'adj close') #On Balance Volume (OBV)\n",
    "\n",
    "data.fillna(0, inplace=True)\n",
    "data = data[data.index > '2013-01-01']\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T07:31:05.890698Z",
     "start_time": "2024-04-18T07:31:05.805407Z"
    }
   },
   "id": "2b31ada5b219068b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "Date                                                                     \n",
       "2013-01-02   13.601429   13.687143   12.955714   13.144286   13.144286   \n",
       "2013-01-03   13.138571   13.988571   13.075714   13.798571   13.798571   \n",
       "2013-01-04   13.791429   13.958571   13.648571   13.711429   13.711429   \n",
       "2013-01-07   13.770000   14.535714   13.731429   14.171429   14.171429   \n",
       "2013-01-08   14.287143   14.427143   13.828571   13.880000   13.880000   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "2023-12-22  494.000000  496.019989  485.450012  486.760010  486.760010   \n",
       "2023-12-26  489.390015  491.480011  486.380005  491.190002  491.190002   \n",
       "2023-12-27  491.239990  494.019989  489.250000  491.790009  491.790009   \n",
       "2023-12-28  492.000000  492.890015  489.070007  490.510010  490.510010   \n",
       "2023-12-29  490.369995  492.230011  481.940002  486.880005  486.880005   \n",
       "\n",
       "              Volume        rsi       macd  macd_signal      bb_bbu  \\\n",
       "Date                                                                  \n",
       "2013-01-02  19431300  59.786015   0.409583     0.493112   13.904607   \n",
       "2013-01-03  27912500  66.916605   0.443505     0.483191   14.023396   \n",
       "2013-01-04  17761100  65.256940   0.458076     0.478168   14.042879   \n",
       "2013-01-07  45550400  69.550191   0.500967     0.482728   14.192091   \n",
       "2013-01-08  24714900  64.142371   0.505614     0.487305   14.239750   \n",
       "...              ...        ...        ...          ...         ...   \n",
       "2023-12-22   2701100  62.629263  11.455084    10.464237  500.795062   \n",
       "2023-12-26   2034500  64.613544  11.575034    10.686397  502.529506   \n",
       "2023-12-27   2561300  64.885498  11.584966    10.866111  504.295854   \n",
       "2023-12-28   1710500  63.759744  11.358617    10.964612  505.904987   \n",
       "2023-12-29   2739500  60.551250  10.762262    10.924142  507.105229   \n",
       "\n",
       "                bb_bbl           obv  \n",
       "Date                                  \n",
       "2013-01-02   11.868393  1.391383e+08  \n",
       "2013-01-03   11.891604  1.670508e+08  \n",
       "2013-01-04   12.052264  1.492897e+08  \n",
       "2013-01-07   12.089195  1.948401e+08  \n",
       "2013-01-08   12.201250  1.701252e+08  \n",
       "...                ...           ...  \n",
       "2023-12-22  442.219937  1.362563e+09  \n",
       "2023-12-26  441.687492  1.364598e+09  \n",
       "2023-12-27  441.200145  1.367159e+09  \n",
       "2023-12-28  440.923012  1.365448e+09  \n",
       "2023-12-29  441.013771  1.362709e+09  \n",
       "\n",
       "[2768 rows x 12 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>rsi</th>\n",
       "      <th>macd</th>\n",
       "      <th>macd_signal</th>\n",
       "      <th>bb_bbu</th>\n",
       "      <th>bb_bbl</th>\n",
       "      <th>obv</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-02</th>\n",
       "      <td>13.601429</td>\n",
       "      <td>13.687143</td>\n",
       "      <td>12.955714</td>\n",
       "      <td>13.144286</td>\n",
       "      <td>13.144286</td>\n",
       "      <td>19431300</td>\n",
       "      <td>59.786015</td>\n",
       "      <td>0.409583</td>\n",
       "      <td>0.493112</td>\n",
       "      <td>13.904607</td>\n",
       "      <td>11.868393</td>\n",
       "      <td>1.391383e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-03</th>\n",
       "      <td>13.138571</td>\n",
       "      <td>13.988571</td>\n",
       "      <td>13.075714</td>\n",
       "      <td>13.798571</td>\n",
       "      <td>13.798571</td>\n",
       "      <td>27912500</td>\n",
       "      <td>66.916605</td>\n",
       "      <td>0.443505</td>\n",
       "      <td>0.483191</td>\n",
       "      <td>14.023396</td>\n",
       "      <td>11.891604</td>\n",
       "      <td>1.670508e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-04</th>\n",
       "      <td>13.791429</td>\n",
       "      <td>13.958571</td>\n",
       "      <td>13.648571</td>\n",
       "      <td>13.711429</td>\n",
       "      <td>13.711429</td>\n",
       "      <td>17761100</td>\n",
       "      <td>65.256940</td>\n",
       "      <td>0.458076</td>\n",
       "      <td>0.478168</td>\n",
       "      <td>14.042879</td>\n",
       "      <td>12.052264</td>\n",
       "      <td>1.492897e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-07</th>\n",
       "      <td>13.770000</td>\n",
       "      <td>14.535714</td>\n",
       "      <td>13.731429</td>\n",
       "      <td>14.171429</td>\n",
       "      <td>14.171429</td>\n",
       "      <td>45550400</td>\n",
       "      <td>69.550191</td>\n",
       "      <td>0.500967</td>\n",
       "      <td>0.482728</td>\n",
       "      <td>14.192091</td>\n",
       "      <td>12.089195</td>\n",
       "      <td>1.948401e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-08</th>\n",
       "      <td>14.287143</td>\n",
       "      <td>14.427143</td>\n",
       "      <td>13.828571</td>\n",
       "      <td>13.880000</td>\n",
       "      <td>13.880000</td>\n",
       "      <td>24714900</td>\n",
       "      <td>64.142371</td>\n",
       "      <td>0.505614</td>\n",
       "      <td>0.487305</td>\n",
       "      <td>14.239750</td>\n",
       "      <td>12.201250</td>\n",
       "      <td>1.701252e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-22</th>\n",
       "      <td>494.000000</td>\n",
       "      <td>496.019989</td>\n",
       "      <td>485.450012</td>\n",
       "      <td>486.760010</td>\n",
       "      <td>486.760010</td>\n",
       "      <td>2701100</td>\n",
       "      <td>62.629263</td>\n",
       "      <td>11.455084</td>\n",
       "      <td>10.464237</td>\n",
       "      <td>500.795062</td>\n",
       "      <td>442.219937</td>\n",
       "      <td>1.362563e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-26</th>\n",
       "      <td>489.390015</td>\n",
       "      <td>491.480011</td>\n",
       "      <td>486.380005</td>\n",
       "      <td>491.190002</td>\n",
       "      <td>491.190002</td>\n",
       "      <td>2034500</td>\n",
       "      <td>64.613544</td>\n",
       "      <td>11.575034</td>\n",
       "      <td>10.686397</td>\n",
       "      <td>502.529506</td>\n",
       "      <td>441.687492</td>\n",
       "      <td>1.364598e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-27</th>\n",
       "      <td>491.239990</td>\n",
       "      <td>494.019989</td>\n",
       "      <td>489.250000</td>\n",
       "      <td>491.790009</td>\n",
       "      <td>491.790009</td>\n",
       "      <td>2561300</td>\n",
       "      <td>64.885498</td>\n",
       "      <td>11.584966</td>\n",
       "      <td>10.866111</td>\n",
       "      <td>504.295854</td>\n",
       "      <td>441.200145</td>\n",
       "      <td>1.367159e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-28</th>\n",
       "      <td>492.000000</td>\n",
       "      <td>492.890015</td>\n",
       "      <td>489.070007</td>\n",
       "      <td>490.510010</td>\n",
       "      <td>490.510010</td>\n",
       "      <td>1710500</td>\n",
       "      <td>63.759744</td>\n",
       "      <td>11.358617</td>\n",
       "      <td>10.964612</td>\n",
       "      <td>505.904987</td>\n",
       "      <td>440.923012</td>\n",
       "      <td>1.365448e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-29</th>\n",
       "      <td>490.369995</td>\n",
       "      <td>492.230011</td>\n",
       "      <td>481.940002</td>\n",
       "      <td>486.880005</td>\n",
       "      <td>486.880005</td>\n",
       "      <td>2739500</td>\n",
       "      <td>60.551250</td>\n",
       "      <td>10.762262</td>\n",
       "      <td>10.924142</td>\n",
       "      <td>507.105229</td>\n",
       "      <td>441.013771</td>\n",
       "      <td>1.362709e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2768 rows × 12 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 109
  },
  {
   "cell_type": "code",
   "source": [
    "from gym_anytrading.envs import Actions\n",
    "\n",
    "env_test = customEnv(df=data, window_size=30, frame_bound=(30,len(data)))\n",
    "\n",
    "obs,_ = env_test.reset(seed=42)\n",
    "env_test.trade_fee_ask_percent = 0\n",
    "env_test.trade_fee_bid_percent = 0\n",
    "\n",
    "r= []\n",
    "action_stats = {Actions.Sell: 0, Actions.Buy: 0}\n",
    "buy_rsi = []\n",
    "sell_rsi = []\n",
    "lt=0\n",
    "while True:\n",
    "    actual_lt= env_test._last_trade_tick\n",
    "    obs = obs[np.newaxis, ...] \n",
    "    action, _states = model.predict(obs)\n",
    "    # print(env_test._last_trade_tick)\n",
    "    # break\n",
    "    # print('skipped',action)\n",
    "    if lt!=actual_lt:\n",
    "        # print(action,\"Last trade tick:\",actual_lt)\n",
    "        lt=actual_lt\n",
    "        \n",
    "        # print(action)\n",
    "    action_stats[Actions(action)] += 1\n",
    "    if action == 0:\n",
    "        # print(\"Sell\")\n",
    "        # sell_rsi.append()\n",
    "        # sell_rsi.append(env_test.signal_features[env_test._last_trade_tick][0])\n",
    "        pass\n",
    "\n",
    "        pass\n",
    "    elif action == 1:\n",
    "        # print(\"Buy\")\n",
    "        # buy_rsi.append(env_test.signal_features[env_test._last_trade_tick][0])\n",
    "        pass\n",
    "    else:\n",
    "        print('??')        \n",
    "    obs, rewards,_ , done, info= env_test.step(action)\n",
    "\n",
    "\n",
    "    \n",
    "    r.append(rewards)\n",
    "    if done:\n",
    "        print(\"info\",info)\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T07:42:26.327079Z",
     "start_time": "2024-04-18T07:42:25.095210Z"
    }
   },
   "id": "cba5553d42041bf1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info {'total_reward': 479.8472156524658, 'total_profit': 22.80287807526222, 'position': <Positions.Long: 1>}\n"
     ]
    }
   ],
   "execution_count": 195
  },
  {
   "cell_type": "code",
   "source": [
    "env_test.max_possible_profit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T07:42:28.937739Z",
     "start_time": "2024-04-18T07:42:28.928558Z"
    }
   },
   "id": "2b9db17990b38f35",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "599970977911.104"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 196
  },
  {
   "cell_type": "markdown",
   "source": [
    "OPTUNA"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dda4864de6c1e737"
  },
  {
   "cell_type": "code",
   "source": [
    "env_test.signal_features[env_test._current_tick] #obtener los valores del dia que se va a predecir"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T02:51:29.374889Z",
     "start_time": "2024-04-18T02:51:29.370185Z"
    }
   },
   "id": "5cdb4f322a0b1c2d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.31542854e+01, 6.49043524e+01, 1.55876000e+07, 2.09906265e+00,\n",
       "       2.38018980e+00, 6.44261905e+01, 5.92709523e+01, 1.08207050e+09])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"env information:\")\n",
    "print(\"> shape:\", env_test.unwrapped.shape)\n",
    "print(\"> df.shape:\", env_test.unwrapped.df.shape)\n",
    "print(\"> prices.shape:\", env_test.unwrapped.prices.shape)\n",
    "print(\"> signal_features.shape:\", env_test.unwrapped.signal_features.shape)\n",
    "print(\"> max_possible_profit:\", env_test.unwrapped.max_possible_profit())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T07:36:30.190141Z",
     "start_time": "2024-04-18T07:36:30.183879Z"
    }
   },
   "id": "bf2f10358d794004",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env information:\n",
      "> shape: (30, 8)\n",
      "> df.shape: (2768, 12)\n",
      "> prices.shape: (2768,)\n",
      "> signal_features.shape: (2768, 8)\n",
      "> max_possible_profit: 599970977911.104\n"
     ]
    }
   ],
   "execution_count": 123
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.cla()\n",
    "env_test.render_all()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T07:36:32.099913Z",
     "start_time": "2024-04-18T07:36:31.917193Z"
    }
   },
   "id": "121c1cb2fc07c0b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T07:36:36.665269Z",
     "start_time": "2024-04-18T07:36:36.661233Z"
    }
   },
   "cell_type": "code",
   "source": "print(r)",
   "id": "98e4d80810c709b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0.13714218139648438, 0, 0, 0, 0, 0, 0, 0, 0, 1.345712661743164, 0, -0.5028572082519531, 0, 0, 0.23999977111816406, 0, 0, 0, 0, 1.3500003814697266, 0, 0, -1.7985725402832031, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7.14714241027832, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.6342849731445312, 0, 0, 0, 0, 0.7185707092285156, 0, -0.3514289855957031, 0, 0, 0.025714874267578125, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2.0471420288085938, 0, 0, 2.177143096923828, 0, 0.4971427917480469, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6.342855453491211, 0, 0, 0, 0, 0, 0.7771415710449219, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2.4071426391601562, 0, 0, 0, 0, 0, 0, 0.7242851257324219, 0, 0, 0.09142684936523438, 0, 0, 0, 0, 0, 0, 0, 3.437145233154297, 0, 0, 0, 0, -0.37285614013671875, 0, 0, 0, 0, 0.14142990112304688, 0, 0, 0.7514266967773438, 0, 0, 0, 1.7871437072753906, 0, 0, -2.2628555297851562, 0, 0, 0, 0, 0, 0, 0, 0, 4.75714111328125, 0, 0, 1.2428550720214844, 0, 0, 0, 0, 0, 0, 0, 2.0285720825195312, 0, 0, 0, 0.43000030517578125, 0, 0, 1.2628593444824219, 0, 0, 0, 0, 1.2828559875488281, 0, 0, 0, 2.0914306640625, 0, -0.2685699462890625, 0, 0, -0.6971435546875, 0, 0, 0, 0, 1.3357124328613281, 0, -0.37999725341796875, 0, 0.19571304321289062, 0, 0, 0, 0, -0.2085723876953125, 0, 0, 0.09571456909179688, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1.7542839050292969, 0, 0, 0, 0, 10.434288024902344, 0, 0, 0, 0, 0, -0.035717010498046875, 0, 0, 0.06571578979492188, 0, 0, 0, -0.14857101440429688, 0, 0, 0.9600028991699219, 0, 0, 2.9714317321777344, 0, 0, 0, -0.005718231201171875, 0, 0, -0.6385726928710938, 0, 0, -0.3528556823730469, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -15.75857162475586, 0, 0, 0.3928565979003906, 0, 0, 0, 0, 0, 0, 0, 0, 0, -4.074283599853516, 0, 0, 0, 0.9842872619628906, 0, 0, 0, 0, 0, 2.9014320373535156, 0, 0, 0.17142868041992188, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3.7257118225097656, 0, -1.0414314270019531, 0, 0, 0, 0, 0.46428680419921875, 0, 0, 0, 0, 0, 2.648571014404297, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.48714447021484375, 0, 0, 0, 0, 0, 0, 2.931427001953125, 0, 0, 0, 0, 0, -0.22142791748046875, 0, 0, 0, 0, 0, 0, 0.7228546142578125, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -3.9942779541015625, 0, 0, 0, 0, 0, 3.048572540283203, 0, -0.8028564453125, 0, 0, 0, 0, 0, -0.4914283752441406, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -4.90142822265625, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.4799995422363281, 0, 0, 0, -2.299999237060547, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18.138572692871094, 0, 0, -0.2857170104980469, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5.037139892578125, 0, 0, 0, 0, 0, -1.0085678100585938, 0, -1.2128524780273438, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17.178573608398438, 0, 0.1971435546875, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4.2014312744140625, 0, 0, 0, 4.734283447265625, 0, -0.19857025146484375, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.3214263916015625, 0, 0, 0, 1.9899978637695312, 0, 0, -0.6757125854492188, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2.6599960327148438, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17.71142578125, 0, 0, 0, 0, 0, -2.9300003051757812, 0, -0.29000091552734375, 0, 0, 0, 0, 0, 0, -9.569999694824219, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10.970001220703125, 0, 0, -3.7199935913085938, 0, 0, 0, 0, 0, -3.7300033569335938, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13.459999084472656, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16.520004272460938, 0, 0, 0, 0, 0, -1.4499969482421875, 0, 0, 0, -8.069999694824219, 0, -2.0699996948242188, 0, 0, 0, -5.8800048828125, 0, 1.9200057983398438, 0, -0.220001220703125, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -25.709999084472656, 0, -0.69000244140625, 0, 0, 0, 0, 0, 0, 0, 0.46999359130859375, 0, 0, 0, 0, -0.06000518798828125, 0, 0, 0, 2.8499984741210938, 0, 0, 0, 0, 0, 0.5900039672851562, 0, 0, 0, -2.1500015258789062, 0, 0, 0, 1.8600006103515625, 0, 0, 0, 0, -0.8699951171875, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7.950004577636719, 0, -1.660003662109375, 0, -0.8499984741210938, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -3.3899993896484375, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -10.779998779296875, 0, 0, 0, 0, -0.40000152587890625, 0, -0.8100051879882812, 0, 0.34000396728515625, 0, 0, 0, 0, 1.9000015258789062, 0, -1.279998779296875, 0, 1.25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.5099945068359375, 0, 0, 1.7700042724609375, 0, -3.160003662109375, 0, 0, 0, 0, 0, -0.9900054931640625, 0, -3.3700027465820312, 0, 0, 0, 0, 0.410003662109375, 0, 1.8800048828125, 0, 0, 0, 0, 0, 0.9900054931640625, 0, 0, 0, 0, 0, 0, 0, 0, 26.029998779296875, 0, 0, 0.45999908447265625, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3.5500030517578125, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23.269996643066406, 0, 0, 0, 0, 0, 3.1699981689453125, 0, 0, 0, 0, 0, -0.9799957275390625, 0, 0, 0.17999267578125, 0, 0, -1.279998779296875, 0, -3.1199951171875, 0, 2.8000030517578125, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 21.0, 0, 0, -0.2299957275390625, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -12.79998779296875, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 33.57000732421875, 0, -6.4000091552734375, 0, 0, 0.3699951171875, 0, 0, 0, 2.100006103515625, 0, 0, 0, 0, 0, -0.6399993896484375, 0, 0, 0, 0, -0.6399993896484375, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15.509994506835938, 0, -0.279998779296875, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12.25, 0, 0, 0, 4.410003662109375, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2.6699981689453125, 0, 0, 0, -6.230010986328125, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -5.5200042724609375, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 27.3800048828125, 0, 0, 0, 0, 40.83998107910156, 0, 4.899993896484375, 0, 0, -8.5, 0, 2.3599853515625, 0, 0, 0, 0, -0.6300048828125, 0, 0, 0, 0, 0, 0, 0, 19.870025634765625, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.07000732421875, 0, 0, -0.95001220703125, 0, -9.779998779296875, 0, 0, 0, 0, 0, 0, 0, -12.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 24.720001220703125, 0, 0, 6.96002197265625, 0, 0, 0, 0, 0, 0, 0, 14.430023193359375, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 24.07000732421875, 0, 0, 0, 0, 0, 0, 0, 0, 10.1099853515625, 0, 0, 0, 19.3599853515625, 0, 0, 0, 12.110015869140625, 0, 0, 0, -32.279998779296875, 0, 0, 0, 0, 7.78997802734375, 0, 0, 0, 0, 0, 0, 15.1099853515625, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -58.30999755859375, 0, 0, 0, 0, 0, 0, 12.77001953125, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2.3800048828125, 0, -4.16998291015625, 0, 0, 8.27001953125, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -111.00997924804688, 0, 0, 0, 0, 0, 0, 0, 0, -18.930007934570312, 0, 0, 0, 0, 0, 0, 37.32000732421875, 0, 17.769989013671875, 0, 0, 4.70001220703125, 0, -4.649993896484375, 0, -3.25, 0, -14.089996337890625, 0, 0, 0, 0, 0, 0, 0, 0, 4.19000244140625, 0, 0, 0, 0, 0, -10.079986572265625, 0, 0, 0, 5.100006103515625, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15.41998291015625, 0, -7.660003662109375, 0, 0, 0, 2.21002197265625, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -82.02999877929688, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -19.290008544921875, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11.139984130859375, 0, 0, 0, 0, 11.649993896484375, 0, 0.55999755859375, 0, 0, 2.6099853515625, 0, 0, 0, 0, 0, 0, 15.540008544921875, 0, 0, 0, 0.79998779296875, 0, 0, 0, -8.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 34.3699951171875, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8.66998291015625, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.399993896484375, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 32.589996337890625, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7.83001708984375, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16.550018310546875, 0, 0, 0, 0, 0, 0, -2.019989013671875, 0, 0, -9.42999267578125, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 113.95001220703125, 0, 0, 0, -3.16998291015625, 0, 0, 0, 0, 0, 0, 8.42999267578125, 0, 0, 0, 0, -3.889984130859375, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7.790008544921875, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16.610015869140625, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3.90997314453125, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 26.459991455078125, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35.230010986328125, 0, 0, 0, 0, 0, 0, -48.8900146484375, 0, 0, 0, 0, 0, 0, 0, -14.220001220703125, 0, 0, 0, 0, 0, 0, 0, 0, 0, -11.8499755859375, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7.149993896484375, 0, 0, 0, 0, 0, 0, 0, 0, 28.9000244140625, 0, 0, 0, 0, 0, 11.989990234375, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -25.3800048828125, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 43.76995849609375, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 37.16998291015625, 0, 0, 0, 0, -7.77001953125, 0, 0, 0, 0, 0, 0, 0, 0, 0, 46.719970703125, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -51.260009765625, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -245.76998901367188, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -169.12998962402344, 0, 0, 0, 0, 0, 0, 0, 0, 4.8899993896484375, 0, 2.25, 0, 0, 0, -1.839996337890625, 0, 0, 0, 0, -13.25, 0, 12.57000732421875, 0, 0, 0, 0, 0, 0, 18.229995727539062, 0, -1.2400054931640625, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 28.029998779296875, 0, 0, 0, 0, 0, 0, 3.6999969482421875, 0, 6.7100067138671875, 0, 0, -1.410003662109375, 0, 0, 0, 0, -3.94000244140625, 0, -14.620010375976562, 0, 0, 0, 0, 0, 0, 0, 0, 0, -4.8899993896484375, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11.779998779296875, 0, 0, 0, 0, 0, 0, 0, 0, 8.370010375976562, 0, 0, 27.300003051757812, 0, 0, 0, 0, -1.67999267578125, 0, -3.839996337890625, 0, 0, 0, 0, 0, 2.66998291015625, 0, 0, 0, 0, 0, 0, 0, 0, 10.079986572265625, 0, 4.80999755859375, 0, -4.3699951171875, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9.45001220703125, 0, 0, -2.519989013671875, 0, -0.209991455078125, 0, 0, 0, -3.839996337890625, 0, 0, 0, 0, 0, 20.290008544921875, 0, 0, 0, 0, 0, -0.930023193359375, 0, 0, 0, 48.04998779296875, 0, 0, 0, 0, -11.010009765625, 0, 4.9000244140625, 0, -4.41998291015625, 0, 0, 0, 0, 0, -2.540008544921875, 0, 0, -2.75, 0, 0, 0, 0, 0, 0, -24.019989013671875, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12.800018310546875, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 71.83001708984375, 0, 0, 0, 0, 0, 0, 0, 0, 0, 40.5, 0, 0, 0, -10.569976806640625, 0, 0, 0, 0, 0, 0, 0, 16.470001220703125, 0, 0, 0, 0, 0, 0, 0, 6.33001708984375, 0, 8.139984130859375, 0, 2.790008544921875, 0, 0, 0, -9.720001220703125, 0, 0, 0, 25.79998779296875, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -36.850006103515625, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 33.779998779296875, 0, 0, 0, 0, 0, 14.040008544921875, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15.25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "execution_count": 125
  },
  {
   "cell_type": "markdown",
   "source": "## Graficos",
   "metadata": {
    "collapsed": false
   },
   "id": "ccd1cc4ef63ea420"
  },
  {
   "cell_type": "code",
   "source": [
    "import quantstats as qs\n",
    "\n",
    "window_size = 30\n",
    "start_index = window_size\n",
    "end_index = len(data)\n",
    "\n",
    "qs.extend_pandas()\n",
    "\n",
    "net_worth = pd.Series(env_test.unwrapped.history['total_profit'], index=df.index[start_index+1:end_index])\n",
    "returns = net_worth.pct_change().iloc[1:]\n",
    "\n",
    "qs.reports.full(returns)\n",
    "qs.reports.html(returns, output='SB3_a2c_quantstats.html')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T08:24:58.214144Z",
     "start_time": "2024-04-18T08:24:47.775213Z"
    }
   },
   "id": "9dd56ff57396362a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<h4>Performance Metrics</h4>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pisky/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.prod with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n",
      "/home/pisky/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.prod with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n",
      "/home/pisky/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.prod with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n",
      "/home/pisky/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/quantstats/stats.py:510: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  returns = _utils._prepare_returns(returns, rf).resample(resolution).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Strategy\n",
      "-------------------------  ----------\n",
      "Start Period               2013-02-19\n",
      "End Period                 2023-12-29\n",
      "Risk-Free Rate             0.0%\n",
      "Time in Market             11.0%\n",
      "\n",
      "Cumulative Return          2,180.29%\n",
      "CAGR﹪                     21.99%\n",
      "\n",
      "Sharpe                     0.91\n",
      "Prob. Sharpe Ratio         99.89%\n",
      "Smart Sharpe               0.91\n",
      "Sortino                    1.58\n",
      "Smart Sortino              1.57\n",
      "Sortino/√2                 1.12\n",
      "Smart Sortino/√2           1.11\n",
      "Omega                      1.86\n",
      "\n",
      "Max Drawdown               -71.81%\n",
      "Longest DD Days            759\n",
      "Volatility (ann.)          40.55%\n",
      "Calmar                     0.31\n",
      "Skew                       1.98\n",
      "Kurtosis                   87.32\n",
      "\n",
      "Expected Daily %           0.11%\n",
      "Expected Monthly %         2.42%\n",
      "Expected Yearly %          32.88%\n",
      "Kelly Criterion            25.49%\n",
      "Risk of Ruin               0.0%\n",
      "Daily Value-at-Risk        -4.05%\n",
      "Expected Shortfall (cVaR)  -4.05%\n",
      "\n",
      "Max Consecutive Wins       1\n",
      "Max Consecutive Losses     1\n",
      "Gain/Pain Ratio            0.86\n",
      "Gain/Pain (1M)             1.31\n",
      "\n",
      "Payoff Ratio               1.5\n",
      "Profit Factor              1.86\n",
      "Common Sense Ratio         -\n",
      "CPC Index                  1.54\n",
      "Tail Ratio                 -\n",
      "Outlier Win Ratio          30.3\n",
      "Outlier Loss Ratio         1.28\n",
      "\n",
      "MTD                        16.58%\n",
      "3M                         16.58%\n",
      "6M                         15.3%\n",
      "YTD                        83.48%\n",
      "1Y                         83.48%\n",
      "3Y (ann.)                  4.97%\n",
      "5Y (ann.)                  9.41%\n",
      "10Y (ann.)                 17.45%\n",
      "All-time (ann.)            21.99%\n",
      "\n",
      "Best Day                   36.94%\n",
      "Worst Day                  -38.83%\n",
      "Best Month                 38.86%\n",
      "Worst Month                -38.83%\n",
      "Best Year                  116.55%\n",
      "Worst Year                 -44.44%\n",
      "\n",
      "Avg. Drawdown              -9.0%\n",
      "Avg. Drawdown Days         95\n",
      "Recovery Factor            5.59\n",
      "Ulcer Index                0.21\n",
      "Serenity Index             1.19\n",
      "\n",
      "Avg. Up Month              10.1%\n",
      "Avg. Down Month            -7.9%\n",
      "Win Days %                 55.23%\n",
      "Win Month %                64.22%\n",
      "Win Quarter %              61.36%\n",
      "Win Year %                 90.91%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<h4 style=\"margin-bottom:20px\">Worst 5 Drawdowns</h4>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "        Start      Valley         End  Days  Max Drawdown  99% Max Drawdown\n",
       "1  2021-12-01  2022-05-16  2023-12-29   759    -71.808120        -70.517524\n",
       "2  2018-08-24  2018-12-21  2019-04-18   238    -31.220261        -25.935952\n",
       "3  2015-12-07  2016-09-21  2017-01-31   422    -28.503736        -28.201750\n",
       "4  2014-04-15  2014-05-05  2015-01-22   283    -26.131016        -25.839297\n",
       "5  2019-08-02  2019-10-28  2020-04-21   264    -23.024572        -22.786482"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start</th>\n",
       "      <th>Valley</th>\n",
       "      <th>End</th>\n",
       "      <th>Days</th>\n",
       "      <th>Max Drawdown</th>\n",
       "      <th>99% Max Drawdown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-12-01</td>\n",
       "      <td>2022-05-16</td>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>759</td>\n",
       "      <td>-71.808120</td>\n",
       "      <td>-70.517524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-08-24</td>\n",
       "      <td>2018-12-21</td>\n",
       "      <td>2019-04-18</td>\n",
       "      <td>238</td>\n",
       "      <td>-31.220261</td>\n",
       "      <td>-25.935952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-12-07</td>\n",
       "      <td>2016-09-21</td>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>422</td>\n",
       "      <td>-28.503736</td>\n",
       "      <td>-28.201750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-04-15</td>\n",
       "      <td>2014-05-05</td>\n",
       "      <td>2015-01-22</td>\n",
       "      <td>283</td>\n",
       "      <td>-26.131016</td>\n",
       "      <td>-25.839297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-08-02</td>\n",
       "      <td>2019-10-28</td>\n",
       "      <td>2020-04-21</td>\n",
       "      <td>264</td>\n",
       "      <td>-23.024572</td>\n",
       "      <td>-22.786482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<h4>Strategy Visualization</h4>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pisky/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/quantstats/_plotting/wrappers.py:565: FutureWarning: 'A' is deprecated and will be removed in a future version, please use 'YE' instead.\n",
      "  returns = returns.resample(\"A\").apply(_stats.comp)\n",
      "/home/pisky/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/quantstats/_plotting/wrappers.py:568: FutureWarning: 'A' is deprecated and will be removed in a future version, please use 'YE' instead.\n",
      "  returns = returns.resample(\"A\").last()\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwwAAAF4CAYAAAD5U36FAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAABSbUlEQVR4nO3dd3gVdd7//1fKSXLSSSMQAknoEFoARQRpiuiCBUQUUXEty67e67rquuquu+p9c9/r2i7L2r5rR0BQsIBKEZUqSA3F0BJaThLSe5/fH/zOLIeTkZbKeT6ui+vizGdmzueVOZnM+8xnZrwMwzAEAAAAAA3wbukOAAAAAGi9KBgAAAAAWKJgAAAAAGCJggEAAACAJQoGAAAAAJYoGAAAAABYomAAAAAAYImCAQAAAIAlCgYAAAAAligYAAAAAFiiYAAASd999528vLxO+2/mzJkNLp+VlaW///3vGj58uKKiouTn56eIiAgNHjxYDz/8sA4ePOgy/9VXX+2y3qVLl7qts6ysTImJieY8wcHBSk9PP6ccPj4+Cg8PV0pKih555BFlZWWd88/qVDNnzjTfZ/To0Y22XgBA60DBAADn6b333lNSUpKefPJJrV+/Xnl5eaqpqVFBQYG2bNmiZ599Vj179tQzzzxjLvPWW28pLCzMfD1r1iyVlJS4rPfxxx9XRkaG+fqZZ55RYmLiOfWxvr5eRUVF2rp1q5555hkNGjRIR44cOad1AQA8i29LdwAAWqNp06ZpyJAhbtOTk5NdXs+bN8/lrIPdbtdNN92kbt266ejRo5o7d64KCwtVW1urRx55RN7e3nrooYcUFxenF198UXfccYck6ciRI3r00Uf1yiuvSJI2bNigl19+2VzvmDFj9Nvf/vaccxQXF2vx4sVKTU2VdOKMyAsvvKDnn3/+rNfZWlRXV8swDPn7+7d0VwDgwmYAAIxVq1YZksx/77zzzmmXKS4uNiIjI81lwsLCjJ07d7rMc+TIEaNTp07mPP7+/sbhw4fN9quvvtps8/LyMtasWWNUVVUZffv2NacHBwcb6enp552jsLDQ8PPzM9uuvPLKBtfxww8/GNOmTTPi4+MNPz8/IyQkxBg2bJjxyiuvGNXV1eZ877zzjst7NfRv1apVhmEYxqhRo8xpt99+u8v7nbqek526XGpqqnHttdcaERERhiRj69atRnp6utt7zp0717jooosMu91uhIeHGzfccIPLz93ps88+M6688kojJibG8PX1NUJCQoykpCTj2muvNWbPnm3U1dWd0c8dAC5knGEAgHP0ySefKC8vz3z9X//1X+rbt6/LPJ06ddJf//pX/eY3v5EkVVVV6e2339bf/vY3SdKbb76p5ORkFRYWyjAM3Xnnnbr++uu1a9cucx3PPPOMEhISzru/YWFhCg4OVn5+viQpKirKbZ7HH39cs2fPdplWXV2tDRs2aMOGDZo/f76++uorBQUFnXd/ztaOHTs0bNgwlZWV/eJ8f/3rX7VmzRrzdUVFhRYuXKjt27drx44dCggIkCS9++675hkep5KSEpWUlOjgwYP67LPP9MADD5jzA4CnomAAgAZ8/fXXys3NdZs+bdo0xcfHS5JWr17t0jZ16tQG1zVt2jSzYDh1OefQJOewprS0NP3f//2f2T527FjNmjXrnHM4FRcX69133zWLBUm68cYbXeaZN2+eS7Fw5ZVX6tJLL1V2drbee+89lZaWavXq1XrggQf05ptvaujQofrnP/+p+fPn66effpIkJSUluQyd6tq163n33Wnr1q3y9fXVrbfequ7du+vnn39u8GB+zZo1Gjp0qK688kqtWrVKa9eulSTt27dPixcv1k033SRJeu2118xlhg4dqokTJ6q2tlZHjhzRjz/+qD179jRa3wGgTWvpUxwA0BqcOpTH6p9ziI1hGMZVV13l0lZYWGi5/rCwMHO+Pn36uLVPnDjR7b3OZijS2eQIDAw0/vnPf7otO2jQIHOe2267zaXt448/Ntt8fX2NvLw8s+32228320aNGtVgvxpjSJIkY/HixW7rPnVI0kUXXWQOnaqurjZiYmLMtj/+8Y/mcv379zenr1+/vsH1MiQJAAyDuyQBQCvw5ptvql27di7Tnn322UYZinSq66+/3u2sRXl5ubZt22a+fv/9911uy3ry2Yja2lpt3Lix0ft1OsnJybr22mtPO99dd90lm80mSbLZbC53liooKDD/P3LkSPP/V1xxhcaPH697771Xr776qlJTU5WQkCBvb/5MAgB7QgBowDvvvCPDMNz+nfycgQ4dOrgsc+jQoQbXVVRUpKKiIsvlnNMmTZrkMu3kYUznatq0aZo9e7YmTpxoTpszZ46uvfZaGYZhTisoKHB5fTrHjx8/5z6d+j5VVVVntFyvXr3OaL5Ti6yT76JUX19v/n/27Nm66qqrJEmlpaVavny5/vWvf+m+++5T//79NXr06NNeLwEAnoBrGADgHI0cOVJvv/22+XrhwoXq37+/23wff/yx23IN8fLyatwOSpowYYJ5fcSsWbP0xhtvSJK+/fZbffjhh7r11lslSeHh4S7LXXPNNZb9lKSUlJSz6sfJ39RXVFS4tO3bt++M1nGmF1o7zy44Wf1cQ0NDtXTpUh09elQbNmzQ3r17tXv3bi1atEjl5eX6/vvv9cwzz+jJJ588o/cFgAsVBQMAnKMpU6bowQcfNC8kfvnll3XzzTerd+/e5jyZmZl6+umnzdd+fn5ud+ZpLv/3f/+nefPmmWc7nnrqKU2fPl0+Pj4KCgrSwIEDzWFJeXl5uv/++90OvouKivTVV1+53A3q5HnKy8sbfO+TC5KtW7equrpafn5+OnbsmN57771GSnh2du7cqZ49e6pTp0664YYbzOn333+/XnrpJUnSli1bWqRvANCaUDAAQAOs7pIUFhamu+++W5IUEhKiV155RdOnT5ckFRYWasiQIW4Pbjt53Pz//M//qHPnzs0T4hTh4eG69957zTsh7d+/X/Pnzzf7//DDD+uWW26RJK1du1b9+/fXpEmT1K5dO+Xl5Wnr1q1as2aNOnToYN5pSDpxpyenzZs36/7771d8fLz8/Pz0+9//XtKJuxAtWrTIfN+UlBT17t1bq1atcrk1bXN66KGHtHHjRo0bN07x8fGKjo5WZmam3nnnHXOeU8+8AIBHarnrrQGg9TjTuyR16dLFbdl33nnHsNvtv7icj4+P8Y9//OMX+3Dy3YbOdfd8ugfQ5eTkGIGBgWZ73759jfr6erP90UcfPeufwdatWw1vb2+3+YKCgsx5srOzXR5y5/zn7e1tXHnllWf84LaGNPTgtjNZx6nve+q/gIAAY+PGjWf8sweACxUXPQPAeZo5c6YOHDigJ554QsOGDVNERIR8fX0VFhamQYMG6cEHH1RaWpr+9Kc/tXRXFR0drbvuust8vWvXLvObf+nEhcBr167VjBkzlJiYKH9/f9lsNsXFxWn8+PGaPXu2Vq5c6bLOgQMHau7cuUpJSbF8yFlMTIy+//57XXXVVQoODlZQUJDGjh2r7777zuVsRXN6+OGHdf/992vYsGGKi4uTn5+f/P39lZSUpNtvv10bN27U0KFDW6RvANCaeBnGWdwWAwAAAIBH4QwDAAAAAEsUDAAAAAAsUTAAAAAAsETBAAAAAMASBQMAAAAASxQMAAAAACxRMAAAAACwRMEAAAAAwBIFAwAAAABLFAwAAAAALFEwAAAAALBEwQAAAADAEgUDAAAAAEsUDAAAAAAsUTAAAAAAsETBAAAAAMASBQMAAAAASxQMAAAAACxRMAAAAACwRMEAAAAAwJJvS3cAAC40x44d0/bt25WRkaHCwkLZ7XZ16tRJY8eOVWRkpNv8x48f1zfffKPDhw/Lx8dHPXr00Pjx4xUUFOQy3w8//KBjx47p2LFjKisr06hRozR69Gi39e3Zs0ebN29Wdna2KioqFBgYqE6dOmn06NGKiYlp1Kx79uzRrl27dOzYMZWWliosLEzdu3fXqFGjFBAQ4DZ/WlqavvvuOx0/flxBQUEaOHCgRo0aJW/v/3x/dfDgQaWmpurw4cMqLi5WcHCwEhMTNWbMGIWEhLisb/Xq1UpLS1NBQYGqqqrM9x85cqTbzw8AcG4oGACgka1du1ZHjhxRnz591L59e5WWlmrjxo164403dNddd7kctBcXF+vdd9+Vv7+/xo0bp+rqaq1bt07Z2dm6++675ePjY867atUqBQcHKzY2VgcOHLB8/5ycHAUEBOjiiy9WYGCgSktLtW3bNr311lu68847FRsb22hZv/jiC4WEhKh///4KCwtTdna2Nm3apP379+uee+6RzWYz5923b5/mzZunhIQEXXXVVcrJydHq1atVVlamiRMnmvOtWLFCFRUV6tOnjyIjI1VQUKCNGzdq7969mjVrloKDg815HQ6HYmNjlZycLD8/P+Xm5mrLli3at2+ffvOb38jPz6/RsgKAp/IyDMNo6U4AwIXkyJEj6tixo8vBfl5enl577TX16dNHkydPNqcvWbJE27Zt03333aewsDBJJ75h/+CDDzRx4kQNHjzYnLewsFDh4eEqLy/XP//5T8szDA0pLS3VCy+8oEGDBrkcnJ+vjIwMJSQkuEzbvn27Fi9erEmTJiklJcWc/q9//Uve3t665557zDMK3377rVavXq17771XUVFRkqRDhw6pc+fO8vLyMpc9dOiQ3n33XY0cOVJjx479xT7t3r1bCxYs0JQpU5ScnNxISQHAc3ENAwA0svj4eJdiQZIiIyMVExOj3Nxcl+l79uxRjx49zGJBkpKSkhQZGaldu3a5zBseHn7OfQoKCpLNZlNlZeU5r6MhpxYLktSrVy9JJ4ZaOR0/flzHjx/X4MGDXYYfDR06VNKJg3ynLl26uBQLzml2u93t59cQ58+psbMCgKdiSBIANAPDMFRaWuo2HKmsrEwdO3Z0mz8uLk779u07r/esrKxUXV2dSktLtWHDBlVVVSkxMfG81nkmSktLJUmBgYHmNIfDIUluWUNCQhQaGqqsrKxfXGd1dbWqq6tlt9vd2gzDUEVFherr65WXl6eVK1fKy8urwWIGAHD2KBgAoBmkpqaqpKREY8aMMac5D6xPHpPvFBwcrIqKCtXW1srX99x21f/v//0/5eXlSZL8/Pw0cuRIlyFCTWXt2rXy8vJSnz59zGmny1pSUvKL69ywYYPq6uoaHGJUVlam5557znwdGhqqKVOmmEOcAADnh4IBAJpYbm6uli5dqk6dOmnAgAHm9JqaGklqsCBwTjufguHaa69VVVWVCgoKtG3bNtXW1qq+vt5tuFRjSk1N1datWzV8+HCXO0KdLmtVVZXlOg8dOqTvv/9effv2bfAMid1u16233qra2lo5HA79/PPPqq6uboQ0AACJggEAmlRpaak++ugj+fv768Ybb3QZv++8g1Btba3bcs5p51osSCeupXBKTk7Wq6++KkkaP3685TKVlZUu/fHx8WlwGFBDDh06pM8//1xdu3bVuHHjXNpOl/XkuymdLDc3V/Pnz1dMTIwmTZrU4Dw+Pj5KSkqSJPXo0UNJSUl6++23FRQUpB49epxR3wEA1igYAKCJVFZWas6cOaqsrNQdd9zh9gwB5/Ac53Cdk5WWlsput59XwXAyu92uxMREpaam/mLB8PXXX2v79u3m6y5dumjmzJmnXX9WVpbmzZunmJgYt8JIcs168gXezmlxcXFu6ywqKtIHH3wgf39/TZ8+Xf7+/qfth3SiUAoODlZqaioFAwA0AgoGAGgCtbW1mjt3rvLy8nTrrbcqOjrabZ7Q0FAFBgYqMzPTre3YsWON+rwEZ59Od+egSy+9VP379zdfN/TwtVPl5+drzpw5CgoK0vTp0xt89oEzS2ZmpktxUFJSouLiYrdrK8rLy/Xhhx+qrq5Ot912m1uxdTpnkhUAcGa4rSoANLL6+notXLhQR48e1dSpU12GBp2qd+/e2rt3r4qKisxpBw8eVF5enstFw2ejrKzMbVphYaEOHjzY4B2ZThYdHa2kpCTz3+nmLy0t1YcffigvLy/NmDHD8unKMTExioqK0ubNm1VfX29O37RpkyS5ZK2urtZHH32k4uJiTZ8+vcGnYzvnc14bcbLdu3ersrLytH0HAJwZzjAAQCNbtmyZ0tLS1KNHD1VUVGjHjh0u7Sd/gz9y5Ejt3r1b7733ni6++GLzSc8xMTEaOHCgy3Lbt29XUVGReZB86NAh/fDDD+Y6nc8feO2115SYmKjY2FgFBAQoPz9fW7duVX19vS6//PJGzfrhhx+qoKBAw4cP1+HDh3X48GGzLSgoSF27djVfX3HFFZo7d64+/PBD9e3bVzk5Odq0aZNSUlJczsB8+umnOnbsmAYOHKjc3FyXZy/4+fmZz3nIz8/X+++/r759+yoqKkpeXl5yOBzasWOHwsPDdfHFFzdqVgDwVBQMANDInM8U2Lt3r/bu3evWfnLBEBYWppkzZ2rZsmVauXKlfHx81L17d40fP97t+oWtW7fq0KFD5uuMjAxlZGRIkjp37mwWDEOGDNG+ffu0f/9+VVdXmwfuI0aMUPv27Rs1a3Z2tiRp3bp1bm1dunRxKRh69OihadOm6fvvv9dXX32loKAgjRgxQqNGjXJZzvnz27Ztm7Zt2+bSFhYWZhYMoaGh6t27tzIyMrR9+3bV19crLCxMQ4cO1WWXXebyHAgAwLnzMgzDaOlOAAAAAGiduIYBAAAAgCUKBgAAAACWKBgAAAAAWKJgAAAAAGCJggEAAACAJQoGAAAAAJYoGAAAAABYomAAAAAAYImCAQAAAIAlCgYAAAAAligYAAAAAFiiYAAAAABgiYIBAAAAgCUKBgAAAACWKBgAAAAAWKJgAAAAAGCJggEAAACAJQoGAAAAAJYoGAAAAABYomAAAAAAYImCAQAAAIAlCgYAAAAAligYAAAAAFiiYAAAAABgiYIBAAAAgCUKBgAAAACWKBgAAAAAWKJgAAAAAGCJgqEVy8/Pb+kutBhPzi55dn5Pzi6R35Pze3J2ybPze3J2ifxtIT8FQytWXV3d0l1oMZ6cXfLs/J6cXSK/J+f35OySZ+f35OwS+dtCfgoGAAAAAJYoGAAAAABYomAAAAAAYImCAQAAAIAlCgYAAAAAligYAAAAAFiiYAAAAABgiYKhFfP19W3pLqCFsO0BAEBrQcHQhOrqjfNaPioqqkXfH+eObQ8AAC4UfI3ZhHy8vfTXpT8rPb+82d87MSJQT1/dq9nfFyew7QEAwIWCgqGJpeeXKy2ntKW7gRbAtgcAABcChiQBAAAAsETBAAAAAMASBQMAAAAASxQMAAAAACxRMAAAAACwRMEAAAAAwBIFAwAAAABLFAwAAAAALFEwAAAAALBEwQAAAADAEgUDAAAAAEsUDAAAAAAs+Z7tApWVlVq2bJkOHTqkjIwMlZaW6vrrr9eECRPc5nU4HFqwYIH2798vHx8fJScna+rUqQoNDXWbd+3atVq+fLmOHz+udu3aafTo0Ro3bpy8vLzMeTIzMzVnzhwdOXJE7du310033aSuXbu6rGfz5s2aO3eunn76adnt9rONBwAAAOAkZ32GobS0VEuWLNGxY8cUHx9vOV9BQYGeffZZZWdn67rrrtP48eO1c+dOvfDCC6qpqXGZ94cfftD777+v2NhY3XzzzeratasWLFigr776ypynvr5er7/+uurr6zVlyhSFhobqtddeU0VFhTlPdXW1Fi5cqOuvv55iAQAAAGgEZ32GISwsTP/4xz8UHh6u3NxcPf744w3O99VXX6myslKPPfaYIiMjJUkJCQl68cUXtXbtWo0ePVrSiYP8xYsXq2/fvpo1a5YkacSIEaqvr9fSpUt12WWXKTg4WDk5OcrOztb//u//KiIiQpdccon++Mc/6uDBg+rbt68k6euvv1ZoaKiGDx9+Lj8LAAAAAKc46zMMNptN4eHhp51vy5YtSk5ONosFSerdu7fat2+vzZs3m9PS0tJUVlamUaNGuSw/evRo1dTUaMeOHZJOFBaSFBgYKEny8/OTn5+fOT03N1fLli3TTTfd5DKMCQAAAMC5a5KLngsKClRSUqIuXbq4tSUkJOjw4cPm6yNHjkiS27xdunSRl5eX2d6+fXvZ7XZ98cUXysvL0zfffKOKigp17txZkvTxxx9ryJAhSkxMbIpIAAAAgEc66yFJZ6KoqEjSieFLpwoLC1NlZaWqqqrk7+9vOa+vr6+Cg4NVWFgoSfL399f06dP1/vvva8WKFfL29tbkyZMVGRmpXbt2KS0tTU899VRTxAEAAAA8VpMUDM6Lmm02m1ubc1pNTY38/f1VU1MjHx+fBocR+fr6ulwgfdFFF6lv377Kzs5WVFSUQkNDVVdXp/nz52vixIkKCwvT999/r5UrV0qSxo0b5zbU6Wzk5+ebQ57Olq+vr6Kios75vRtLbm6uamtrW7obZ62qqkpZWVkt3Y1zwrY/P2152zcG8ntufk/OLnl2fk/OLpG/JfPHxsae0XxNUjCcXBSc6tRiwmazqa6uTvX19fL2dh0hVVtb61Z0BAUFKSkpyXy9YsUKeXl5aezYsdqzZ48++eQT/frXv5Yk/fvf/1ZsbKx69ux5TjkiIiLOabnWpDUcuJ6LrKysM/4Qo2Fs+7aJ/J6b35OzS56d35OzS+RvC/mb5BoG5/Ai53CjkxUVFSkgIED+/v4u8xYXF7vMV1tbq9LS0l+8wLqoqEhLly7VjTfeKB8fH23atEkpKSkaOHCgBg4cqJSUFP3444+NlAoAAADwPE1SMLRr104hISE6dOiQW1tGRobL8xuc/z913kOHDskwDHXq1MnyfT755BP17NnTvK1qYWGhy7UQ4eHh5jUQAAAAAM5ekxQMkjRo0CDt3LlTeXl55rQ9e/YoOztbgwcPNqf17NlTQUFB+v77712W//7772Wz2dS/f/8G179//35t2bJFU6dONaeFhoa6jAFzOBwNPlUaAAAAwJk5p2sYVq1apfLycvMpy2lpaaqrq5MkjR07Vna7XVdddZU2b96s559/XuPGjVN1dbWWLVumDh06aMSIEea6/Pz8dM0112ju3Ll6/fXXlZycrH379unHH3/UpEmTFBIS4vb+9fX1mjdvni6//HJFR0eb01NSUvSvf/1LixYtkiTt2LFD991337lEBAAAAKBzLBiWL1/ucuZg9+7d2r17tyTp4osvlt1uV0REhB566CEtWLBAixcvlo+Pj/r27aupU6e6Xcg8evRo+fr6avny5UpNTVV4eLhuuOEGXX755Q2+/+rVq1VaWqqrrrrKZXr//v113XXXadWqVTIMQ9dff72Sk5PPJSIAAAAAnWPBMHv27DOar2PHjrr//vvPaN4RI0a4nHn4JaNGjbK8XeqECRM0YcKEM1oPAAAAgF/WZNcwAAAAAGj7KBgAAAAAWKJgAAAAAGCJggEAAACAJQoGAAAAAJYoGAAAAABYomAAAAAAYImCAQAAAIAlCgYAAAAAligYAAAAAFiiYAAAAABgiYIBAAAAgCUKBgAA0OJ8fX1bugsALFAwAACA81ZXb5zX8lFRUS36/kBLaQvFcuvvIQAAaPV8vL3016U/Kz2/vNnfOzEiUE9f3avZ3xeQThSrPt5e57x8YxTL5/P+Z4KCAQAANIr0/HKl5ZS2dDeAZuUJxTIFAwAAAHAeLvRimWsYAAAAAFiiYAAAAABgiYIBAAAAgCUKBgAAAACWKBgAAAAAWKJgAAAAAGCJggEAAACAJQoGAAAAAJYoGAAAAABYomAAAAAAYImCAQBaEV9f35buAgAALigYAKAR1dUb57V8VFRUi74/AACn4qssAGhEPt5e+uvSn5WeX97s750YEainr+7V7O8LALiwUTAAQCNLzy9XWk5pS3cDAIBGwZAkAAAAAJYoGAAAAABYomAAAAAAYImCAQAAAIAlCgYAAAAAligYAAAAAFiiYAAAAABgiYIBAAAAgCUKBgAAAACWKBgAAAAAWKJgAAAAAGCJggEAAACAJQoGAAAAAJYoGAAAAABYomAAAAAAYImCAQAAAIAlCgYAAAAAligYAAAAAFiiYAAAAABgiYIBAAAAgCXfplpxWlqann/++QbbHnnkESUlJZmvDxw4oE8//VSHDh1SQECAUlJSNHnyZAUEBJjzlJeXa+7cudq5c6fsdruuvvpqjRgxwmW9+fn5+vvf/677779fXbt2bZpgAAAAgAdpsoLBafTo0UpMTHSZFhMTY/7/yJEjeuGFFxQbG6upU6eqoKBAK1asUE5Ojv7whz+Y8y1cuFB79+7VpEmTlJOTow8//FAdOnRwKQwWLFigQYMGUSwAAAAAjaTJC4Zu3bpp6NChlu2LFy+W3W7Xgw8+KLvdLkmKiorSBx98oNTUVPXr10+SlJqaqsmTJ+uSSy6RJB07dkw7duwwi4Off/5Zu3fv1lNPPdXEiQAAAADP0SzXMFRWVqqurs5tekVFhXbv3q2hQ4eaxYIkDRs2TP7+/tq8ebM5rbq6WoGBgebrwMBAVVdXS5Lq6uo0f/58XX311QoLC2vCJAAAAIBnafIzDB988IGqqqrk7e2tbt26afLkyeYQpWPHjqm+vl4JCQmunfL1VXx8vI4cOWJOS0hI0IoVKxQbG6vc3Fzt2rVLt956qyRp1apVqqur07hx45o6DgAAAOBRmqxg8PX1VUpKipKTkxUcHCyHw6Fly5bp2Wef1cMPP6yEhAQVFRVJUoNnBcLCwpSVlWW+vvHGG/Xyyy/riSeekCQNGjRIQ4cOVXFxsb788kvdeeed8vVt8voHAAAA8ChNdoTdtWtXl4uPBwwYoJSUFD311FNatGiRHnjgAdXU1JzoRAMH+jabzWyXpLi4OD399NM6duyYAgMDzQunFy1apG7duqlfv37av3+/FixYoKKiIg0cOFA33HDDeRUR+fn55rCns+Xr66uoqKhzfu/Gkpubq9ra2pbuxlmrqqpyKRjbErb9+WHbn7+2uu2ltr39z1dbzs5n//y05W3fGNpy/rb+2Y+NjT2j+Zr1K/mYmBgNHDhQW7ZsUV1dnWw2myQ1GLCmpsZsd7LZbC7Dl9LT07Vp0yY98cQTKisr08svv6wJEyaoZ8+eeu+997R06VJdc80159zfiIiIc162tWgNH+JzkZWVdcYfYjSMbe+52uq2lzx7+3ty9sbSVj/7nr7tPT1/Y2jqz36zP7itXbt2qqurU2VlpTkUyTk06WRFRUUKDw+3XI9hGJo3b57GjRunmJgYpaamKigoSFdddZWSkpI0fvx4bdy4saliAAAAAB6h2QuG3Nxc+fr6KiAgQHFxcfL29lZGRobLPLW1tTpy5Ig6depkuZ61a9eqsLBQV111lSSpsLDQ5VqI8PBwFRYWNkUEAAAAwGM0WcFQUlLiNu3IkSPavn27evXqJR8fH9ntdvXu3VubNm1SRUWFOd+GDRtUVVWlwYMHN7ju8vJyLV682OVp0KGhoTp+/Lh5+1aHw6HQ0NAmSAYAAAB4jia7huGtt96SzWZT165dFRISIofDodWrV8tms2nKlCnmfNddd53+8Y9/6LnnntPIkSNVWFio5cuXq2fPnuZD2071xRdfKCYmRhdffLE5LTk5WXPnztW///1vJSUlaenSpbr00kubKh4AAADgEZqsYBg4cKB+/PFHrVixQhUVFQoODtbAgQM1ceJEtW/f3pyvc+fOeuCBB/Tpp59qwYIF8vf31/Dhw3X99dfLy8vLbb3Hjh3TDz/8oEceecRlemhoqH7zm99owYIF2rNnj/r3769JkyY1VTwAAADAIzRZwTB27FiNHTv2jObt1q2b/vSnP53RvHFxcXr11VcbbEtOTlZycvIZ9xEAAADAL2v2i54BAAAAtB0UDAAAAAAsUTAAAAAAsETBAAAAAMASBQMAAAAASxQMAAAAACxRMAAAAACwRMEAAAAAwBIFAwAAAABLFAwAAAAALFEwAAAAALBEwQAAAADAEgUDAAAAAEsUDAAAAAAsUTAAAAAAsETBAAAAAMASBQMAAAAASxQMAAAAACxRMAAAAACwRMEAAAAAwBIFAwAAAABLFAwAAAAALFEwAAAAALBEwQAAAADAEgUDAAAAAEsUDAAAAAAsUTAAAAAAsETBAAAAAMASBQMAAAAASxQMAAAAACxRMAAAALQgX1/flu4C8IsoGAAAAM5TXb1xzstGRUW12HsDZ4KSFgAA4Dz5eHvpr0t/Vnp+ebO+b2JEoJ6+ulezvic8DwUDAACtAMNS2r70/HKl5ZS2dDeARseQJAAAGsH5DgthWAqA1oqvMwAAaAQtNSRFYlgKgKZFwQAAQCNhSAqACxFDkgAAAABYomAAAAAAYImCAQAAAIAlCgYAAAAAligYAAAAAFiiYAAAAABgiYIBAAAAgCUKBgAAAACWKBgAAAAAWKJgAAAAAGCJggEAAACAJQoGAAAAAJYoGAAArYavr29LdwEAcIpWsWeuqanRF198oR9//FFlZWWKi4vTNddco759+5rzbNu2TYsWLVJhYaG6d++uGTNmKDw83GU9c+fOVU5Oju6///5mTgAAkKS6ekM+3l7nvHxUVFSLvj8AwF2rKBjee+89bd68WePGjVNMTIw2bNigV155RQ888IB69Oih48eP66233tKQIUOUlJSklStX6r333nMpDI4ePaq1a9fqL3/5SwsmAQDP5uPtpb8u/Vnp+eXN/t6JEYF6+upezf6+AHCha/GCIT09XZs2bdL111+vCRMmSJIuueQSPfnkk/rkk0/06KOPavfu3QoPD9fMmTPl5eWl2NhYvfDCC6qpqZHNZpMkzZs3T6NHj1ZsbGxLxkEjYVgC0Hal55crLae0pbsBAGgkLX4Nw5YtW+Tl5aWRI0ea02w2my699FJlZGQoNzdXNTU1CgwMlJfXidPMQUFBMgxD1dXVkqSNGzcqOztbv/rVr1okA9zV1RvntXxjDEsAAADA+Wvxr3GPHDmi6OhoBQUFuUxPSEgw2xMSErRw4UJt3LhRSUlJWrp0qWJiYhQUFKSqqip98sknuv7662W321sgARrCsAQAAIALQ4sXDEVFRQoLC3Ob7pxWWFioQYMGacyYMfr3v/8tSQoMDNSsWbMkSUuXLlW7du10ySWXNHrfHA6HHA6Hy7R27dopMTFRlZWV2r17t9syKSkpkqS0tDSVlZUpID9d0eWVkqSgqI7yDw5TZXGByvOzXJbzDQhSaGxn1dfXqfDwXrf1hnfqJm9fm0qyj6imwvVUv71djOxhkaoqK1bZ8WOSpID8AG3ZItntdvXu3VuStHXrVhmG6zfvvXv3lt1u16FDh5SXl+fS1r59e8XFxamkpET79u1zabPZbOrXr58kKTU1VTU1NS7t3bt3lySV5+eosth1vX7B4QqO6qDa6koVZ6afktRLEQknDvaLjqWrrqbSpTUoOk7+QaGqKMpTRUGOa5/swQppH6+62hpt2bJFpxowYIB8fHy0b98+lZSUuLTFx8crOjpa+fn5ysjIcH3PoCD17NlTkhpcb58+fRQQEKD09HQVFBSY0wPy09XZL1j28GjVVJSqJPuIy3Levn4K79RVklRweJ+M+lqX9pDYLrIFBKo8P1uVxfkubf7B4QqK6qDaqgoVO1z7ay8IlHTic7h7925VVrr+DJOSkhQeHq6srCxlZma6tIWHhyspKUnV1dXauXOnW9aBAwfK29tbe/fuVWmp6+ewc+fOioqKUm5urg4fPuzSFhwcrB49eqi+vl7btm1zW29ycrL8/Px08OBBpaenu/SrY8eOio2NVWFhoQ4ePOiyXEBAgPr06SPpxI0R6uvrXdp79eqlwMBAHT58WLm5uS5tMTEx6tSpk0pLS7V3r+vvnK+vr/r37y9J2rVrl6qqqlzau3XrptDQUOt9RESg6qqrVJTp2l9Jikg48ftY7MhQbVWFS9v57iMSIwJ14MABFRUVubTFxcWpffv2KigoUHq66+9cU+wjAvLTFVNVq3bxJ/YDBUf2yag75fPdvrNs9qBG3UcE5AeooCBR7dq1U3Z2to4dO+ayZFhYmLp27aqamhqlpqa6/QwbYx9x8j7ffN+OSfLx81dpzjFVlxe7tNnDoxplHxGQf1wn756ioqLUuXNnlZeX6+eff3Zdr7e3Bg4cKKnx9xEJ4QGSpGLHIdVWuX5hFBgZq4CQdqosKVB53imfb/9AhXboIqO+XgWH09zWG9apm3x8bSrJOaqactdtYw+PVmJETKvYRzi3f0j7eNnswaooPK6KQtf1+gWGKjgmrtH2Ec6/9yEhIerevbvq6uq0fft2t/X269dPNputVewjJNfjiL1797p91rp3766QkBAdO3ZM2dnZLm2RkZHq0qWLKioqtGfPHpc2Ly8vDRo0SJK0Z88eVVS4/gwTE5tmHxHjUynFBKuqtEhlua5ZfP3tCu2QIEnKz3Dtr3R++4jEiBhJ0o4dO1Rb67qP6NGjh4KDg3X06FHl5LgeMzn3EWfMaGGPPfaY8cILL7hNz8nJMe655x7jm2++Mafl5eUZBw8eNCoqKgzDMIysrCzj3nvvNdLT043q6mrjo48+Mh555BFj9uzZxr59+867b3/6058MSS7/Jk+ebDgcDmPdunVubZIMh8NhOBwOY/DgwW5tH3zwgWEYhvHKK6+4tY0fP94wDMMoKipqcL05OTmGYRjGpEmT3Nqee+45wzAM4+OPP3Zr69evn9knPz8/t/bvvvvOcDgcxvTp093a7rvvPsPhcBiffPKJW1uHDh3M9Xbo0MGtfcWKlYZhGMaf//xnt7Y777zTMAzD2Llzp1ubn5+f+fMfNGiQW/vHH39sGIZhPPfcc25tkyZNMj87Df0M9+7dazgcDmPUqFFubbNnzzYcDofx8ssvu7UNHjzYzNrQetetW2c4HA5j8uTJbm1/+9vfDMMwjK+//tqtrWvXrmbWqKioBtdrGIbxwAMPuLX97ne/MwzDMDZv3uzWFhISYtTW1hnZ2dlGjx493Nrfffddw+FwGI8++qhb28SJEw2Hw9HgeiUZGRkZhsPhMC655BK3tmeffdZwOBzGs88+69Y2atQowzAMo7KyssH1HjlyxDAMw7jhhhsa3DaGYRifffaZW1ufPn3Mn2FISIhb+zfffGM4HA5j5syZbm333HOP4XA4jC+++MKtLSIiwtzmCQkJbu0fffSR4XA4jAcffNCtbfr06YZhGMa+ffsazOo0bNgwt7bG2EdMnDixwc+hw+Ew3nzzTbe25OTkJtlHxMXFmVnj4uLc2letWtUk+4h58+YZ2dnZxt/+9rcGf4YOh8NITU1t8Gd4PvuI7Oxso7a2rsH1Ov8e3XLLLQ1um6bYR8ycOdNwOBzGN99849YWHBxsbvPG3kdUVlYahmE0+DN86623DMMwjLfeesutrTH2EYsWLXJr69Gjh5k1ODjYrb2p9hFff/21YRhGg5/DW265pUn2EaNHjzYcDoexd+/eBtebmppqOBwOY/z48Q1+Ds91H/H9998btbV1xp133unW9uc//9kwDMNYtWqVW1tT7iOc/U1OTnZrf/PNNw2Hw9Ho+4hXXnnFMAzD+OCDD9zahg0bZmZtaL3nu4+ora0zIiMj3dq/+OILw+FwGPfcc49bm3Mfcaa8/v/Ot5gnn3xSQUFBeuihh1ymZ2Zm6sknn9RNN92kMWPGNLjsyy+/rLCwMN12221avHixtm3bpltvvVVpaWlatmyZZs+ercDAwHPuW2OcYThZQkKCIiIidPz4cR054lolNvTNQEFBgdq1ayepdX8zYHWG4Xy+GcjNzdXx48eb7ZuBpjjDIEkdOnRQhw4dVFxcrP3797u0+fv7m7cOPvWbgYKCAl188cWn/Wagub89bK4zDM7PvdRGzzCcxz6ivr5ehmGc0T7iZBfKPqKgoEDdunVrdd8eNsc+YtOmTS6f/V/aR0hn9u1hW9pHREREKCIi4rT7iMLCQpe2C2EfcfLf+8Y8jjhZU+8j/uvdlTpw1PXMUUBopAIjYlRTUaaSbNdt7uXj2yhnIYNLHfrdiMT/rLcN7iOcn7XGPI6QGvcMQ4sXDC+++KLy8vL09NNPu0zfs2ePXnzxRc2aNcvc8CfbsWOH3n77bT311FMKDQ3V448/rl/96lcaPny4JOnRRx/Vtddeq2HDhjVLjqaQlZXlsXd98uTskmfn9+TsEvk9Ob8nZ5c8O/+FkH3Gh1ta5O5oPWOC9eGMlGZ/38bUFrZ/i98lqVOnTjp+/LhbFe2scOPj492Wqamp0ccff6yJEycqNDRU0olrIU5+kFtYWJjbtxAAAAAAzk6LFwyDBw+WYRhavXq1Oa2mpkbr1683T1+eavny5fL19XUZqhQaGqqsrBOnwurq6nT8+PEGL6YGAAAAcOZa/C5JiYmJGjx4sD777DOVlpaaT3rOzc3VH/7wB7f5CwoK9PXXX2vWrFny8fExp6ekpOjLL79UfX29Dhw4oJqaGiUnJzdjEgAAAODC0+IFgyTdcccdioyM1I8//qiysjJ17NhR9957r3mByMkWLlyo3r17mxcxOU2aNEklJSVasmSJQkND9Zvf/EYhISHNFQEAAAC4ILWKgsFms2nKlCmaMmXKaee9++67G5zu7++vO+64o7G7BgAAAHi0Fr+GAQAAAEDrRcEAAAAAwBIFAwAAAABLFAwAAAAALFEwAAAAALBEwQAAAADAEgUDAAAAAEsUDAAAAAAsUTAAAAAAsETBAAAAAMASBQMAAAAASxQMAAAAACxRMAAAAACwRMEAAAAAwBIFAwAAAABLFAwAAAAALFEwAAAAALBEwQAAAADAEgUDAAAAAEsUDAAAAAAsUTAAAAAAsETBAAAAAMASBQMAAAAASxQMAAAAACxRMAAAAACwRMEAAAAAwBIFAwAAAABLFAwAAAAALFEwAAAAALBEwQAAAADAEgUDAAAAAEsUDAAAAAAsUTAAAAAAsETBAAAAAMASBQMAAAAASxQMAAAAACxRMAAAAACwRMEAAAAAwBIFAwAAAABLFAwAAAAALFEwAAAAALBEwQAAAADAEgUDAAAAAEsUDAAAAAAsUTAAAAAAsETBAAAAAMASBQMAAAAASxQMAAAAACxRMAAAAACw5NvSHQAAAEDblhgR6FHv62koGAAAAHDO6uoNPX11rxZ9fx9vrxZ7f0/QZAVDbm6uHn/88Qbb7rrrLg0dOtRlmsPh0IIFC7R//375+PgoOTlZU6dOVWhoqDlPTU2NPvnkE/3000/y8fHRZZddpl/96lcu66msrNQTTzyhqVOnur0HAAAAGtf5Hqzn5uYqKiqqxd4fp9fkZxiGDBmifv36uUxLSkpyeV1QUKBnn31WAQEBuu6661RVVaVly5bp6NGjeuyxx2Sz2SRJy5Yt0/r163X11VersrJSS5YsUXR0tC666CJzXV9++aViYmIoFgAAANqA2tralu4CTqPJC4b4+HgNGzbsF+f56quvVFlZqccee0yRkZGSpISEBL344otau3atRo8eLUlKTU3VFVdcoSuvvFLSiUJjx44dZsGQlZWl7777To888kjTBQIAAAA8SLPcJamqquoXq8ctW7YoOTnZLBYkqXfv3mrfvr02b95sTqupqVFg4H8ubgkMDFR1dbX5ev78+Ro+fLji4+MbOQEAAADgmZr8DMPSpUu1aNEieXl5KT4+Xtdee62Sk5PN9oKCApWUlKhLly5uyyYkJGj79u3m6y5dumj16tXq2bOnKisrtWnTJo0ZM0aStG3bNh06dEh33XVXU0cCAAAAPEaTFQze3t7q06ePBg4cqPDwcOXm5mrFihV65ZVXNGvWLA0cOFCSVFRUJEkKCwtzW0dYWJgqKytVVVUlf39/TZo0SS+99JKeeuopSVK3bt00duxY1dTUaMGCBbrmmmsUFBTUVJEAAAAAj9NkBUNERITuv/9+l2nDhg3T3//+dy1cuNAsGGpqaiTJvLD5ZM5pNTU18vf3V7t27fSXv/xFmZmZ8vHxUWxsrLy9vfXll18qICBAl112mTIzMzV37lzl5OSoR48emj59uux2+zllyM/Pdxny1NyqqqqUlZXVYu/fkjw5u+TZ+T05u0R+T87vydklz87vydkl8rdk/tjY2DOa77wLhvr6epWUlLhMCwoKkq+v+6qDgoI0fPhwff311+YttE4uCk7VUDHh4+Pjco1CXl6evvnmG/3+97+XYRh69dVX1a9fP02ZMkULFizQvHnzdMcdd5xTtoiIiHNarrFkZWWd8Ya80Hhydsmz83tydon8npzfk7NLnp3fk7NL5G8L+c+7YMjPz3d73sIf//hH9ezZs8H527VrJ0kqLy+X9J+hSM6hSScrKipSQECA/P39Ld9/4cKFGjBggLp37659+/apqKhIU6ZMkc1m0zXXXKOXXnpJt99+u7y9m+X6bgAAAOCCct4FQ1hYmP7whz+4TOvUqZPl/Lm5uZKk4OBgSScKiJCQEB06dMht3oyMjF+849GePXu0a9cuPfnkk5KkwsJCBQYGmmckwsLCVFtbq9LSUpcHwLUVrb3abEqenF3y7PyenF0ivyfn9+Tskmfn9+TsEvnbQv7zLhhsNpt69+7tNr2kpEQhISEu0woKCrR27Vp16NDBZbjPoEGDtG7dOuXl5Zm3Vt2zZ4+ys7PNuyCdqq6uTvPnz9eECRPMsxahoaEqKSlRWVmZgoKClJWVJW9vb7M4AQAAAHB2muyi508++UTHjx9Xr169FB4erry8PP3www+qqqrStGnTXOa96qqrtHnzZj3//PMaN26cqqurtWzZMnXo0EEjRoxocP2rVq1STU2NrrjiCnNaUlKSQkND9cYbb2jQoEFavny5Bg0axHAkAAAA4Bx5GYZhNMWKN27cqB9++EFZWVkqKytTYGCgunXrpquuukoJCQlu82dmZmrBggU6cOCAfHx81LdvX02dOrXB260WFxfrr3/9q+644w7zbktOGRkZmjNnjnmXpNtuu83tTAcAAACAM9NkBQMAAACAto+xOgAAAAAsUTAAAAAAsETBAAAAAMASBQMAAAAASxQMAAAAACxRMAAAAACwRMEAAAAAwBIFAwAAAABLFAwAAAAALFEwAAAAALBEwQAAAADAEgUDgCZXWFjY0l1oMT/88IMqKipauhsAAJwz35bugCfIyMiQJAUEBCgqKkq+vr4yDENeXl4t27Fmsn37dn3++eeaPn26unbtqvr6enl7e0atunHjRr3//vu66667NHDgwJbuTrPbvHmzVq1apaioKI0ZM0ZdunRp6S41m40bN+rzzz9Xbm6u/Pz8NGzYMI/6vd+zZ48cDodCQkIUGxur+Ph4j/rdP3TokAICAmS32xUaGipJHpP/4MGDstlsCgkJUXh4uCTPyS5JBQUFCgsL85i8J8vKylJwcLC8vb0VGBgoSR6137uQj/coGJrQ0aNH9fHHH8vhcKi6ulp1dXUaOXKkJk+eLJvN1tLdaxbHjx/X3LlzVVhYqGXLlum3v/2tR+xEjxw5og8//FDHjh1T//79FRkZ2dJdalbFxcWaM2eOdu/erb59+youLk4hISEt3a1m4dz2R48eVffu3ZWbm6usrKwL5o/G6WRlZWnOnDk6fPiwAgICVFRUpPDwcD322GMKDQ294A8cMzMzNWfOHGVmZqq2tlZ2u12XX365xo4de0EdPDQkOztb7733no4ePSpJ8vPz0/jx4z0iu3Qi/zvvvCPDMHTbbbcpLi6upbvUbI4dO6aFCxcqJydHpaWlCgkJ0TXXXKPBgwfLx8enpbvX5DzheI+CoQnU1dVp+fLl+uabb9SpUyddc801ateunTZs2KDvvvtOMTExGjNmTEt3s1kEBgaqvLxcHTt2VHp6ujZu3KiLLrrogj1oqKmp0Zw5c7RhwwZ1795dd999txITE81vGD3F5s2blZmZqZtvvlm9e/dWu3btzLYL9aChsrJSc+bM0aZNm9S9e3fdc8896tixo1544QUVFhbKy8vrgv3cO+Xk5Oitt95SYGCgZsyYodjYWB04cEDz58/XkiVLdPPNN1/Q+Y8eParXXntNYWFhuv766xUQEKD169fr008/VU5OjmbMmHFBfvYlqaKiQh9++KHq6uo0Y8YMBQQEaM2aNVq8eLGys7N16623XrDZDcPQjh079PnnnyszM1OGYWjnzp2Kjo6Wn59fS3evSdXX12vt2rX6/PPPFRsbq8suu0x1dXX66aefNG/ePEnSRRdddMHu9z3peI+CoQkcOHBAW7du1UUXXaSxY8cqKipKPj4+iouL044dO5SZmam6uroLvuo2DENVVVXq0qWL+vTpo3Xr1mnFihUaNGiQbDbbBXfwZBiGtmzZog0bNmjAgAGaOnWqoqKiWrpbza68vFwrV65UYmKihg8fbk7PzMxUbGys+UfjQtr+NTU1ev3115WXl6ebb75Z/fr1U0REhCoqKhQeHq59+/appqbmgvmmycrmzZt1/Phx/fa3v1X37t3l6+ur6Ohoff/996qrq7ugtnlDNm3apLKyMt1xxx1KTEyUj4+PevbsqTlz5mjNmjWKi4vT8OHD5e/v39JdbXQHDx7U/v37dfPNN+uiiy6SJHXt2lXffPONli1bps6dO+vSSy+Vr++Fd9hRWlqqL7/8UllZWbrpppu0e/durVq1Sj169FBiYmJLd69Jpaena/ny5erRo4cmTpyoDh06SJJSUlL0P//zP9q6dasGDBhwQX7mJc863rvwfnNbAbvdrmHDhmnw4MEu3yzn5OTIbrerU6dO8vHxuWArbicvLy/5+Pjo4MGDmjFjhgzD0NKlS7V8+XJdffXVLd29Rufl5aVu3bopJSVFBw4ckN1ulyRt3bpVP/74owIDA9W+fXtdcsklF/QZh8LCQpWUlJjFwoYNG7RkyRJVV1crLCxM/fr106RJky6YA8f6+nrZbDZNmTJFNptN0dHR5h8Hu92usLAwZWVlKSsrS/Hx8S3c26blcDgUGBio3r17m9PKy8tls9mUkpJywWzzhtTU1CgjI0Pt27dXt27dJJ349jEsLExDhgzRjh079MMPP6hDhw7q1atXC/e28Tj/jhUXF8vHx0f9+vWTdCJ7UFCQRo8erezsbC1dulSdO3e+IA+g/f39ddFFFykpKUldu3ZVly5d9Pzzz2v9+vWKiYlRUFBQS3exyRw/flyBgYGaOnWqeb1KbW2tYmNj1bt3bx0/flze3t4X7PGOJx3vXbh772ZSV1cn6cRBg1N8fLzGjBnj8uHZv3+/Pv74Y5WUlOjo0aM6cOCAioqK3JZtaxrK71RfX6+6ujpFRESorKxMw4cPV1xcnNasWWPuRC607JGRkRo+fLgqKys1b948Pf/883rjjTeUnZ2t1NRULVq0SC+99JL279/vtmxbY7XtAwMDVVtbq/z8fG3fvl3vvfeeEhISzG8dlyxZosWLF6u8vLzZ+9xYTs7uPAiOj49XbGysWSw45+nTp4/LXZIMw2jm3jY+q20fGhqqwsJCrVy5UtnZ2dq7d69effVVHTlyRPPmzdObb76p3bt3t0SXG1VD+W02myoqKlRVVaXs7GyX+RMSEuTl5SWHw6HNmzersrKyWfvbmGpqahqcbrfbVVtbq/T0dEkyD44iIiI0YcIEVVdXa926daqqqmq2vjaFhvL7+flp7Nix6tq1qySpY8eOuuyyy7RhwwYdPHiwubvYZBrKPmzYMM2cOVPh4eHm74PzLJLdbldlZaXq6ura/MGy1HB+Tzre4wzDOaqrq9Pnn3+usrIyzZgxo8FvzpwHE0uWLNEXX3yhHj16aMSIEfL29tZrr72mqKgo/elPf2qT37qdSX5vb2/ZbDYVFBTI29tb4eHhGj58uBYtWqRvvvlGM2bMUHl5uex2e5s6XWeV3fkNQmJiokaMGKGVK1eqR48e+v3vf6+OHTsqKChIP/74oz755BPNmzdPf/nLXy7IbV9dXa3o6GitW7dOkjRmzBhdc801CggIUHFxsZYuXaoVK1aoc+fOGjRoUJv6Q3Imn3sn52c6ICBAhmFoz549io+Pb1N5T2WV37mvu/jii3X48GEtWLBAy5cvV2Fhofr27asRI0aosLBQO3bs0Ouvv67f/e536tmzZ5v7WZwu/7Bhw/Txxx8rNTVV7du3Nz8DW7ZsUXx8vKKjo7Vp0yZdeeWV5ueirfwM6urqtGTJEh09elQ+Pj5KSEjQsGHDFBYWJklq166dQkNDtXXrVg0cOND8Qsjb21udOnXS8OHDtWbNGo0bN06xsbEtnObs/VL++vp6l79hfn5+GjlypLZu3arvvvtOnTp1crmOq6053bZ3bs9T94fOGx4EBAS06eGIp9v2J3/WL8TjPScKhnNw8OBBzZ8/X4cOHVJYWJh27typ5ORkt18I5/8TExM1a9YsdevWTcHBwZJOfAMxf/58ffrpp7rhhhva1C/TmeaXTlwI165dO5WUlEiShg8frl27dmnz5s2qqKhQTk6OJk+e7DKEoTU7k+xBQUEaMGCAKisrNWTIEPXq1cs8KBgxYoTy8/O1dOlSrVmzRiNGjLjgtn1MTIw6duyorVu3ymaz6fLLL1dAQIAkKSQkROPHj9fPP/+s9evXm7eabQsHTWfzuZf+U0A6byVbUFDQpseynkn++Ph43X777dq7d682bNiguLg43XLLLYqIiJAkDRo0SO+8845WrFihpKSkNnVB6JnkHzVqlNatW6fFixcrPz9fPXv2NMd4T5gwQVFRUdq8ebNSU1Pb1IWQ27Zt0/z58+Xr66uoqChlZWVp69at2rFjhx5++GFJUpcuXdS5c2ft3btXaWlpLvt0m82mAQMGaPXq1dq0aZMmTZrUpvZ7p8vfUI7o6GhdccUVmjdvnnbt2qVLLrmkTQ5NOZNt35CKigodPXrUPLPcVp3ptr9Qj/dO1vZ63MKct87Ky8vT8OHDVV1drdWrV6u6utocp3eqPn36aODAgeaHR5L69++vxMRErV+/XjU1NW3mw3O2+e12u/Lz883sNptNUVFRqqqq0pYtW9S/f3/Fx8e3iWEaZ5M9ISHBLIROvshXkoYMGSK73a60tLQ2teM4k/zOjBMmTJBhGKqurjYPCp2npUNCQtSnTx/t2rVL5eXlbeKP57n83nt5eckwDEVGRqpTp07KycmRj49PmzwlfTb5IyMjdckll5in6iMiIszMHTt2VP/+/bVz504VFxe3VJyzdrr89fX1MgxDPj4+uvPOO9WzZ0+tW7dOr732mtavX6/rrrtOkyZNUs+ePeXn56eCgoI2c+C4b98+LV68WImJifr1r3+t3/72t3rqqac0YcIEHThwwDyTKEkTJ05UUVGRfvzxR1VUVLgMO42JiVH79u21d+9e1dbWtpn93unyr1+/XpL7UBNvb2+lpKSoa9euWrVqlTlMrS1sc6dzzW4YhgoLC1VaWqqEhARJ7mcf2oJzyX8hHe+dqm32ugX5+/urtLRU06ZN02233abBgwcrLS1NP/744xkt7/xghYeHy263KyAgoE09BfbU/EOGDPnF/BUVFYqIiFBVVZX279+vf/7zn1q1apUiIyNls9kUFham4ODgNlEwnM229/PzMy90c2Zz/qHo2LGjeceItrTjOJNt7zxA6Ny5sy677DJJ0vfffy/pP/ltNpu8vb0VEBCg0tLS5g9yDs71997Ly0s1NTUKCQlRenq6iouL29Q2dzrb3/uioiJt377dvFbH+Tvg5+dn3ikqNze3eTrfCE6X38vLyywQO3TooFmzZunBBx/UAw88oP/+7//W2LFjJZ24vsfPz08BAQFt4sCxrq5O+/btU0VFhcaPH6/ExETzC4CLLrpIkZGR2rJli6QT29g5VMN5t7iThYeHy9/fXz4+Pm3mTklnkn/z5s2SGt6Xh4aGavz48crMzNS2bdtUVlZmnn2TWvd49vPJ7uXlpczMTElS9+7dJZ3I6ryuSWr913Kd77aX2v7x3qna3l+uFlRfX6/o6Gj9+c9/1tChQyVJV155pfz9/bVu3Trl5+eb91pvaFlJ5rdxztO2PXr0aDMPtGoo//jx438xv6+vr/Ly8rRw4UI999xzMgxDv//973XbbbcpPDxcS5YsMb+Jas3OZ9ufWjBs3LhRhYWFbeqhPuey7a+//np16NBBO3bs0Nq1a838x48f1759+9SlSxdFR0e3SJ6zcb7bPiAgQB06dDAvBG9rzmXbO8csHzp0yDyzUldXp+zsbO3atUsJCQnmBaKt3dnkP7kojo+PV8+ePWWz2cxhaBs3blRpaWmb+d133hb2oYceMr8pPvlskbP4kf5zIfjUqVMVERGhr7/+Wmlpaea+/dixY8rJyWlT1y+cTf5Tf/+dr/v166chQ4Zo1apVWrRokd5++229++67KiwsbNV/984nuyTt3LlTHTp0UHBwsAoLC/XTTz/prbfe0uuvv67i4uJWXzA3xrZvy8d7DWkbZX4L2LRpk37++WdFRkaqW7du6tGjh7nxnaeanH9IRo0aZY5Jv+aaaywvAJZO3HLyyJEjWr58ucLCwnTZZZe1yl+cxsofGhqqlJQUHT58WDfddJP69++vsLAw83St84FWren0fFNt+6KiImVkZGjlypXq3Llzqx3b2Rj56+vrZbfbNXXqVH322Wf66KOPtHXrVnXs2FFHjx6Vw+HQjBkzWt2Y3sbe9k49e/bUt99+a94ZqrUORWusbe/v76+RI0dq8eLFmjNnjkaNGqXS0lLt2LFDDodDU6dObZVP/m2q7V9UVKTDhw9r7dq1GjBggPr06dNckc5YQ9mlE9cmnLytnGcRq6urVVZWZh40+fr6qr6+XkFBQbrhhhu0ePFivfHGGxo2bJiioqL0888/q66uTkOGDGnJmJbON/+p2//k/b7zYvc1a9Zo0KBBLrcgbQ0aM7vzYDkjI0OhoaHat2+fvvvuO+3YsUPJycn63e9+1+puK95U276tHO+dKS+jtZ8XambFxcV69913tX//frVv3165ubmqrq7WFVdcofHjxyswMNDtqvi6ujrNnj1bNTU1+vWvf62EhAS3A4L9+/dr/fr1KikpUVpamiIjI3Xrrbe2untSN1b+ky/uzM/PV2VlpaKjo10eXNXaLgBtqm1/4MABbdiwQcXFxfr5558VHR2t2267TZ07d27BtO4aM79ziIZ0YujJF198oX379kk6ceHzlClTzJ1ya9BU295py5YtevPNN3XddddpwoQJLZDwlzVmfuk/fzA//PBDbdiwQbW1tbLb7YqJiWl1215quu1fUVGhb7/9VmlpaTpy5Iiio6N1++23t6ozDGeSvaHCrqioSI888ohmzJihESNGuJ1Jzc7O1qJFi3Tw4EHz2qVp06aZQ1Rai8bMf+o8Bw4c0BdffKG9e/eqY8eOuvnmm1vVmbWmyl5aWqqnnnpKNptNJSUlCg8P1/Tp01vd80eaKn9bOd47W5xhOMWuXbuUnp6uGTNmqGfPnvL19dWCBQv07bffqqysTLfccovLVfHO26lNmDBB7733nn744QclJCS4HTRUVVXp0KFDCg4O1rRp01yegNuaNFb+kwsB5x1STtWaigWp6bZ9ZWWl0tLSFBISohtvvFGXXnppS8Q7rabKHxUVpZkzZ6qurk55eXlq3759S8T7RU2V3XkA2a9fP919990aPHhwS8Q7rcbO78w9depUjR07VhUVFaqvr291B4tOTbX9a2trFRAQIJvNphtuuKFV/u6fSfaGvhXNyMiQl5eXOnXqJMn9Yt727dvrnnvuUU1NjXJzc1tVkXSypsovnbhj3t69ezV16tRWeVespsqem5ur4uJiBQcHa/LkyRo9enRzxDlrTZW/urq6TRzvnS0KhlOsW7dO7du3dxkuMn36dEnS6tWrlZycrAEDBricopKkoUOHasOGDUpNTVVqaqr69eun7OxseXt7Kzo6Wn379lWHDh0UHh7eKociODVVfkmtbvjBqZpy2997772Kjo726G3v6+vbKosFqemyOw8ubTZbqy0WpKbL7+/vr44dO7ZIprPRVPlDQkI0cuRIjR07ttXu+842u9PBgwcVHBzsMrSmrKxM3t7e5lPupRMXjLfWYkFquvyGYSg2NlavvPJKq93vN1X2hIQE3XnnnUpJSWl1XwyerLHze3l5KTAwUH369FFsbGyrP947WxdOkvNkGIZqamrc7t5QV1dnPsWxc+fO+vjjj92eWui82Ou6665TbW2tvv32W61evVpvv/22Fi9erMLCQkknvmlvrR+epszvfMJha/2D2Rzbvn379h697cneOjVH/tasOfL7+fm1yn3fuWZ35j506JB5UFRZWal9+/bpnXfe0Zdffqnq6mpJF+Zn/0zzO+dvjT+DpszufIr50KFDW22x0FT5lyxZYj7JvDUf752rCyvNGcrKytL8+fM1b948LV68WNnZ2fLy8pLNZpOfn5/Kysp09OhRSf85yO3cubNGjhypvLw8ffvtt5L+c3GP85ciPj5ePXr00J49ezRnzhwVFhbqkksuaVUXN0nNn9/5NMjWgG3PtvfE7BL5PTl/Y2V3XndWXl6urKws8/kiX375pf71r38pIyNDvXr1anUP5PPk/M2d3XkhcGvR3Pmdt0y/EHnUkKTa2lotXrxY3333nTp27KiKigodP35cGzdu1OTJkzVkyBBdfPHFevPNN5Wenq6OHTu6XOTWp08f9ezZUytXrtSYMWNcrp4/duyYNm3apL1798rPz0/XXXedee/t1sKT83tydsmz83tydon8npy/KbJLJy5orqioUHZ2tt58801lZWVp4sSJre6Cfk/O78nZJfI3BY8pGCorK/X1119r69atmjRpkgYOHKjo6GilpaXpnXfe0cqVK9W/f38NHDhQnTp10oYNG9SrVy+XceeRkZHq3r27MjIytGvXLg0YMMCsSFNTU7V8+XINGTJEN998c6ursj05vydnlzw7vydnl8jvyfmbKrt04i4xVVVV2r17t4YNG6aHHnqoVWWXPDu/J2eXyN9UPGZIUmlpqTZt2qQ+ffrosssuM8eU9+7dWwMGDFB2drYyMzPl7e2tK664QgcOHNCWLVvM8Wi1tbWSpAEDBqiqqsp87Tw9PWDAAP3tb3/THXfc0So/PJ6c35OzS56d35OzS+T35PxNlV06cWvkcePG6YknntDMmTNbXXbJs/N7cnaJ/E3FYwqGyMhITZgwQbfccot5BwfnBSy9evVSRUWFOfbMWXkuX75cu3btkiTzdJRzGecHy1mNdujQQTExMc0X6Cx5cn5Pzi55dn5Pzi6R35PzN1V2SerataumTp3aqu+A5cn5PTm7RP6m4jEFg5eXl3kv3FMvWsvLyzPnkSS73a5p06bJy8tLixcvVmpqqqQTT+3bsGGD2rVrp759+zZ3hPPiyfk9Obvk2fk9ObtEfk/O78nZJc/O78nZJfI3FY+5hkH6zwfm1IcLFRQUKDg42LxnfH19vdq1a6c77rhDn376qV599VXFxcXJz89Phw8f1oQJExQSEtLqnytwKk/O78nZJc/O78nZJfJ7cn5Pzi55dn5Pzi6Rvyl4VMFwKucHaf/+/erWrZt8fHzMD5Uk9enTR507d9aaNWuUm5uryspK3XDDDa3q0e7nw5Pze3J2ybPze3J2ifyenN+Ts0uend+Ts0vkbwweXTBIUklJiRwOh4YOHSpJ5m21KioqFBQUpODg4Av6dlmenN+Ts0uend+Ts0vk9+T8npxd8uz8npxdIv/58phrGKxkZmaqtrZWCQkJkk7cMmvjxo166aWXVFJS0rKdawaenN+Ts0uend+Ts0vk9+T8npxd8uz8npxdIv/58tgzDM7xaBkZGbLb7QoLC1NaWpq+/fZbpaamqlOnTvLy8rpgx615cn5Pzi55dn5Pzi6R35Pze3J2ybPze3J2ifyNxWMLBueHIj09XUFBQVq2bJl++uknhYaG6r777lOfPn1auIdNy5Pze3J2ybPze3J2ifyenN+Ts0uend+Ts0vkbyweWzBIUk1NjXJzc5Wbm6uSkhJNmjRJl19+eUt3q9l4cn5Pzi55dn5Pzi6R35Pze3J2ybPze3J2ifyNwcswDKOlO9GSPvnkE3l5eWnSpEmy2Wwt3Z1m58n5PTm75Nn5PTm7RH5Pzu/J2SXPzu/J2SXyny+PLxhOvq2WJ/Lk/J6cXfLs/J6cXSK/J+f35OySZ+f35OwS+c+XxxcMAAAAAKxRagEAAACwRMEAAAAAwBIFAwAAAABLFAwAAAAALFEwAAAAALBEwQAAAADAEgUDAAAAAEsUDAAAAAAsUTAAAAAAsETBAAAAAMASBQMAAAAAS/8fMqliDevEk7sAAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pisky/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/quantstats/_plotting/core.py:440: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  returns.fillna(0).resample(resample).apply(apply_fnc).resample(resample).last()\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 796x400 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x280 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x280 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x280 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x280 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x320 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 840.8x440 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pisky/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/quantstats/_plotting/core.py:1016: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  port[\"Weekly\"].ffill(inplace=True)\n",
      "/home/pisky/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/quantstats/_plotting/core.py:1018: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  port[\"Monthly\"] = port[\"Daily\"].resample(\"M\").apply(apply_fnc)\n",
      "/home/pisky/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/quantstats/_plotting/core.py:1019: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  port[\"Monthly\"].ffill(inplace=True)\n",
      "/home/pisky/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/quantstats/_plotting/core.py:1021: FutureWarning: 'Q' is deprecated and will be removed in a future version, please use 'QE' instead.\n",
      "  port[\"Quarterly\"] = port[\"Daily\"].resample(\"Q\").apply(apply_fnc)\n",
      "/home/pisky/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/quantstats/_plotting/core.py:1022: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  port[\"Quarterly\"].ffill(inplace=True)\n",
      "/home/pisky/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/quantstats/_plotting/core.py:1024: FutureWarning: 'A' is deprecated and will be removed in a future version, please use 'YE' instead.\n",
      "  port[\"Yearly\"] = port[\"Daily\"].resample(\"A\").apply(apply_fnc)\n",
      "/home/pisky/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/quantstats/_plotting/core.py:1025: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  port[\"Yearly\"].ffill(inplace=True)\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pisky/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.prod with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n",
      "/home/pisky/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.prod with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n",
      "/home/pisky/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.prod with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n",
      "/home/pisky/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/quantstats/stats.py:510: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  returns = _utils._prepare_returns(returns, rf).resample(resolution).sum()\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "/home/pisky/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/quantstats/_plotting/wrappers.py:565: FutureWarning: 'A' is deprecated and will be removed in a future version, please use 'YE' instead.\n",
      "  returns = returns.resample(\"A\").apply(_stats.comp)\n",
      "/home/pisky/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/quantstats/_plotting/wrappers.py:568: FutureWarning: 'A' is deprecated and will be removed in a future version, please use 'YE' instead.\n",
      "  returns = returns.resample(\"A\").last()\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "/home/pisky/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/quantstats/_plotting/core.py:440: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  returns.fillna(0).resample(resample).apply(apply_fnc).resample(resample).last()\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "/home/pisky/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/quantstats/_plotting/core.py:1016: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  port[\"Weekly\"].ffill(inplace=True)\n",
      "/home/pisky/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/quantstats/_plotting/core.py:1018: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  port[\"Monthly\"] = port[\"Daily\"].resample(\"M\").apply(apply_fnc)\n",
      "/home/pisky/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/quantstats/_plotting/core.py:1019: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  port[\"Monthly\"].ffill(inplace=True)\n",
      "/home/pisky/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/quantstats/_plotting/core.py:1021: FutureWarning: 'Q' is deprecated and will be removed in a future version, please use 'QE' instead.\n",
      "  port[\"Quarterly\"] = port[\"Daily\"].resample(\"Q\").apply(apply_fnc)\n",
      "/home/pisky/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/quantstats/_plotting/core.py:1022: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  port[\"Quarterly\"].ffill(inplace=True)\n",
      "/home/pisky/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/quantstats/_plotting/core.py:1024: FutureWarning: 'A' is deprecated and will be removed in a future version, please use 'YE' instead.\n",
      "  port[\"Yearly\"] = port[\"Daily\"].resample(\"A\").apply(apply_fnc)\n",
      "/home/pisky/PyCharm enviroments/mem-ind/lib/python3.10/site-packages/quantstats/_plotting/core.py:1025: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  port[\"Yearly\"].ffill(inplace=True)\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n",
      "findfont: Font family 'Arial' not found.\n"
     ]
    }
   ],
   "execution_count": 202
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T08:24:22.039636Z",
     "start_time": "2024-04-18T08:24:22.036223Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b84d320949ab7340",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 200
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
