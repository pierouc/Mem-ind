{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 01:17:55.987950: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-15 01:17:57.489866: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from gym_anytrading.envs import StocksEnv\n",
    "from finta import TA\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T05:17:58.862765Z",
     "start_time": "2024-04-15T05:17:53.898129Z"
    }
   },
   "id": "3f30d7eff5e960be",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tickers_negro = [\n",
    "    (\"NVDA\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"INTC\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"FRT\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"NKE\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"TSM\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"USB\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"XOM\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"BA\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"NEM\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"HBAN\", \"2012-01-01\", \"2023-12-31\"),\n",
    "\n",
    "    (\"VZ\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"PCG\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"FCX\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"C\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"OXY\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"KEY\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"WFC\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"MRO\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"GOOGL\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"NFLX\", \"2012-01-01\", \"2023-12-31\"),\n",
    "\n",
    "    (\"HDB\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"MU\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"AVY\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"MET\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"MSTR\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"WMB\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"BSX\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"EBAY\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"SO\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"CTSH\", \"2012-01-01\", \"2023-12-31\"),\n",
    "\n",
    "    (\"BBWI\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"V\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"VFC\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"MOS\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"CRM\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"SCHW\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"CNP\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"MDT\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"EXC\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"MTCH\", \"2012-01-01\", \"2023-12-31\"),\n",
    "\n",
    "    (\"PARA\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"UAL\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"JNJ\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"LUV\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"MCD\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"JPM\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"GILD\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"CVS\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"ORCL\", \"2012-01-01\", \"2023-12-31\"),\n",
    "    (\"UHS\", \"2012-01-01\", \"2023-12-31\")\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T05:21:24.367668Z",
     "start_time": "2024-04-15T05:21:24.354733Z"
    }
   },
   "id": "be1d7d6cd209d8c4",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "RSI"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c61b8fd8b31c21b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/plain": "                  Open        High         Low       Close   Adj Close  \\\nDate                                                                     \n2013-01-02    3.140000    3.182500    3.127500    3.180000    2.935840   \n2013-01-03    3.180000    3.217500    3.145000    3.182500    2.938148   \n2013-01-04    3.187500    3.297500    3.177500    3.287500    3.035087   \n2013-01-07    3.285000    3.295000    3.170000    3.192500    2.947381   \n2013-01-08    3.200000    3.210000    3.100000    3.122500    2.882756   \n...                ...         ...         ...         ...         ...   \n2023-12-22  151.500000  153.570007  151.250000  153.080002  152.890701   \n2023-12-26  153.279999  155.179993  152.630005  154.130005  153.939407   \n2023-12-27  153.929993  153.949997  151.699997  152.610001  152.421280   \n2023-12-28  152.460007  153.759995  151.910004  152.679993  152.491180   \n2023-12-29  152.699997  153.050003  151.350006  152.440002  152.251495   \n\n              Volume        rsi      macd  macd_signal      bb_bbu  \\\nDate                                                                 \n2013-01-02  47883600  60.223811  0.011304     0.011125    3.206539   \n2013-01-03  29888800  60.423354  0.017204     0.012340    3.216986   \n2013-01-04  52496800  67.742597  0.029362     0.015745    3.246299   \n2013-01-07  61073200  57.399520  0.031557     0.018907    3.249990   \n2013-01-08  46642400  51.196690  0.027762     0.020678    3.243191   \n...              ...        ...       ...          ...         ...   \n2023-12-22    382700  71.002013  5.003540     4.321617  156.466951   \n2023-12-26    288000  72.122511  5.169724     4.491238  157.901990   \n2023-12-27    374400  68.024727  5.119908     4.616972  158.752434   \n2023-12-28    332900  68.114559  5.028107     4.699199  159.373893   \n2023-12-29    239900  67.415174  4.879763     4.735312  159.932114   \n\n                bb_bbl          obv  from  \nDate                                       \n2013-01-02    2.971461 -390527600.0   0.0  \n2013-01-03    2.976764 -360638800.0   0.0  \n2013-01-04    2.977201 -308142000.0   0.0  \n2013-01-07    2.993260 -369215200.0   0.0  \n2013-01-08    3.013309 -415857600.0   0.0  \n...                ...          ...   ...  \n2023-12-22  129.343047   71061900.0   0.0  \n2023-12-26  129.598008   71349900.0   0.0  \n2023-12-27  130.400564   70975500.0   0.0  \n2023-12-28  131.472104   71308400.0   0.0  \n2023-12-29  132.409885   71068500.0   0.0  \n\n[138400 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Adj Close</th>\n      <th>Volume</th>\n      <th>rsi</th>\n      <th>macd</th>\n      <th>macd_signal</th>\n      <th>bb_bbu</th>\n      <th>bb_bbl</th>\n      <th>obv</th>\n      <th>from</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2013-01-02</th>\n      <td>3.140000</td>\n      <td>3.182500</td>\n      <td>3.127500</td>\n      <td>3.180000</td>\n      <td>2.935840</td>\n      <td>47883600</td>\n      <td>60.223811</td>\n      <td>0.011304</td>\n      <td>0.011125</td>\n      <td>3.206539</td>\n      <td>2.971461</td>\n      <td>-390527600.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2013-01-03</th>\n      <td>3.180000</td>\n      <td>3.217500</td>\n      <td>3.145000</td>\n      <td>3.182500</td>\n      <td>2.938148</td>\n      <td>29888800</td>\n      <td>60.423354</td>\n      <td>0.017204</td>\n      <td>0.012340</td>\n      <td>3.216986</td>\n      <td>2.976764</td>\n      <td>-360638800.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2013-01-04</th>\n      <td>3.187500</td>\n      <td>3.297500</td>\n      <td>3.177500</td>\n      <td>3.287500</td>\n      <td>3.035087</td>\n      <td>52496800</td>\n      <td>67.742597</td>\n      <td>0.029362</td>\n      <td>0.015745</td>\n      <td>3.246299</td>\n      <td>2.977201</td>\n      <td>-308142000.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2013-01-07</th>\n      <td>3.285000</td>\n      <td>3.295000</td>\n      <td>3.170000</td>\n      <td>3.192500</td>\n      <td>2.947381</td>\n      <td>61073200</td>\n      <td>57.399520</td>\n      <td>0.031557</td>\n      <td>0.018907</td>\n      <td>3.249990</td>\n      <td>2.993260</td>\n      <td>-369215200.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2013-01-08</th>\n      <td>3.200000</td>\n      <td>3.210000</td>\n      <td>3.100000</td>\n      <td>3.122500</td>\n      <td>2.882756</td>\n      <td>46642400</td>\n      <td>51.196690</td>\n      <td>0.027762</td>\n      <td>0.020678</td>\n      <td>3.243191</td>\n      <td>3.013309</td>\n      <td>-415857600.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2023-12-22</th>\n      <td>151.500000</td>\n      <td>153.570007</td>\n      <td>151.250000</td>\n      <td>153.080002</td>\n      <td>152.890701</td>\n      <td>382700</td>\n      <td>71.002013</td>\n      <td>5.003540</td>\n      <td>4.321617</td>\n      <td>156.466951</td>\n      <td>129.343047</td>\n      <td>71061900.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2023-12-26</th>\n      <td>153.279999</td>\n      <td>155.179993</td>\n      <td>152.630005</td>\n      <td>154.130005</td>\n      <td>153.939407</td>\n      <td>288000</td>\n      <td>72.122511</td>\n      <td>5.169724</td>\n      <td>4.491238</td>\n      <td>157.901990</td>\n      <td>129.598008</td>\n      <td>71349900.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2023-12-27</th>\n      <td>153.929993</td>\n      <td>153.949997</td>\n      <td>151.699997</td>\n      <td>152.610001</td>\n      <td>152.421280</td>\n      <td>374400</td>\n      <td>68.024727</td>\n      <td>5.119908</td>\n      <td>4.616972</td>\n      <td>158.752434</td>\n      <td>130.400564</td>\n      <td>70975500.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2023-12-28</th>\n      <td>152.460007</td>\n      <td>153.759995</td>\n      <td>151.910004</td>\n      <td>152.679993</td>\n      <td>152.491180</td>\n      <td>332900</td>\n      <td>68.114559</td>\n      <td>5.028107</td>\n      <td>4.699199</td>\n      <td>159.373893</td>\n      <td>131.472104</td>\n      <td>71308400.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2023-12-29</th>\n      <td>152.699997</td>\n      <td>153.050003</td>\n      <td>151.350006</td>\n      <td>152.440002</td>\n      <td>152.251495</td>\n      <td>239900</td>\n      <td>67.415174</td>\n      <td>4.879763</td>\n      <td>4.735312</td>\n      <td>159.932114</td>\n      <td>132.409885</td>\n      <td>71068500.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>138400 rows Ã— 13 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "datas = {}\n",
    "\n",
    "for k,i in enumerate(tickers_negro): \n",
    "    datas[i[0]] = yf.download(i[0], start=i[1], end= i[2],interval=\"1d\")\n",
    "\n",
    "    # prices = np.array(data['Adj Close'].values) #Adj Close prices\n",
    "    dates = np.array(datas[i[0]].index.values).astype('datetime64[ns]') #Dates\n",
    "    datas[i[0]]['rsi'] = TA.RSI(datas[i[0]],14,column='adj close') #Relative Strength Index (RSI)\n",
    "    datas[i[0]]['macd'] = TA.MACD(datas[i[0]],column='adj close')['MACD'] #MACD Line\n",
    "    datas[i[0]]['macd_signal'] = TA.MACD(datas[i[0]], column='adj close')['SIGNAL'] #MACD Signal Line\n",
    "    # datas[i[0]]['bb_bbm'] = TA.BBANDS(datas[i[0]], column='adj close')['BB_MIDDLE'] #Bollinger Bands (BB) middle band (BBM)\n",
    "    datas[i[0]]['bb_bbu'] = TA.BBANDS(datas[i[0]], column='adj close')['BB_UPPER'] #Bollinger Bands (BB) upper band (BBU)\n",
    "    datas[i[0]]['bb_bbl'] = TA.BBANDS(datas[i[0]],column='adj close')['BB_LOWER'] #Bollinger Bands (BB) lower band (BBL)\n",
    "    # datas[i[0]]['bb_width'] = TA.BBWIDTH(datas[i[0]],column='adj close') #Bollinger Bands (BB) width\n",
    "    datas[i[0]]['obv'] = TA.OBV(datas[i[0]],'adj close') #On Balance Volume (OBV)\n",
    "    # datas[i[0]]['from'] = pd.DataFrame([k] * len(datas[i[0]]))\n",
    "    \n",
    "    datas[i[0]].fillna(0, inplace=True)\n",
    "    datas[i[0]] = datas[i[0]][datas[i[0]].index > '2013-01-01']\n",
    "    datas[i[0]]\n",
    "    \n",
    "df= pd.concat([df for df in datas.values()], axis=0)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T05:22:14.878337Z",
     "start_time": "2024-04-15T05:21:29.693682Z"
    }
   },
   "id": "b28d72a1a8c1956d",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/plain": "2768"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = yf.download('GOOG', start='2012-01-01', end='2023-12-31' ,interval=\"1d\")\n",
    "prices = np.array(data['Adj Close'].values) #Adj Close prices\n",
    "\n",
    "\n",
    "dates = np.array(data.index.values).astype('datetime64[ns]') #Dates\n",
    "data['rsi'] = TA.RSI(data,14,column='adj close') #Relative Strength Index (RSI)\n",
    "data['macd'] = TA.MACD(data,column='adj close')['MACD'] #MACD Line\n",
    "data['macd_signal'] = TA.MACD(data, column='adj close')['SIGNAL'] #MACD Signal Line\n",
    "# data['bb_bbm'] = TA.BBANDS(data, column='adj close')['BB_MIDDLE'] #Bollinger Bands (BB) middle band (BBM)\n",
    "data['bb_bbu'] = TA.BBANDS(data, column='adj close')['BB_UPPER'] #Bollinger Bands (BB) upper band (BBU)\n",
    "data['bb_bbl'] = TA.BBANDS(data,column='adj close')['BB_LOWER'] #Bollinger Bands (BB) lower band (BBL)\n",
    "# data['bb_width'] = TA.BBWIDTH(data,column='adj close') #Bollinger Bands (BB) width\n",
    "data['obv'] = TA.OBV(data,'adj close') #On Balance Volume (OBV)\n",
    "\n",
    "data.fillna(0, inplace=True)\n",
    "data = data[data.index > '2013-01-01']\n",
    "len(data)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T08:14:56.453031Z",
     "start_time": "2024-04-15T08:14:56.367219Z"
    }
   },
   "id": "615e4f3698685dfc",
   "execution_count": 60
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Enviroment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29948330e9554f1f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def add_signals(env):\n",
    "    start = env.frame_bound[0] - env.window_size\n",
    "    end = env.frame_bound[1]\n",
    "    prices = env.df.loc[:, 'Adj Close'].to_numpy()[start:end]\n",
    "    signal_features = env.df.loc[:,['Adj Close','rsi','Volume','macd','macd_signal','bb_bbu','bb_bbl','obv']].to_numpy()[start:end]\n",
    "    return prices, signal_features\n",
    "\n",
    "# def reward_function(env,action):\n",
    "#     pass\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T06:12:55.836009Z",
     "start_time": "2024-04-15T06:12:55.830366Z"
    }
   },
   "id": "ef126a3fef2e014b",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class customEnv(StocksEnv):\n",
    "    \"\"\"\n",
    "    Custom Environment for RL trading\n",
    "    \"\"\"\n",
    "    _process_data = add_signals    \n",
    "\n",
    "    # _calculate_reward = reward_function\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T06:12:56.310351Z",
     "start_time": "2024-04-15T06:12:56.305604Z"
    }
   },
   "id": "d4ea84ef234a5f4a",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "996648e2a345f8d4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Eval num_timesteps=500, episode_reward=-2.47 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | -2.47     |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.684    |\n",
      "|    explained_variance | -1.58e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -0.0209   |\n",
      "|    value_loss         | 0.00358   |\n",
      "-------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 50  |\n",
      "|    iterations      | 100 |\n",
      "|    time_elapsed    | 9   |\n",
      "|    total_timesteps | 500 |\n",
      "----------------------------\n",
      "Eval num_timesteps=1000, episode_reward=4.91 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 4.91     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.692   |\n",
      "|    explained_variance | 0.58     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -0.0363  |\n",
      "|    value_loss         | 0.00339  |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 48   |\n",
      "|    iterations      | 200  |\n",
      "|    time_elapsed    | 20   |\n",
      "|    total_timesteps | 1000 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=1500, episode_reward=11.84 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 11.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.557   |\n",
      "|    explained_variance | -8.31    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 0.0185   |\n",
      "|    value_loss         | 0.00227  |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 48   |\n",
      "|    iterations      | 300  |\n",
      "|    time_elapsed    | 30   |\n",
      "|    total_timesteps | 1500 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-2.23 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | -2.23    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.682   |\n",
      "|    explained_variance | -4.28    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -0.112   |\n",
      "|    value_loss         | 0.0237   |\n",
      "------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 49   |\n",
      "|    iterations      | 400  |\n",
      "|    time_elapsed    | 40   |\n",
      "|    total_timesteps | 2000 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-2.90 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | -2.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.671   |\n",
      "|    explained_variance | 0.307    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -0.08    |\n",
      "|    value_loss         | 0.0161   |\n",
      "------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 49   |\n",
      "|    iterations      | 500  |\n",
      "|    time_elapsed    | 50   |\n",
      "|    total_timesteps | 2500 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=3000, episode_reward=1.77 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 1.77     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.692   |\n",
      "|    explained_variance | -146     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -0.3     |\n",
      "|    value_loss         | 0.181    |\n",
      "------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 49   |\n",
      "|    iterations      | 600  |\n",
      "|    time_elapsed    | 60   |\n",
      "|    total_timesteps | 3000 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=3500, episode_reward=11.78 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 11.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.497   |\n",
      "|    explained_variance | -22.6    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -0.00397 |\n",
      "|    value_loss         | 0.00107  |\n",
      "------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 47   |\n",
      "|    iterations      | 700  |\n",
      "|    time_elapsed    | 74   |\n",
      "|    total_timesteps | 3500 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=11.80 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 11.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.396   |\n",
      "|    explained_variance | 0.952    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.0317  |\n",
      "|    value_loss         | 0.00185  |\n",
      "------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 44   |\n",
      "|    iterations      | 800  |\n",
      "|    time_elapsed    | 89   |\n",
      "|    total_timesteps | 4000 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4500, episode_reward=17.74 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 17.7      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.493    |\n",
      "|    explained_variance | -4.66e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -0.0192   |\n",
      "|    value_loss         | 0.00542   |\n",
      "-------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 44   |\n",
      "|    iterations      | 900  |\n",
      "|    time_elapsed    | 100  |\n",
      "|    total_timesteps | 4500 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=5000, episode_reward=12.89 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 12.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.287   |\n",
      "|    explained_variance | -7.67    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -0.0556  |\n",
      "|    value_loss         | 0.00294  |\n",
      "------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 43   |\n",
      "|    iterations      | 1000 |\n",
      "|    time_elapsed    | 116  |\n",
      "|    total_timesteps | 5000 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=5500, episode_reward=15.70 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 15.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.416   |\n",
      "|    explained_variance | -23      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.00493 |\n",
      "|    value_loss         | 0.00131  |\n",
      "------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 42   |\n",
      "|    iterations      | 1100 |\n",
      "|    time_elapsed    | 130  |\n",
      "|    total_timesteps | 5500 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6000, episode_reward=17.94 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.408   |\n",
      "|    explained_variance | -84.6    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.0047  |\n",
      "|    value_loss         | 0.00312  |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 42   |\n",
      "|    iterations      | 1200 |\n",
      "|    time_elapsed    | 142  |\n",
      "|    total_timesteps | 6000 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6500, episode_reward=15.48 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 15.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.424   |\n",
      "|    explained_variance | -111     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -0.0896  |\n",
      "|    value_loss         | 0.00909  |\n",
      "------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 41   |\n",
      "|    iterations      | 1300 |\n",
      "|    time_elapsed    | 157  |\n",
      "|    total_timesteps | 6500 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=7000, episode_reward=15.46 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 15.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.352   |\n",
      "|    explained_variance | -108     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -0.00796 |\n",
      "|    value_loss         | 0.00157  |\n",
      "------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 40   |\n",
      "|    iterations      | 1400 |\n",
      "|    time_elapsed    | 170  |\n",
      "|    total_timesteps | 7000 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=7500, episode_reward=11.86 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 11.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.375   |\n",
      "|    explained_variance | -289     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -0.0119  |\n",
      "|    value_loss         | 0.00204  |\n",
      "------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 40   |\n",
      "|    iterations      | 1500 |\n",
      "|    time_elapsed    | 185  |\n",
      "|    total_timesteps | 7500 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8000, episode_reward=10.48 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 10.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.358   |\n",
      "|    explained_variance | -55.8    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | -0.0191  |\n",
      "|    value_loss         | 0.00286  |\n",
      "------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 41   |\n",
      "|    iterations      | 1600 |\n",
      "|    time_elapsed    | 195  |\n",
      "|    total_timesteps | 8000 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8500, episode_reward=11.78 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 11.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.481   |\n",
      "|    explained_variance | -11.5    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -0.0164  |\n",
      "|    value_loss         | 0.00438  |\n",
      "------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 40   |\n",
      "|    iterations      | 1700 |\n",
      "|    time_elapsed    | 209  |\n",
      "|    total_timesteps | 8500 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=9000, episode_reward=8.82 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 8.82     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.515   |\n",
      "|    explained_variance | -15.8    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -0.0182  |\n",
      "|    value_loss         | 0.00378  |\n",
      "------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 40   |\n",
      "|    iterations      | 1800 |\n",
      "|    time_elapsed    | 224  |\n",
      "|    total_timesteps | 9000 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=9500, episode_reward=-2.54 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | -2.54    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.602   |\n",
      "|    explained_variance | -7.51    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -0.0136  |\n",
      "|    value_loss         | 0.00209  |\n",
      "------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 40   |\n",
      "|    iterations      | 1900 |\n",
      "|    time_elapsed    | 236  |\n",
      "|    total_timesteps | 9500 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-3.42 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | -3.42    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.511   |\n",
      "|    explained_variance | -13.7    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 0.00553  |\n",
      "|    value_loss         | 0.000732 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 39    |\n",
      "|    iterations      | 2000  |\n",
      "|    time_elapsed    | 251   |\n",
      "|    total_timesteps | 10000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=-3.12 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | -3.12    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.545   |\n",
      "|    explained_variance | -7.92    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | -0.324   |\n",
      "|    value_loss         | 0.182    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 39    |\n",
      "|    iterations      | 2100  |\n",
      "|    time_elapsed    | 263   |\n",
      "|    total_timesteps | 10500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=-1.58 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | -1.58    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.609   |\n",
      "|    explained_variance | 0.695    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | -0.0481  |\n",
      "|    value_loss         | 0.00299  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 39    |\n",
      "|    iterations      | 2200  |\n",
      "|    time_elapsed    | 278   |\n",
      "|    total_timesteps | 11000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=4.96 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 4.96     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.614   |\n",
      "|    explained_variance | -2.15    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | -0.0141  |\n",
      "|    value_loss         | 0.000721 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 39    |\n",
      "|    iterations      | 2300  |\n",
      "|    time_elapsed    | 289   |\n",
      "|    total_timesteps | 11500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=9.11 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 9.11     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.508   |\n",
      "|    explained_variance | -47.2    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | -0.0278  |\n",
      "|    value_loss         | 0.00175  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 39    |\n",
      "|    iterations      | 2400  |\n",
      "|    time_elapsed    | 302   |\n",
      "|    total_timesteps | 12000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=8.90 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 8.9      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.493   |\n",
      "|    explained_variance | -27      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2499     |\n",
      "|    policy_loss        | -0.00292 |\n",
      "|    value_loss         | 0.00099  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 39    |\n",
      "|    iterations      | 2500  |\n",
      "|    time_elapsed    | 318   |\n",
      "|    total_timesteps | 12500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=0.32 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 0.318    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.552   |\n",
      "|    explained_variance | 0.512    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2599     |\n",
      "|    policy_loss        | -0.0251  |\n",
      "|    value_loss         | 0.00271  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 39    |\n",
      "|    iterations      | 2600  |\n",
      "|    time_elapsed    | 330   |\n",
      "|    total_timesteps | 13000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=-2.83 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | -2.83    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.443   |\n",
      "|    explained_variance | -4.54    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2699     |\n",
      "|    policy_loss        | -0.00383 |\n",
      "|    value_loss         | 0.000517 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 39    |\n",
      "|    iterations      | 2700  |\n",
      "|    time_elapsed    | 345   |\n",
      "|    total_timesteps | 13500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=3.79 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 3.79     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.49    |\n",
      "|    explained_variance | -30.9    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2799     |\n",
      "|    policy_loss        | -0.0129  |\n",
      "|    value_loss         | 0.00128  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 39    |\n",
      "|    iterations      | 2800  |\n",
      "|    time_elapsed    | 357   |\n",
      "|    total_timesteps | 14000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=5.20 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 5.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.467   |\n",
      "|    explained_variance | -1.77    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2899     |\n",
      "|    policy_loss        | 0.0089   |\n",
      "|    value_loss         | 0.000227 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 38    |\n",
      "|    iterations      | 2900  |\n",
      "|    time_elapsed    | 372   |\n",
      "|    total_timesteps | 14500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=1.24 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 1.24     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.509   |\n",
      "|    explained_variance | 0.132    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | 0.00159  |\n",
      "|    value_loss         | 0.00542  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 39    |\n",
      "|    iterations      | 3000  |\n",
      "|    time_elapsed    | 382   |\n",
      "|    total_timesteps | 15000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 0.743    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.47    |\n",
      "|    explained_variance | -0.41    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3099     |\n",
      "|    policy_loss        | 0.0377   |\n",
      "|    value_loss         | 0.00497  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 39    |\n",
      "|    iterations      | 3100  |\n",
      "|    time_elapsed    | 395   |\n",
      "|    total_timesteps | 15500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=5.34 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 5.34     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.448   |\n",
      "|    explained_variance | -17.4    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3199     |\n",
      "|    policy_loss        | -0.00191 |\n",
      "|    value_loss         | 0.000804 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 38    |\n",
      "|    iterations      | 3200  |\n",
      "|    time_elapsed    | 412   |\n",
      "|    total_timesteps | 16000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=10.26 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 10.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 16500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.43    |\n",
      "|    explained_variance | -27.4    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3299     |\n",
      "|    policy_loss        | -0.00789 |\n",
      "|    value_loss         | 0.000472 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 39    |\n",
      "|    iterations      | 3300  |\n",
      "|    time_elapsed    | 423   |\n",
      "|    total_timesteps | 16500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=16.56 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 16.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 17000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.329   |\n",
      "|    explained_variance | -57.9    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3399     |\n",
      "|    policy_loss        | -0.00637 |\n",
      "|    value_loss         | 0.00194  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 38    |\n",
      "|    iterations      | 3400  |\n",
      "|    time_elapsed    | 437   |\n",
      "|    total_timesteps | 17000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=10.35 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 10.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 17500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.276   |\n",
      "|    explained_variance | -566     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3499     |\n",
      "|    policy_loss        | -0.137   |\n",
      "|    value_loss         | 0.102    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 38    |\n",
      "|    iterations      | 3500  |\n",
      "|    time_elapsed    | 449   |\n",
      "|    total_timesteps | 17500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=11.32 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 11.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.237   |\n",
      "|    explained_variance | -248     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3599     |\n",
      "|    policy_loss        | -0.00581 |\n",
      "|    value_loss         | 0.00143  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 38    |\n",
      "|    iterations      | 3600  |\n",
      "|    time_elapsed    | 464   |\n",
      "|    total_timesteps | 18000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=12.90 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 12.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 18500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.261   |\n",
      "|    explained_variance | -50.1    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3699     |\n",
      "|    policy_loss        | -0.00334 |\n",
      "|    value_loss         | 0.00134  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 38    |\n",
      "|    iterations      | 3700  |\n",
      "|    time_elapsed    | 475   |\n",
      "|    total_timesteps | 18500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=12.65 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 12.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 19000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.403   |\n",
      "|    explained_variance | -32.2    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3799     |\n",
      "|    policy_loss        | -0.00583 |\n",
      "|    value_loss         | 0.00268  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 38    |\n",
      "|    iterations      | 3800  |\n",
      "|    time_elapsed    | 488   |\n",
      "|    total_timesteps | 19000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=8.27 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 8.27     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 19500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.476   |\n",
      "|    explained_variance | -9.66    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3899     |\n",
      "|    policy_loss        | -0.0104  |\n",
      "|    value_loss         | 0.00215  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 38    |\n",
      "|    iterations      | 3900  |\n",
      "|    time_elapsed    | 504   |\n",
      "|    total_timesteps | 19500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 0.672    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.591   |\n",
      "|    explained_variance | 0.138    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | -0.0278  |\n",
      "|    value_loss         | 0.00202  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 38    |\n",
      "|    iterations      | 4000  |\n",
      "|    time_elapsed    | 516   |\n",
      "|    total_timesteps | 20000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=12.65 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 12.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 20500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.317   |\n",
      "|    explained_variance | -130     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4099     |\n",
      "|    policy_loss        | -0.113   |\n",
      "|    value_loss         | 0.127    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 38    |\n",
      "|    iterations      | 4100  |\n",
      "|    time_elapsed    | 533   |\n",
      "|    total_timesteps | 20500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=13.24 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 13.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 21000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.25    |\n",
      "|    explained_variance | -14.6    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4199     |\n",
      "|    policy_loss        | -0.00295 |\n",
      "|    value_loss         | 0.000646 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 38    |\n",
      "|    iterations      | 4200  |\n",
      "|    time_elapsed    | 545   |\n",
      "|    total_timesteps | 21000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=12.09 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 12.1      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 21500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.182    |\n",
      "|    explained_variance | -59.9     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4299      |\n",
      "|    policy_loss        | -0.000823 |\n",
      "|    value_loss         | 0.000387  |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 38    |\n",
      "|    iterations      | 4300  |\n",
      "|    time_elapsed    | 554   |\n",
      "|    total_timesteps | 21500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=12.74 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 12.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 22000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.236   |\n",
      "|    explained_variance | -137     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4399     |\n",
      "|    policy_loss        | -0.143   |\n",
      "|    value_loss         | 0.154    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 39    |\n",
      "|    iterations      | 4400  |\n",
      "|    time_elapsed    | 563   |\n",
      "|    total_timesteps | 22000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=13.89 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 13.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 22500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.132   |\n",
      "|    explained_variance | -29.6    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4499     |\n",
      "|    policy_loss        | 0.000261 |\n",
      "|    value_loss         | 0.00401  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 39    |\n",
      "|    iterations      | 4500  |\n",
      "|    time_elapsed    | 573   |\n",
      "|    total_timesteps | 22500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=13.37 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 13.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 23000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.298   |\n",
      "|    explained_variance | -25.9    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4599     |\n",
      "|    policy_loss        | -0.00801 |\n",
      "|    value_loss         | 0.000976 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 39    |\n",
      "|    iterations      | 4600  |\n",
      "|    time_elapsed    | 582   |\n",
      "|    total_timesteps | 23000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=-1.64 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | -1.64    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 23500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.473   |\n",
      "|    explained_variance | -2.29    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4699     |\n",
      "|    policy_loss        | 0.00106  |\n",
      "|    value_loss         | 0.000818 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 39    |\n",
      "|    iterations      | 4700  |\n",
      "|    time_elapsed    | 592   |\n",
      "|    total_timesteps | 23500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=0.07 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 0.065    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 24000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.393   |\n",
      "|    explained_variance | -0.229   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4799     |\n",
      "|    policy_loss        | 0.049    |\n",
      "|    value_loss         | 0.0103   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 39    |\n",
      "|    iterations      | 4800  |\n",
      "|    time_elapsed    | 601   |\n",
      "|    total_timesteps | 24000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=-1.39 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | -1.39    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 24500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.256   |\n",
      "|    explained_variance | -197     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4899     |\n",
      "|    policy_loss        | -0.00564 |\n",
      "|    value_loss         | 0.00303  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 40    |\n",
      "|    iterations      | 4900  |\n",
      "|    time_elapsed    | 610   |\n",
      "|    total_timesteps | 24500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 0.635    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 25000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.376   |\n",
      "|    explained_variance | -12.1    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4999     |\n",
      "|    policy_loss        | -0.00046 |\n",
      "|    value_loss         | 2.8e-05  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 40    |\n",
      "|    iterations      | 5000  |\n",
      "|    time_elapsed    | 619   |\n",
      "|    total_timesteps | 25000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=-0.79 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | -0.787   |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 25500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.491   |\n",
      "|    explained_variance | -11.5    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5099     |\n",
      "|    policy_loss        | -0.00794 |\n",
      "|    value_loss         | 0.000805 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 40    |\n",
      "|    iterations      | 5100  |\n",
      "|    time_elapsed    | 629   |\n",
      "|    total_timesteps | 25500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=-0.52 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | -0.517   |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 26000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.497   |\n",
      "|    explained_variance | 0.0861   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5199     |\n",
      "|    policy_loss        | -0.00704 |\n",
      "|    value_loss         | 0.000335 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 40    |\n",
      "|    iterations      | 5200  |\n",
      "|    time_elapsed    | 638   |\n",
      "|    total_timesteps | 26000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=-0.30 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | -0.301   |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 26500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.364   |\n",
      "|    explained_variance | -3.27    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5299     |\n",
      "|    policy_loss        | -0.00629 |\n",
      "|    value_loss         | 0.000617 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 40    |\n",
      "|    iterations      | 5300  |\n",
      "|    time_elapsed    | 648   |\n",
      "|    total_timesteps | 26500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=13.62 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 13.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 27000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.296   |\n",
      "|    explained_variance | -222     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5399     |\n",
      "|    policy_loss        | -0.0548  |\n",
      "|    value_loss         | 0.134    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 41    |\n",
      "|    iterations      | 5400  |\n",
      "|    time_elapsed    | 657   |\n",
      "|    total_timesteps | 27000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=-0.53 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | -0.531   |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 27500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.513   |\n",
      "|    explained_variance | -0.642   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5499     |\n",
      "|    policy_loss        | -0.0272  |\n",
      "|    value_loss         | 0.00312  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 41    |\n",
      "|    iterations      | 5500  |\n",
      "|    time_elapsed    | 668   |\n",
      "|    total_timesteps | 27500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=2.60 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 2.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 28000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.308   |\n",
      "|    explained_variance | -12.1    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5599     |\n",
      "|    policy_loss        | -0.00508 |\n",
      "|    value_loss         | 0.000196 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 41    |\n",
      "|    iterations      | 5600  |\n",
      "|    time_elapsed    | 680   |\n",
      "|    total_timesteps | 28000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=10.10 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 10.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 28500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.222   |\n",
      "|    explained_variance | -3.58    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5699     |\n",
      "|    policy_loss        | -0.0327  |\n",
      "|    value_loss         | 0.15     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 41    |\n",
      "|    iterations      | 5700  |\n",
      "|    time_elapsed    | 693   |\n",
      "|    total_timesteps | 28500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=9.43 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 9.43     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 29000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.216   |\n",
      "|    explained_variance | -94.5    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5799     |\n",
      "|    policy_loss        | -0.0141  |\n",
      "|    value_loss         | 0.00123  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 41    |\n",
      "|    iterations      | 5800  |\n",
      "|    time_elapsed    | 703   |\n",
      "|    total_timesteps | 29000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=5.47 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 5.47     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 29500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.25    |\n",
      "|    explained_variance | -246     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5899     |\n",
      "|    policy_loss        | -0.0359  |\n",
      "|    value_loss         | 0.153    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 41    |\n",
      "|    iterations      | 5900  |\n",
      "|    time_elapsed    | 712   |\n",
      "|    total_timesteps | 29500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=8.33 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 8.33     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 30000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.253   |\n",
      "|    explained_variance | -20.9    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5999     |\n",
      "|    policy_loss        | -0.0275  |\n",
      "|    value_loss         | 0.00198  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 41    |\n",
      "|    iterations      | 6000  |\n",
      "|    time_elapsed    | 721   |\n",
      "|    total_timesteps | 30000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=14.27 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 14.3      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 30500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.228    |\n",
      "|    explained_variance | -1.01e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6099      |\n",
      "|    policy_loss        | -0.0325   |\n",
      "|    value_loss         | 0.12      |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 41    |\n",
      "|    iterations      | 6100  |\n",
      "|    time_elapsed    | 730   |\n",
      "|    total_timesteps | 30500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=12.16 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 12.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 31000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.235   |\n",
      "|    explained_variance | -53.3    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6199     |\n",
      "|    policy_loss        | 0.000944 |\n",
      "|    value_loss         | 0.000541 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 41    |\n",
      "|    iterations      | 6200  |\n",
      "|    time_elapsed    | 739   |\n",
      "|    total_timesteps | 31000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=-3.77 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | -3.77    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 31500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.157   |\n",
      "|    explained_variance | -81.1    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6299     |\n",
      "|    policy_loss        | 0.00114  |\n",
      "|    value_loss         | 0.00363  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 42    |\n",
      "|    iterations      | 6300  |\n",
      "|    time_elapsed    | 748   |\n",
      "|    total_timesteps | 31500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=1.19 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 1.19     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 32000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.416   |\n",
      "|    explained_variance | -0.378   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6399     |\n",
      "|    policy_loss        | 0.012    |\n",
      "|    value_loss         | 0.000415 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 42    |\n",
      "|    iterations      | 6400  |\n",
      "|    time_elapsed    | 757   |\n",
      "|    total_timesteps | 32000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=10.69 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 10.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 32500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.237   |\n",
      "|    explained_variance | -23.7    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6499     |\n",
      "|    policy_loss        | 0.00139  |\n",
      "|    value_loss         | 0.00102  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 42    |\n",
      "|    iterations      | 6500  |\n",
      "|    time_elapsed    | 766   |\n",
      "|    total_timesteps | 32500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=2.00 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 2        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 33000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.462   |\n",
      "|    explained_variance | -249     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6599     |\n",
      "|    policy_loss        | -0.082   |\n",
      "|    value_loss         | 0.142    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 42    |\n",
      "|    iterations      | 6600  |\n",
      "|    time_elapsed    | 775   |\n",
      "|    total_timesteps | 33000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=33500, episode_reward=-1.70 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | -1.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 33500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.397   |\n",
      "|    explained_variance | -0.778   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6699     |\n",
      "|    policy_loss        | 0.095    |\n",
      "|    value_loss         | 0.0278   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 42    |\n",
      "|    iterations      | 6700  |\n",
      "|    time_elapsed    | 784   |\n",
      "|    total_timesteps | 33500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=1.85 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 1.85     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 34000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.365   |\n",
      "|    explained_variance | -10.1    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6799     |\n",
      "|    policy_loss        | -0.034   |\n",
      "|    value_loss         | 0.00218  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 42    |\n",
      "|    iterations      | 6800  |\n",
      "|    time_elapsed    | 793   |\n",
      "|    total_timesteps | 34000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=34500, episode_reward=2.88 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 2.88     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 34500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.374   |\n",
      "|    explained_variance | -16.1    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6899     |\n",
      "|    policy_loss        | -0.0191  |\n",
      "|    value_loss         | 0.00117  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 42    |\n",
      "|    iterations      | 6900  |\n",
      "|    time_elapsed    | 802   |\n",
      "|    total_timesteps | 34500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=4.71 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 4.71     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 35000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.307   |\n",
      "|    explained_variance | -6.52    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6999     |\n",
      "|    policy_loss        | 0.0125   |\n",
      "|    value_loss         | 0.000932 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 43    |\n",
      "|    iterations      | 7000  |\n",
      "|    time_elapsed    | 811   |\n",
      "|    total_timesteps | 35000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=35500, episode_reward=1.16 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 1.16      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 35500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.495    |\n",
      "|    explained_variance | -3.28e+05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7099      |\n",
      "|    policy_loss        | -0.0976   |\n",
      "|    value_loss         | 0.142     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 43    |\n",
      "|    iterations      | 7100  |\n",
      "|    time_elapsed    | 820   |\n",
      "|    total_timesteps | 35500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=1.83 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 1.83     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 36000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.516   |\n",
      "|    explained_variance | -28      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7199     |\n",
      "|    policy_loss        | -0.152   |\n",
      "|    value_loss         | 0.137    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 43    |\n",
      "|    iterations      | 7200  |\n",
      "|    time_elapsed    | 829   |\n",
      "|    total_timesteps | 36000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=36500, episode_reward=1.93 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 1.93     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 36500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.559   |\n",
      "|    explained_variance | -3.83    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7299     |\n",
      "|    policy_loss        | -0.0291  |\n",
      "|    value_loss         | 0.00287  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 43    |\n",
      "|    iterations      | 7300  |\n",
      "|    time_elapsed    | 838   |\n",
      "|    total_timesteps | 36500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=5.33 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 5.33     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 37000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.381   |\n",
      "|    explained_variance | -84.3    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7399     |\n",
      "|    policy_loss        | -0.0512  |\n",
      "|    value_loss         | 0.138    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 43    |\n",
      "|    iterations      | 7400  |\n",
      "|    time_elapsed    | 848   |\n",
      "|    total_timesteps | 37000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=37500, episode_reward=13.46 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 13.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 37500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.197   |\n",
      "|    explained_variance | -24.6    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7499     |\n",
      "|    policy_loss        | -0.003   |\n",
      "|    value_loss         | 0.00165  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 43    |\n",
      "|    iterations      | 7500  |\n",
      "|    time_elapsed    | 856   |\n",
      "|    total_timesteps | 37500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=12.59 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 12.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 38000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.186   |\n",
      "|    explained_variance | -70.7    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7599     |\n",
      "|    policy_loss        | -0.00442 |\n",
      "|    value_loss         | 0.00173  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 43    |\n",
      "|    iterations      | 7600  |\n",
      "|    time_elapsed    | 865   |\n",
      "|    total_timesteps | 38000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=38500, episode_reward=2.27 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 2.27     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 38500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.241   |\n",
      "|    explained_variance | -828     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7699     |\n",
      "|    policy_loss        | -0.0312  |\n",
      "|    value_loss         | 0.131    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 44    |\n",
      "|    iterations      | 7700  |\n",
      "|    time_elapsed    | 874   |\n",
      "|    total_timesteps | 38500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=2.46 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 2.46     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 39000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.294   |\n",
      "|    explained_variance | -25.6    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7799     |\n",
      "|    policy_loss        | -0.0208  |\n",
      "|    value_loss         | 0.00124  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 44    |\n",
      "|    iterations      | 7800  |\n",
      "|    time_elapsed    | 883   |\n",
      "|    total_timesteps | 39000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=39500, episode_reward=2.98 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 2.98     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 39500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.368   |\n",
      "|    explained_variance | -11.2    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7899     |\n",
      "|    policy_loss        | -0.053   |\n",
      "|    value_loss         | 0.168    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 44    |\n",
      "|    iterations      | 7900  |\n",
      "|    time_elapsed    | 892   |\n",
      "|    total_timesteps | 39500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=3.58 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 3.58     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 40000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.284   |\n",
      "|    explained_variance | 0.627    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7999     |\n",
      "|    policy_loss        | -0.062   |\n",
      "|    value_loss         | 0.00393  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 44    |\n",
      "|    iterations      | 8000  |\n",
      "|    time_elapsed    | 900   |\n",
      "|    total_timesteps | 40000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=40500, episode_reward=2.68 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 2.68     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 40500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.411   |\n",
      "|    explained_variance | -408     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8099     |\n",
      "|    policy_loss        | -0.0614  |\n",
      "|    value_loss         | 0.142    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 44    |\n",
      "|    iterations      | 8100  |\n",
      "|    time_elapsed    | 909   |\n",
      "|    total_timesteps | 40500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=2.92 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 2.92      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 41000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.552    |\n",
      "|    explained_variance | -3.52e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8199      |\n",
      "|    policy_loss        | -0.21     |\n",
      "|    value_loss         | 0.209     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 44    |\n",
      "|    iterations      | 8200  |\n",
      "|    time_elapsed    | 918   |\n",
      "|    total_timesteps | 41000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=41500, episode_reward=0.51 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 0.513    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 41500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.541   |\n",
      "|    explained_variance | -0.928   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8299     |\n",
      "|    policy_loss        | -0.127   |\n",
      "|    value_loss         | 0.112    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 44    |\n",
      "|    iterations      | 8300  |\n",
      "|    time_elapsed    | 927   |\n",
      "|    total_timesteps | 41500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=-0.37 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | -0.365   |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 42000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.47    |\n",
      "|    explained_variance | -0.467   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8399     |\n",
      "|    policy_loss        | -0.148   |\n",
      "|    value_loss         | 0.165    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 44    |\n",
      "|    iterations      | 8400  |\n",
      "|    time_elapsed    | 936   |\n",
      "|    total_timesteps | 42000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=42500, episode_reward=1.44 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 1.44     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 42500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.527   |\n",
      "|    explained_variance | -356     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8499     |\n",
      "|    policy_loss        | -0.149   |\n",
      "|    value_loss         | 0.208    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 44    |\n",
      "|    iterations      | 8500  |\n",
      "|    time_elapsed    | 945   |\n",
      "|    total_timesteps | 42500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=-0.80 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | -0.802   |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 43000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.446   |\n",
      "|    explained_variance | -266     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8599     |\n",
      "|    policy_loss        | -0.0578  |\n",
      "|    value_loss         | 0.0786   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 45    |\n",
      "|    iterations      | 8600  |\n",
      "|    time_elapsed    | 954   |\n",
      "|    total_timesteps | 43000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=43500, episode_reward=1.13 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 1.13     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 43500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.539   |\n",
      "|    explained_variance | -1.87    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8699     |\n",
      "|    policy_loss        | 0.0656   |\n",
      "|    value_loss         | 0.00518  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 45    |\n",
      "|    iterations      | 8700  |\n",
      "|    time_elapsed    | 962   |\n",
      "|    total_timesteps | 43500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=4.08 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 4.08     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 44000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.473   |\n",
      "|    explained_variance | -14.1    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8799     |\n",
      "|    policy_loss        | -0.142   |\n",
      "|    value_loss         | 0.137    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 45    |\n",
      "|    iterations      | 8800  |\n",
      "|    time_elapsed    | 971   |\n",
      "|    total_timesteps | 44000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=44500, episode_reward=2.90 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 2.9      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 44500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.305   |\n",
      "|    explained_variance | 0.599    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8899     |\n",
      "|    policy_loss        | -0.1     |\n",
      "|    value_loss         | 0.00708  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 45    |\n",
      "|    iterations      | 8900  |\n",
      "|    time_elapsed    | 980   |\n",
      "|    total_timesteps | 44500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=14.66 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 14.7      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 45000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.202    |\n",
      "|    explained_variance | -1.92e+07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8999      |\n",
      "|    policy_loss        | -0.0455   |\n",
      "|    value_loss         | 0.14      |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 45    |\n",
      "|    iterations      | 9000  |\n",
      "|    time_elapsed    | 990   |\n",
      "|    total_timesteps | 45000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=45500, episode_reward=4.64 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 4.64     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 45500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.197   |\n",
      "|    explained_variance | -410     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9099     |\n",
      "|    policy_loss        | -0.0211  |\n",
      "|    value_loss         | 0.127    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 45    |\n",
      "|    iterations      | 9100  |\n",
      "|    time_elapsed    | 999   |\n",
      "|    total_timesteps | 45500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=4.82 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 4.82      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 46000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.188    |\n",
      "|    explained_variance | -1.22e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9199      |\n",
      "|    policy_loss        | -0.0185   |\n",
      "|    value_loss         | 0.136     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 45    |\n",
      "|    iterations      | 9200  |\n",
      "|    time_elapsed    | 1008  |\n",
      "|    total_timesteps | 46000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=46500, episode_reward=13.20 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 13.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 46500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.169   |\n",
      "|    explained_variance | -4.1e+03 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9299     |\n",
      "|    policy_loss        | -0.0373  |\n",
      "|    value_loss         | 0.145    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 45    |\n",
      "|    iterations      | 9300  |\n",
      "|    time_elapsed    | 1016  |\n",
      "|    total_timesteps | 46500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=16.92 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 16.9      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 47000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.157    |\n",
      "|    explained_variance | -7.51e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9399      |\n",
      "|    policy_loss        | -0.0279   |\n",
      "|    value_loss         | 0.143     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 45    |\n",
      "|    iterations      | 9400  |\n",
      "|    time_elapsed    | 1025  |\n",
      "|    total_timesteps | 47000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=47500, episode_reward=3.52 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 3.52     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 47500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.135   |\n",
      "|    explained_variance | -246     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9499     |\n",
      "|    policy_loss        | -0.0132  |\n",
      "|    value_loss         | 0.143    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 45    |\n",
      "|    iterations      | 9500  |\n",
      "|    time_elapsed    | 1033  |\n",
      "|    total_timesteps | 47500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=1.81 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 1.81     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 48000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.166   |\n",
      "|    explained_variance | -385     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9599     |\n",
      "|    policy_loss        | -0.0157  |\n",
      "|    value_loss         | 0.153    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 46    |\n",
      "|    iterations      | 9600  |\n",
      "|    time_elapsed    | 1042  |\n",
      "|    total_timesteps | 48000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=48500, episode_reward=5.33 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 5.33      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 48500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.227    |\n",
      "|    explained_variance | -2.27e+06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9699      |\n",
      "|    policy_loss        | -0.0244   |\n",
      "|    value_loss         | 0.155     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 46    |\n",
      "|    iterations      | 9700  |\n",
      "|    time_elapsed    | 1052  |\n",
      "|    total_timesteps | 48500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=2.26 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 2.26     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 49000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.164   |\n",
      "|    explained_variance | -36.9    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9799     |\n",
      "|    policy_loss        | -0.247   |\n",
      "|    value_loss         | 0.157    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 46    |\n",
      "|    iterations      | 9800  |\n",
      "|    time_elapsed    | 1062  |\n",
      "|    total_timesteps | 49000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=49500, episode_reward=11.46 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 11.5      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 49500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.153    |\n",
      "|    explained_variance | -5.37e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9899      |\n",
      "|    policy_loss        | -0.0571   |\n",
      "|    value_loss         | 0.143     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 46    |\n",
      "|    iterations      | 9900  |\n",
      "|    time_elapsed    | 1072  |\n",
      "|    total_timesteps | 49500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=10.96 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 11       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 50000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0848  |\n",
      "|    explained_variance | -83.5    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9999     |\n",
      "|    policy_loss        | 0.000165 |\n",
      "|    value_loss         | 0.000529 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 46    |\n",
      "|    iterations      | 10000 |\n",
      "|    time_elapsed    | 1083  |\n",
      "|    total_timesteps | 50000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=50500, episode_reward=4.48 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 4.48      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 50500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.276    |\n",
      "|    explained_variance | -4.92e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10099     |\n",
      "|    policy_loss        | -0.0353   |\n",
      "|    value_loss         | 0.159     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 46    |\n",
      "|    iterations      | 10100 |\n",
      "|    time_elapsed    | 1092  |\n",
      "|    total_timesteps | 50500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=-0.68 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | -0.677   |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 51000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.445   |\n",
      "|    explained_variance | -0.55    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10199    |\n",
      "|    policy_loss        | -0.0923  |\n",
      "|    value_loss         | 0.0951   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 46    |\n",
      "|    iterations      | 10200 |\n",
      "|    time_elapsed    | 1101  |\n",
      "|    total_timesteps | 51000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=51500, episode_reward=4.13 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 4.13     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 51500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.464   |\n",
      "|    explained_variance | -0.443   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10299    |\n",
      "|    policy_loss        | -0.11    |\n",
      "|    value_loss         | 0.136    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 46    |\n",
      "|    iterations      | 10300 |\n",
      "|    time_elapsed    | 1110  |\n",
      "|    total_timesteps | 51500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=5.53 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 5.53     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 52000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.179   |\n",
      "|    explained_variance | -796     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10399    |\n",
      "|    policy_loss        | -0.0245  |\n",
      "|    value_loss         | 0.145    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 46    |\n",
      "|    iterations      | 10400 |\n",
      "|    time_elapsed    | 1120  |\n",
      "|    total_timesteps | 52000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=52500, episode_reward=5.50 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 5.5      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 52500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.382   |\n",
      "|    explained_variance | -92.8    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10499    |\n",
      "|    policy_loss        | -0.154   |\n",
      "|    value_loss         | 0.167    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 46    |\n",
      "|    iterations      | 10500 |\n",
      "|    time_elapsed    | 1129  |\n",
      "|    total_timesteps | 52500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=5.55 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 5.55      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 53000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.128    |\n",
      "|    explained_variance | -1.87e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10599     |\n",
      "|    policy_loss        | -0.012    |\n",
      "|    value_loss         | 0.146     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 46    |\n",
      "|    iterations      | 10600 |\n",
      "|    time_elapsed    | 1138  |\n",
      "|    total_timesteps | 53000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=53500, episode_reward=6.03 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 6.03     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 53500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.439   |\n",
      "|    explained_variance | -15.1    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10699    |\n",
      "|    policy_loss        | -0.235   |\n",
      "|    value_loss         | 0.235    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 46    |\n",
      "|    iterations      | 10700 |\n",
      "|    time_elapsed    | 1146  |\n",
      "|    total_timesteps | 53500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=-2.56 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | -2.56    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 54000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.333   |\n",
      "|    explained_variance | -22.7    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10799    |\n",
      "|    policy_loss        | -0.0168  |\n",
      "|    value_loss         | 0.000549 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 46    |\n",
      "|    iterations      | 10800 |\n",
      "|    time_elapsed    | 1156  |\n",
      "|    total_timesteps | 54000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=54500, episode_reward=5.42 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 5.42     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 54500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.458   |\n",
      "|    explained_variance | -5.1     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10899    |\n",
      "|    policy_loss        | -0.0184  |\n",
      "|    value_loss         | 0.000828 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 46    |\n",
      "|    iterations      | 10900 |\n",
      "|    time_elapsed    | 1164  |\n",
      "|    total_timesteps | 54500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=5.65 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 5.65      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 55000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.13     |\n",
      "|    explained_variance | -1.33e+05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10999     |\n",
      "|    policy_loss        | -0.0113   |\n",
      "|    value_loss         | 0.149     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 46    |\n",
      "|    iterations      | 11000 |\n",
      "|    time_elapsed    | 1173  |\n",
      "|    total_timesteps | 55000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=55500, episode_reward=5.80 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 5.8       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 55500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.11     |\n",
      "|    explained_variance | -3.55e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11099     |\n",
      "|    policy_loss        | -0.0101   |\n",
      "|    value_loss         | 0.163     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 46    |\n",
      "|    iterations      | 11100 |\n",
      "|    time_elapsed    | 1182  |\n",
      "|    total_timesteps | 55500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=7.74 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 7.74     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 56000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.104   |\n",
      "|    explained_variance | -91      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11199    |\n",
      "|    policy_loss        | -0.0293  |\n",
      "|    value_loss         | 0.00127  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 47    |\n",
      "|    iterations      | 11200 |\n",
      "|    time_elapsed    | 1191  |\n",
      "|    total_timesteps | 56000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=56500, episode_reward=6.95 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 6.95     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 56500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0909  |\n",
      "|    explained_variance | -744     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11299    |\n",
      "|    policy_loss        | -0.00993 |\n",
      "|    value_loss         | 0.151    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 47    |\n",
      "|    iterations      | 11300 |\n",
      "|    time_elapsed    | 1199  |\n",
      "|    total_timesteps | 56500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=11.27 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 11.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 57000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.146   |\n",
      "|    explained_variance | -303     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11399    |\n",
      "|    policy_loss        | -0.00608 |\n",
      "|    value_loss         | 0.00192  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 47    |\n",
      "|    iterations      | 11400 |\n",
      "|    time_elapsed    | 1208  |\n",
      "|    total_timesteps | 57000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=57500, episode_reward=7.22 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 7.22      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 57500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0999   |\n",
      "|    explained_variance | -52.1     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11499     |\n",
      "|    policy_loss        | -0.000275 |\n",
      "|    value_loss         | 0.000531  |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 47    |\n",
      "|    iterations      | 11500 |\n",
      "|    time_elapsed    | 1217  |\n",
      "|    total_timesteps | 57500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=11.39 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 11.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 58000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.102    |\n",
      "|    explained_variance | -1.13e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11599     |\n",
      "|    policy_loss        | -0.0155   |\n",
      "|    value_loss         | 0.152     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 47    |\n",
      "|    iterations      | 11600 |\n",
      "|    time_elapsed    | 1226  |\n",
      "|    total_timesteps | 58000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=58500, episode_reward=11.65 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 11.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 58500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.111   |\n",
      "|    explained_variance | -168     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11699    |\n",
      "|    policy_loss        | -0.0164  |\n",
      "|    value_loss         | 0.151    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 47    |\n",
      "|    iterations      | 11700 |\n",
      "|    time_elapsed    | 1234  |\n",
      "|    total_timesteps | 58500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=9.00 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 9        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 59000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.4     |\n",
      "|    explained_variance | -8.62    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11799    |\n",
      "|    policy_loss        | -0.269   |\n",
      "|    value_loss         | 0.244    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 47    |\n",
      "|    iterations      | 11800 |\n",
      "|    time_elapsed    | 1244  |\n",
      "|    total_timesteps | 59000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=59500, episode_reward=5.19 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 5.19      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 59500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.112    |\n",
      "|    explained_variance | -3.83e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11899     |\n",
      "|    policy_loss        | -0.00931  |\n",
      "|    value_loss         | 0.155     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 47    |\n",
      "|    iterations      | 11900 |\n",
      "|    time_elapsed    | 1254  |\n",
      "|    total_timesteps | 59500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=4.86 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 4.86      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 60000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0786   |\n",
      "|    explained_variance | -1.18e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11999     |\n",
      "|    policy_loss        | -0.00621  |\n",
      "|    value_loss         | 0.153     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 47    |\n",
      "|    iterations      | 12000 |\n",
      "|    time_elapsed    | 1264  |\n",
      "|    total_timesteps | 60000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=60500, episode_reward=4.94 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 4.94      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 60500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.168    |\n",
      "|    explained_variance | -1.31e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12099     |\n",
      "|    policy_loss        | -0.0165   |\n",
      "|    value_loss         | 0.155     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 47    |\n",
      "|    iterations      | 12100 |\n",
      "|    time_elapsed    | 1273  |\n",
      "|    total_timesteps | 60500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=6.21 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 6.21      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 61000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.136    |\n",
      "|    explained_variance | -7.73e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12199     |\n",
      "|    policy_loss        | -0.0122   |\n",
      "|    value_loss         | 0.157     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 47    |\n",
      "|    iterations      | 12200 |\n",
      "|    time_elapsed    | 1281  |\n",
      "|    total_timesteps | 61000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=61500, episode_reward=6.23 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 6.23      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 61500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.313    |\n",
      "|    explained_variance | -3.75e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12299     |\n",
      "|    policy_loss        | -0.161    |\n",
      "|    value_loss         | 0.16      |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 47    |\n",
      "|    iterations      | 12300 |\n",
      "|    time_elapsed    | 1290  |\n",
      "|    total_timesteps | 61500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=5.51 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 5.51     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 62000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.274   |\n",
      "|    explained_variance | -3.28    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12399    |\n",
      "|    policy_loss        | -0.0319  |\n",
      "|    value_loss         | 0.143    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 47    |\n",
      "|    iterations      | 12400 |\n",
      "|    time_elapsed    | 1299  |\n",
      "|    total_timesteps | 62000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=62500, episode_reward=6.92 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 6.92     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 62500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.207   |\n",
      "|    explained_variance | -168     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12499    |\n",
      "|    policy_loss        | -0.0213  |\n",
      "|    value_loss         | 0.136    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 47    |\n",
      "|    iterations      | 12500 |\n",
      "|    time_elapsed    | 1308  |\n",
      "|    total_timesteps | 62500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=6.12 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 6.12     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 63000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.111   |\n",
      "|    explained_variance | -779     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12599    |\n",
      "|    policy_loss        | -0.0117  |\n",
      "|    value_loss         | 0.136    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 47    |\n",
      "|    iterations      | 12600 |\n",
      "|    time_elapsed    | 1316  |\n",
      "|    total_timesteps | 63000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=63500, episode_reward=5.15 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 5.15      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 63500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.119    |\n",
      "|    explained_variance | -2.88e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12699     |\n",
      "|    policy_loss        | -0.00969  |\n",
      "|    value_loss         | 0.146     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 47    |\n",
      "|    iterations      | 12700 |\n",
      "|    time_elapsed    | 1327  |\n",
      "|    total_timesteps | 63500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=7.01 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 7.01     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 64000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.43    |\n",
      "|    explained_variance | -0.439   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12799    |\n",
      "|    policy_loss        | -0.186   |\n",
      "|    value_loss         | 0.138    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 47    |\n",
      "|    iterations      | 12800 |\n",
      "|    time_elapsed    | 1336  |\n",
      "|    total_timesteps | 64000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=64500, episode_reward=5.08 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 5.08     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 64500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.328   |\n",
      "|    explained_variance | -6.06    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12899    |\n",
      "|    policy_loss        | -0.271   |\n",
      "|    value_loss         | 0.168    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 47    |\n",
      "|    iterations      | 12900 |\n",
      "|    time_elapsed    | 1345  |\n",
      "|    total_timesteps | 64500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=9.44 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 9.44      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 65000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.148    |\n",
      "|    explained_variance | -1.22e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12999     |\n",
      "|    policy_loss        | -0.0394   |\n",
      "|    value_loss         | 0.145     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 47    |\n",
      "|    iterations      | 13000 |\n",
      "|    time_elapsed    | 1354  |\n",
      "|    total_timesteps | 65000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=65500, episode_reward=5.17 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 5.17     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 65500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.113   |\n",
      "|    explained_variance | -369     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13099    |\n",
      "|    policy_loss        | -0.00939 |\n",
      "|    value_loss         | 0.149    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 13100 |\n",
      "|    time_elapsed    | 1363  |\n",
      "|    total_timesteps | 65500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=5.53 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 5.53     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 66000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0871  |\n",
      "|    explained_variance | -731     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13199    |\n",
      "|    policy_loss        | -0.00704 |\n",
      "|    value_loss         | 0.155    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 13200 |\n",
      "|    time_elapsed    | 1371  |\n",
      "|    total_timesteps | 66000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=66500, episode_reward=4.89 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 4.89      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 66500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0902   |\n",
      "|    explained_variance | -5.71e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13299     |\n",
      "|    policy_loss        | -0.00717  |\n",
      "|    value_loss         | 0.151     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 13300 |\n",
      "|    time_elapsed    | 1380  |\n",
      "|    total_timesteps | 66500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=9.41 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 9.41     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 67000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.146   |\n",
      "|    explained_variance | -326     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13399    |\n",
      "|    policy_loss        | -0.0134  |\n",
      "|    value_loss         | 0.155    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 13400 |\n",
      "|    time_elapsed    | 1388  |\n",
      "|    total_timesteps | 67000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=67500, episode_reward=4.23 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 4.23     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 67500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0817  |\n",
      "|    explained_variance | -382     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13499    |\n",
      "|    policy_loss        | -0.00668 |\n",
      "|    value_loss         | 0.144    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 13500 |\n",
      "|    time_elapsed    | 1397  |\n",
      "|    total_timesteps | 67500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=5.46 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 5.46     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 68000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.151   |\n",
      "|    explained_variance | -50.9    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13599    |\n",
      "|    policy_loss        | -0.00507 |\n",
      "|    value_loss         | 0.00107  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 13600 |\n",
      "|    time_elapsed    | 1406  |\n",
      "|    total_timesteps | 68000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=68500, episode_reward=7.38 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 7.38     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 68500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.416   |\n",
      "|    explained_variance | -3.63    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13699    |\n",
      "|    policy_loss        | -0.21    |\n",
      "|    value_loss         | 0.153    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 13700 |\n",
      "|    time_elapsed    | 1415  |\n",
      "|    total_timesteps | 68500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=4.77 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 4.77      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 69000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.221    |\n",
      "|    explained_variance | -2.93e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13799     |\n",
      "|    policy_loss        | -0.0263   |\n",
      "|    value_loss         | 0.156     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 13800 |\n",
      "|    time_elapsed    | 1424  |\n",
      "|    total_timesteps | 69000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=69500, episode_reward=4.30 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 4.3      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 69500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0831  |\n",
      "|    explained_variance | -72.7    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13899    |\n",
      "|    policy_loss        | -0.00822 |\n",
      "|    value_loss         | 0.154    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 13900 |\n",
      "|    time_elapsed    | 1433  |\n",
      "|    total_timesteps | 69500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=4.83 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 4.83      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 70000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0659   |\n",
      "|    explained_variance | -2.41e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13999     |\n",
      "|    policy_loss        | -0.334    |\n",
      "|    value_loss         | 0.154     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 14000 |\n",
      "|    time_elapsed    | 1442  |\n",
      "|    total_timesteps | 70000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=70500, episode_reward=6.69 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 6.69      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 70500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.138    |\n",
      "|    explained_variance | -8.13e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14099     |\n",
      "|    policy_loss        | -0.0129   |\n",
      "|    value_loss         | 0.149     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 14100 |\n",
      "|    time_elapsed    | 1452  |\n",
      "|    total_timesteps | 70500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=5.42 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 5.42      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 71000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0536   |\n",
      "|    explained_variance | -1.16e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14199     |\n",
      "|    policy_loss        | -0.00388  |\n",
      "|    value_loss         | 0.159     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 14200 |\n",
      "|    time_elapsed    | 1462  |\n",
      "|    total_timesteps | 71000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=71500, episode_reward=5.06 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 5.06      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 71500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0431   |\n",
      "|    explained_variance | -3.24e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14299     |\n",
      "|    policy_loss        | -0.00338  |\n",
      "|    value_loss         | 0.155     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 14300 |\n",
      "|    time_elapsed    | 1473  |\n",
      "|    total_timesteps | 71500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=8.14 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 8.14      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 72000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0574   |\n",
      "|    explained_variance | -180      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14399     |\n",
      "|    policy_loss        | -0.000118 |\n",
      "|    value_loss         | 0.000425  |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 14400 |\n",
      "|    time_elapsed    | 1483  |\n",
      "|    total_timesteps | 72000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=72500, episode_reward=8.55 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 8.55     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 72500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0655  |\n",
      "|    explained_variance | -114     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14499    |\n",
      "|    policy_loss        | 6.79e-05 |\n",
      "|    value_loss         | 0.000444 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 14500 |\n",
      "|    time_elapsed    | 1491  |\n",
      "|    total_timesteps | 72500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=73000, episode_reward=11.89 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 11.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 73000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.128   |\n",
      "|    explained_variance | -38      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14599    |\n",
      "|    policy_loss        | -0.00064 |\n",
      "|    value_loss         | 0.000519 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 14600 |\n",
      "|    time_elapsed    | 1500  |\n",
      "|    total_timesteps | 73000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=73500, episode_reward=7.25 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 7.25      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 73500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.133    |\n",
      "|    explained_variance | -2.73e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14699     |\n",
      "|    policy_loss        | -0.0779   |\n",
      "|    value_loss         | 0.141     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 14700 |\n",
      "|    time_elapsed    | 1509  |\n",
      "|    total_timesteps | 73500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=8.88 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 8.88     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 74000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.109   |\n",
      "|    explained_variance | -48.4    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14799    |\n",
      "|    policy_loss        | -0.00125 |\n",
      "|    value_loss         | 0.000727 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 14800 |\n",
      "|    time_elapsed    | 1518  |\n",
      "|    total_timesteps | 74000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=74500, episode_reward=11.57 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 11.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 74500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.133   |\n",
      "|    explained_variance | -54.8    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14899    |\n",
      "|    policy_loss        | -0.00165 |\n",
      "|    value_loss         | 0.000504 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 14900 |\n",
      "|    time_elapsed    | 1527  |\n",
      "|    total_timesteps | 74500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=4.23 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 4.23     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 75000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0194  |\n",
      "|    explained_variance | -700     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14999    |\n",
      "|    policy_loss        | -0.00104 |\n",
      "|    value_loss         | 0.139    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 15000 |\n",
      "|    time_elapsed    | 1535  |\n",
      "|    total_timesteps | 75000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=75500, episode_reward=3.60 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 3.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 75500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.417   |\n",
      "|    explained_variance | -161     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15099    |\n",
      "|    policy_loss        | -0.14    |\n",
      "|    value_loss         | 0.143    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 15100 |\n",
      "|    time_elapsed    | 1544  |\n",
      "|    total_timesteps | 75500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=5.24 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 5.24     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 76000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.332   |\n",
      "|    explained_variance | -638     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15199    |\n",
      "|    policy_loss        | -0.144   |\n",
      "|    value_loss         | 0.134    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 15200 |\n",
      "|    time_elapsed    | 1552  |\n",
      "|    total_timesteps | 76000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=76500, episode_reward=4.04 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 4.04     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 76500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.214   |\n",
      "|    explained_variance | -249     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15299    |\n",
      "|    policy_loss        | -0.0238  |\n",
      "|    value_loss         | 0.138    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 15300 |\n",
      "|    time_elapsed    | 1561  |\n",
      "|    total_timesteps | 76500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=77000, episode_reward=5.79 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 5.79      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 77000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0226   |\n",
      "|    explained_variance | -1.79e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15399     |\n",
      "|    policy_loss        | -0.00124  |\n",
      "|    value_loss         | 0.138     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 15400 |\n",
      "|    time_elapsed    | 1569  |\n",
      "|    total_timesteps | 77000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=77500, episode_reward=13.77 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 13.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 77500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.04    |\n",
      "|    explained_variance | -787     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15499    |\n",
      "|    policy_loss        | -0.00376 |\n",
      "|    value_loss         | 0.14     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 15500 |\n",
      "|    time_elapsed    | 1578  |\n",
      "|    total_timesteps | 77500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=6.34 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 6.34      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 78000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0176   |\n",
      "|    explained_variance | -82.2     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15599     |\n",
      "|    policy_loss        | -0.000944 |\n",
      "|    value_loss         | 0.141     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 15600 |\n",
      "|    time_elapsed    | 1587  |\n",
      "|    total_timesteps | 78000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=78500, episode_reward=5.61 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 5.61      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 78500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0138   |\n",
      "|    explained_variance | -5.7e+03  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15699     |\n",
      "|    policy_loss        | -0.000694 |\n",
      "|    value_loss         | 0.137     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 15700 |\n",
      "|    time_elapsed    | 1595  |\n",
      "|    total_timesteps | 78500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=11.18 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 11.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 79000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0143  |\n",
      "|    explained_variance | -294     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15799    |\n",
      "|    policy_loss        | -0.00102 |\n",
      "|    value_loss         | 0.145    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 15800 |\n",
      "|    time_elapsed    | 1604  |\n",
      "|    total_timesteps | 79000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=79500, episode_reward=11.00 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 11        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 79500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0989   |\n",
      "|    explained_variance | -8.45e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15899     |\n",
      "|    policy_loss        | -0.128    |\n",
      "|    value_loss         | 0.136     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 15900 |\n",
      "|    time_elapsed    | 1614  |\n",
      "|    total_timesteps | 79500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=11.28 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 11.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 80000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0299  |\n",
      "|    explained_variance | -4.5e+04 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15999    |\n",
      "|    policy_loss        | -0.00257 |\n",
      "|    value_loss         | 0.136    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 16000 |\n",
      "|    time_elapsed    | 1625  |\n",
      "|    total_timesteps | 80000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=80500, episode_reward=7.42 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 7.42      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 80500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0135   |\n",
      "|    explained_variance | -1.05e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16099     |\n",
      "|    policy_loss        | -0.000683 |\n",
      "|    value_loss         | 0.135     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 16100 |\n",
      "|    time_elapsed    | 1635  |\n",
      "|    total_timesteps | 80500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=81000, episode_reward=7.59 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 7.59     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 81000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0579  |\n",
      "|    explained_variance | -721     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16199    |\n",
      "|    policy_loss        | -0.00399 |\n",
      "|    value_loss         | 0.141    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 16200 |\n",
      "|    time_elapsed    | 1646  |\n",
      "|    total_timesteps | 81000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=81500, episode_reward=7.45 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 7.45     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 81500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0285  |\n",
      "|    explained_variance | -20.5    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16299    |\n",
      "|    policy_loss        | -0.371   |\n",
      "|    value_loss         | 0.142    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 16300 |\n",
      "|    time_elapsed    | 1656  |\n",
      "|    total_timesteps | 81500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=8.44 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 8.44      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 82000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0263   |\n",
      "|    explained_variance | -1.63e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16399     |\n",
      "|    policy_loss        | -0.00151  |\n",
      "|    value_loss         | 0.137     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 16400 |\n",
      "|    time_elapsed    | 1668  |\n",
      "|    total_timesteps | 82000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=82500, episode_reward=7.32 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 7.32      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 82500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0237   |\n",
      "|    explained_variance | -1.67e+05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16499     |\n",
      "|    policy_loss        | -0.00133  |\n",
      "|    value_loss         | 0.142     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 16500 |\n",
      "|    time_elapsed    | 1678  |\n",
      "|    total_timesteps | 82500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=83000, episode_reward=6.72 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 6.72      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 83000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0156   |\n",
      "|    explained_variance | -765      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16599     |\n",
      "|    policy_loss        | -0.000823 |\n",
      "|    value_loss         | 0.145     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 16600 |\n",
      "|    time_elapsed    | 1689  |\n",
      "|    total_timesteps | 83000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=83500, episode_reward=-1.43 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | -1.43    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 83500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.156   |\n",
      "|    explained_variance | -60.7    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16699    |\n",
      "|    policy_loss        | -0.0334  |\n",
      "|    value_loss         | 0.00177  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 16700 |\n",
      "|    time_elapsed    | 1699  |\n",
      "|    total_timesteps | 83500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=-4.07 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | -4.07     |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 84000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.142    |\n",
      "|    explained_variance | -7.08e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16799     |\n",
      "|    policy_loss        | -0.00557  |\n",
      "|    value_loss         | 0.000658  |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 16800 |\n",
      "|    time_elapsed    | 1707  |\n",
      "|    total_timesteps | 84000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=84500, episode_reward=-3.31 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | -3.31     |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 84500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.136    |\n",
      "|    explained_variance | -1.35e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16899     |\n",
      "|    policy_loss        | -0.00626  |\n",
      "|    value_loss         | 0.00119   |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 16900 |\n",
      "|    time_elapsed    | 1716  |\n",
      "|    total_timesteps | 84500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=-0.79 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | -0.786   |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 85000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0959  |\n",
      "|    explained_variance | -30.4    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16999    |\n",
      "|    policy_loss        | -0.00884 |\n",
      "|    value_loss         | 0.0726   |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 17000 |\n",
      "|    time_elapsed    | 1727  |\n",
      "|    total_timesteps | 85000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=85500, episode_reward=6.20 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 6.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 85500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.376   |\n",
      "|    explained_variance | -0.147   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17099    |\n",
      "|    policy_loss        | -0.253   |\n",
      "|    value_loss         | 0.219    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 17100 |\n",
      "|    time_elapsed    | 1737  |\n",
      "|    total_timesteps | 85500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=4.83 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 4.83     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 86000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.11    |\n",
      "|    explained_variance | -717     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17199    |\n",
      "|    policy_loss        | -0.00946 |\n",
      "|    value_loss         | 0.138    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 17200 |\n",
      "|    time_elapsed    | 1747  |\n",
      "|    total_timesteps | 86000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=86500, episode_reward=5.31 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 5.31      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 86500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0364   |\n",
      "|    explained_variance | -1.88e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17299     |\n",
      "|    policy_loss        | -0.00224  |\n",
      "|    value_loss         | 0.141     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 17300 |\n",
      "|    time_elapsed    | 1756  |\n",
      "|    total_timesteps | 86500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=87000, episode_reward=8.52 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 8.52     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 87000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0474  |\n",
      "|    explained_variance | -324     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17399    |\n",
      "|    policy_loss        | -0.00313 |\n",
      "|    value_loss         | 0.149    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 17400 |\n",
      "|    time_elapsed    | 1765  |\n",
      "|    total_timesteps | 87000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=87500, episode_reward=5.46 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 5.46     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 87500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0362  |\n",
      "|    explained_variance | -395     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17499    |\n",
      "|    policy_loss        | -0.00228 |\n",
      "|    value_loss         | 0.147    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 17500 |\n",
      "|    time_elapsed    | 1774  |\n",
      "|    total_timesteps | 87500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=7.97 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 7.97     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 88000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.033   |\n",
      "|    explained_variance | -874     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17599    |\n",
      "|    policy_loss        | -0.00201 |\n",
      "|    value_loss         | 0.145    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 17600 |\n",
      "|    time_elapsed    | 1783  |\n",
      "|    total_timesteps | 88000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=88500, episode_reward=5.20 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 5.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 88500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0874  |\n",
      "|    explained_variance | -235     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17699    |\n",
      "|    policy_loss        | -0.0067  |\n",
      "|    value_loss         | 0.132    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 17700 |\n",
      "|    time_elapsed    | 1792  |\n",
      "|    total_timesteps | 88500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=89000, episode_reward=4.44 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 4.44     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 89000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.132   |\n",
      "|    explained_variance | -2.4e+05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17799    |\n",
      "|    policy_loss        | -0.0127  |\n",
      "|    value_loss         | 0.15     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 17800 |\n",
      "|    time_elapsed    | 1801  |\n",
      "|    total_timesteps | 89000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=89500, episode_reward=5.26 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 5.26      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 89500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.014    |\n",
      "|    explained_variance | -6.87e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17899     |\n",
      "|    policy_loss        | -0.000812 |\n",
      "|    value_loss         | 0.151     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 17900 |\n",
      "|    time_elapsed    | 1810  |\n",
      "|    total_timesteps | 89500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=5.34 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 5.34      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 90000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0116   |\n",
      "|    explained_variance | -3.17e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17999     |\n",
      "|    policy_loss        | -0.000606 |\n",
      "|    value_loss         | 0.148     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 18000 |\n",
      "|    time_elapsed    | 1819  |\n",
      "|    total_timesteps | 90000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=90500, episode_reward=5.05 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 5.05      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 90500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0165   |\n",
      "|    explained_variance | -1.12e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18099     |\n",
      "|    policy_loss        | -0.000913 |\n",
      "|    value_loss         | 0.154     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 18100 |\n",
      "|    time_elapsed    | 1827  |\n",
      "|    total_timesteps | 90500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=91000, episode_reward=6.49 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 6.49      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 91000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0333   |\n",
      "|    explained_variance | -2.53e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18199     |\n",
      "|    policy_loss        | -0.00208  |\n",
      "|    value_loss         | 0.152     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 18200 |\n",
      "|    time_elapsed    | 1836  |\n",
      "|    total_timesteps | 91000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=91500, episode_reward=6.61 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 6.61      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 91500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0228   |\n",
      "|    explained_variance | -2.52e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18299     |\n",
      "|    policy_loss        | -0.00131  |\n",
      "|    value_loss         | 0.156     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 18300 |\n",
      "|    time_elapsed    | 1845  |\n",
      "|    total_timesteps | 91500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=7.06 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 7.06      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 92000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0524   |\n",
      "|    explained_variance | -2.28e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18399     |\n",
      "|    policy_loss        | -0.00362  |\n",
      "|    value_loss         | 0.156     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 18400 |\n",
      "|    time_elapsed    | 1854  |\n",
      "|    total_timesteps | 92000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=92500, episode_reward=6.97 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 6.97     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 92500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0419  |\n",
      "|    explained_variance | -473     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18499    |\n",
      "|    policy_loss        | -0.00321 |\n",
      "|    value_loss         | 0.152    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 18500 |\n",
      "|    time_elapsed    | 1862  |\n",
      "|    total_timesteps | 92500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=93000, episode_reward=6.87 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 6.87      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 93000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0662   |\n",
      "|    explained_variance | -2.89e+06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18599     |\n",
      "|    policy_loss        | -0.00504  |\n",
      "|    value_loss         | 0.157     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 18600 |\n",
      "|    time_elapsed    | 1871  |\n",
      "|    total_timesteps | 93000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=93500, episode_reward=6.57 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 6.57      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 93500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0449   |\n",
      "|    explained_variance | -6.92e+05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18699     |\n",
      "|    policy_loss        | -0.00301  |\n",
      "|    value_loss         | 0.156     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 18700 |\n",
      "|    time_elapsed    | 1880  |\n",
      "|    total_timesteps | 93500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=7.49 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 7.49      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 94000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0306   |\n",
      "|    explained_variance | -1.54e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18799     |\n",
      "|    policy_loss        | -0.00191  |\n",
      "|    value_loss         | 0.159     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 18800 |\n",
      "|    time_elapsed    | 1889  |\n",
      "|    total_timesteps | 94000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=94500, episode_reward=9.71 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 9.71     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 94500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.041   |\n",
      "|    explained_variance | -39.8    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18899    |\n",
      "|    policy_loss        | -0.0252  |\n",
      "|    value_loss         | 0.00213  |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 18900 |\n",
      "|    time_elapsed    | 1898  |\n",
      "|    total_timesteps | 94500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=7.11 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 7.11     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 95000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0629  |\n",
      "|    explained_variance | -705     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18999    |\n",
      "|    policy_loss        | -0.00469 |\n",
      "|    value_loss         | 0.157    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 19000 |\n",
      "|    time_elapsed    | 1907  |\n",
      "|    total_timesteps | 95000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=95500, episode_reward=6.90 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 6.9       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 95500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0381   |\n",
      "|    explained_variance | -5.45e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19099     |\n",
      "|    policy_loss        | -0.00269  |\n",
      "|    value_loss         | 0.159     |\n",
      "-------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 19100 |\n",
      "|    time_elapsed    | 1916  |\n",
      "|    total_timesteps | 95500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=7.26 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 7.26     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 96000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0612  |\n",
      "|    explained_variance | -1.5e+03 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19199    |\n",
      "|    policy_loss        | -0.00594 |\n",
      "|    value_loss         | 0.153    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 19200 |\n",
      "|    time_elapsed    | 1925  |\n",
      "|    total_timesteps | 96000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=96500, episode_reward=7.44 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 7.44     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 96500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0426  |\n",
      "|    explained_variance | -511     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19299    |\n",
      "|    policy_loss        | -0.00308 |\n",
      "|    value_loss         | 0.154    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 19300 |\n",
      "|    time_elapsed    | 1934  |\n",
      "|    total_timesteps | 96500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=97000, episode_reward=6.17 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 6.17     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 97000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0851  |\n",
      "|    explained_variance | -379     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19399    |\n",
      "|    policy_loss        | -0.00658 |\n",
      "|    value_loss         | 0.15     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 19400 |\n",
      "|    time_elapsed    | 1943  |\n",
      "|    total_timesteps | 97000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=97500, episode_reward=7.19 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 7.19     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 97500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0616  |\n",
      "|    explained_variance | -754     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19499    |\n",
      "|    policy_loss        | -0.00435 |\n",
      "|    value_loss         | 0.15     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 19500 |\n",
      "|    time_elapsed    | 1952  |\n",
      "|    total_timesteps | 97500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=98000, episode_reward=7.75 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 7.75     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 98000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.179   |\n",
      "|    explained_variance | -9.04    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19599    |\n",
      "|    policy_loss        | -0.208   |\n",
      "|    value_loss         | 0.157    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 49    |\n",
      "|    iterations      | 19600 |\n",
      "|    time_elapsed    | 1961  |\n",
      "|    total_timesteps | 98000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=98500, episode_reward=5.49 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 5.49     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 98500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.331   |\n",
      "|    explained_variance | -3.91    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19699    |\n",
      "|    policy_loss        | -0.0598  |\n",
      "|    value_loss         | 0.161    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 50    |\n",
      "|    iterations      | 19700 |\n",
      "|    time_elapsed    | 1969  |\n",
      "|    total_timesteps | 98500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=99000, episode_reward=6.72 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 6.72     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 99000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.146   |\n",
      "|    explained_variance | -462     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19799    |\n",
      "|    policy_loss        | -0.0145  |\n",
      "|    value_loss         | 0.156    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 50    |\n",
      "|    iterations      | 19800 |\n",
      "|    time_elapsed    | 1979  |\n",
      "|    total_timesteps | 99000 |\n",
      "------------------------------\n",
      "Eval num_timesteps=99500, episode_reward=5.52 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.76e+03 |\n",
      "|    mean_reward        | 5.52     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 99500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.148   |\n",
      "|    explained_variance | -291     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19899    |\n",
      "|    policy_loss        | -0.0152  |\n",
      "|    value_loss         | 0.157    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 50    |\n",
      "|    iterations      | 19900 |\n",
      "|    time_elapsed    | 1988  |\n",
      "|    total_timesteps | 99500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=5.78 +/- 0.00\n",
      "Episode length: 2757.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.76e+03  |\n",
      "|    mean_reward        | 5.78      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 100000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.12     |\n",
      "|    explained_variance | -1.33e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19999     |\n",
      "|    policy_loss        | -0.011    |\n",
      "|    value_loss         | 0.155     |\n",
      "-------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 50     |\n",
      "|    iterations      | 20000  |\n",
      "|    time_elapsed    | 1998   |\n",
      "|    total_timesteps | 100000 |\n",
      "-------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "<stable_baselines3.a2c.a2c.A2C at 0x7955050c9150>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "env = customEnv(df=data, window_size=10, frame_bound=(10, len(data)))\n",
    "env.trade_fee_ask_percent = 0\n",
    "env.trade_fee_bid_percent = 0\n",
    "env.reset(seed=42)\n",
    "env = Monitor(env, './logs')  # Envolver con Monitor wrapper\n",
    "env_maker = lambda: env\n",
    "env_train = DummyVecEnv([env_maker])\n",
    "#add dqn\n",
    "model = A2C('MlpPolicy', env_train, verbose=1)\n",
    "\n",
    "eval_callback = EvalCallback(env_train, best_model_save_path='./logs/',\n",
    "                             log_path='./logs/', eval_freq=500,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "model.learn(total_timesteps=100000,callback=eval_callback)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T08:07:46.743744Z",
     "start_time": "2024-04-15T07:34:28.711758Z"
    }
   },
   "id": "7ab823a0bfd15f88",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model.save('RL_del_gran_pisky_model_callback.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T08:12:12.872714Z",
     "start_time": "2024-04-15T08:12:12.856342Z"
    }
   },
   "id": "40ee6249f9dedbc8",
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39b34dbe4dbeaf7a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#load model\n",
    "from stable_baselines3 import A2C\n",
    "model = A2C.load('RL_del_gran_pisky_model_callback.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T08:12:14.585526Z",
     "start_time": "2024-04-15T08:12:14.566357Z"
    }
   },
   "id": "602239b2e2c64da3",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/plain": "                  Open        High         Low       Close   Adj Close  \\\nDate                                                                     \n2013-01-02   26.200001   26.240000   25.700001   25.920000   22.821421   \n2013-01-03   25.975000   26.299999   25.889999   26.184999   23.054739   \n2013-01-04   26.250000   26.520000   26.235001   26.440001   23.279261   \n2013-01-07   26.395000   26.540001   26.264999   26.480000   23.314474   \n2013-01-08   26.360001   26.469999   26.094999   26.200001   23.067947   \n...                ...         ...         ...         ...         ...   \n2023-12-22  108.260002  110.800003  107.449997  108.040001  107.655373   \n2023-12-26  108.300003  108.690002  107.480003  108.019997  107.635437   \n2023-12-27  108.339996  108.589996  106.849998  107.129997  106.748604   \n2023-12-28  107.209999  109.400002  106.809998  108.820000  108.432594   \n2023-12-29  108.959999  109.959999  108.089996  108.570000  108.183487   \n\n              Volume        rsi      macd  macd_signal      bb_bbu  \\\nDate                                                                 \n2013-01-02   7512800  62.141607  0.438271     0.378947   26.297056   \n2013-01-03   7232200  64.435317  0.457652     0.394688   26.427891   \n2013-01-04   6794000  66.536399  0.485532     0.412857   26.582832   \n2013-01-07   6489400  66.867012  0.504651     0.431216   26.736540   \n2013-01-08   6714200  62.231503  0.494213     0.443815   26.826219   \n...              ...        ...       ...          ...         ...   \n2023-12-22  46642900  38.127686  2.977695     3.754972  126.709398   \n2023-12-26  12846700  38.099763  1.985707     3.401119  126.640695   \n2023-12-27  10157900  36.808365  1.115134     2.943922  126.772160   \n2023-12-28   9352900  40.904503  0.554689     2.466075  126.851145   \n2023-12-29   7660900  40.486427  0.089401     1.990740  126.942737   \n\n                bb_bbl          obv  \nDate                                 \n2013-01-02   23.770944   -8987000.0  \n2013-01-03   23.816859   -1754800.0  \n2013-01-04   23.863918    5039200.0  \n2013-01-07   23.897710   11528600.0  \n2013-01-08   23.963281    4814400.0  \n...                ...          ...  \n2023-12-22  105.889603  480664100.0  \n2023-12-26  105.964306  467817400.0  \n2023-12-27  105.670840  457659500.0  \n2023-12-28  105.436855  467012400.0  \n2023-12-29  105.175264  459351500.0  \n\n[2768 rows x 12 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Adj Close</th>\n      <th>Volume</th>\n      <th>rsi</th>\n      <th>macd</th>\n      <th>macd_signal</th>\n      <th>bb_bbu</th>\n      <th>bb_bbl</th>\n      <th>obv</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2013-01-02</th>\n      <td>26.200001</td>\n      <td>26.240000</td>\n      <td>25.700001</td>\n      <td>25.920000</td>\n      <td>22.821421</td>\n      <td>7512800</td>\n      <td>62.141607</td>\n      <td>0.438271</td>\n      <td>0.378947</td>\n      <td>26.297056</td>\n      <td>23.770944</td>\n      <td>-8987000.0</td>\n    </tr>\n    <tr>\n      <th>2013-01-03</th>\n      <td>25.975000</td>\n      <td>26.299999</td>\n      <td>25.889999</td>\n      <td>26.184999</td>\n      <td>23.054739</td>\n      <td>7232200</td>\n      <td>64.435317</td>\n      <td>0.457652</td>\n      <td>0.394688</td>\n      <td>26.427891</td>\n      <td>23.816859</td>\n      <td>-1754800.0</td>\n    </tr>\n    <tr>\n      <th>2013-01-04</th>\n      <td>26.250000</td>\n      <td>26.520000</td>\n      <td>26.235001</td>\n      <td>26.440001</td>\n      <td>23.279261</td>\n      <td>6794000</td>\n      <td>66.536399</td>\n      <td>0.485532</td>\n      <td>0.412857</td>\n      <td>26.582832</td>\n      <td>23.863918</td>\n      <td>5039200.0</td>\n    </tr>\n    <tr>\n      <th>2013-01-07</th>\n      <td>26.395000</td>\n      <td>26.540001</td>\n      <td>26.264999</td>\n      <td>26.480000</td>\n      <td>23.314474</td>\n      <td>6489400</td>\n      <td>66.867012</td>\n      <td>0.504651</td>\n      <td>0.431216</td>\n      <td>26.736540</td>\n      <td>23.897710</td>\n      <td>11528600.0</td>\n    </tr>\n    <tr>\n      <th>2013-01-08</th>\n      <td>26.360001</td>\n      <td>26.469999</td>\n      <td>26.094999</td>\n      <td>26.200001</td>\n      <td>23.067947</td>\n      <td>6714200</td>\n      <td>62.231503</td>\n      <td>0.494213</td>\n      <td>0.443815</td>\n      <td>26.826219</td>\n      <td>23.963281</td>\n      <td>4814400.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2023-12-22</th>\n      <td>108.260002</td>\n      <td>110.800003</td>\n      <td>107.449997</td>\n      <td>108.040001</td>\n      <td>107.655373</td>\n      <td>46642900</td>\n      <td>38.127686</td>\n      <td>2.977695</td>\n      <td>3.754972</td>\n      <td>126.709398</td>\n      <td>105.889603</td>\n      <td>480664100.0</td>\n    </tr>\n    <tr>\n      <th>2023-12-26</th>\n      <td>108.300003</td>\n      <td>108.690002</td>\n      <td>107.480003</td>\n      <td>108.019997</td>\n      <td>107.635437</td>\n      <td>12846700</td>\n      <td>38.099763</td>\n      <td>1.985707</td>\n      <td>3.401119</td>\n      <td>126.640695</td>\n      <td>105.964306</td>\n      <td>467817400.0</td>\n    </tr>\n    <tr>\n      <th>2023-12-27</th>\n      <td>108.339996</td>\n      <td>108.589996</td>\n      <td>106.849998</td>\n      <td>107.129997</td>\n      <td>106.748604</td>\n      <td>10157900</td>\n      <td>36.808365</td>\n      <td>1.115134</td>\n      <td>2.943922</td>\n      <td>126.772160</td>\n      <td>105.670840</td>\n      <td>457659500.0</td>\n    </tr>\n    <tr>\n      <th>2023-12-28</th>\n      <td>107.209999</td>\n      <td>109.400002</td>\n      <td>106.809998</td>\n      <td>108.820000</td>\n      <td>108.432594</td>\n      <td>9352900</td>\n      <td>40.904503</td>\n      <td>0.554689</td>\n      <td>2.466075</td>\n      <td>126.851145</td>\n      <td>105.436855</td>\n      <td>467012400.0</td>\n    </tr>\n    <tr>\n      <th>2023-12-29</th>\n      <td>108.959999</td>\n      <td>109.959999</td>\n      <td>108.089996</td>\n      <td>108.570000</td>\n      <td>108.183487</td>\n      <td>7660900</td>\n      <td>40.486427</td>\n      <td>0.089401</td>\n      <td>1.990740</td>\n      <td>126.942737</td>\n      <td>105.175264</td>\n      <td>459351500.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2768 rows Ã— 12 columns</p>\n</div>"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = yf.download('NKE', start='2012-01-01', end= '2024-01-01',interval=\"1d\")\n",
    "prices = np.array(data['Adj Close'].values) #Adj Close prices\n",
    "dates = np.array(data.index.values).astype('datetime64[ns]') #Dates\n",
    "data['rsi'] = TA.RSI(data,14,column='adj close') #Relative Strength Index (RSI)\n",
    "data['macd'] = TA.MACD(data,column='adj close')['MACD'] #MACD Line\n",
    "data['macd_signal'] = TA.MACD(data, column='adj close')['SIGNAL'] #MACD Signal Line\n",
    "# data['bb_bbm'] = TA.BBANDS(data, column='adj close')['BB_MIDDLE'] #Bollinger Bands (BB) middle band (BBM)\n",
    "data['bb_bbu'] = TA.BBANDS(data, column='adj close')['BB_UPPER'] #Bollinger Bands (BB) upper band (BBU)\n",
    "data['bb_bbl'] = TA.BBANDS(data,column='adj close')['BB_LOWER'] #Bollinger Bands (BB) lower band (BBL)\n",
    "# data['bb_width'] = TA.BBWIDTH(data,column='adj close') #Bollinger Bands (BB) width\n",
    "data['obv'] = TA.OBV(data,'adj close') #On Balance Volume (OBV)\n",
    "\n",
    "data.fillna(0, inplace=True)\n",
    "data = data[data.index > '2013-01-01']\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T08:17:10.436613Z",
     "start_time": "2024-04-15T08:17:10.320174Z"
    }
   },
   "id": "2b31ada5b219068b",
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info {'total_reward': 28.1407413482666, 'total_profit': 1.7757067315852784, 'position': <Positions.Long: 1>}\n"
     ]
    }
   ],
   "source": [
    "from gym_anytrading.envs import Actions\n",
    "\n",
    "env_test = customEnv(df=data, window_size=10, frame_bound=(10,len(data)))\n",
    "\n",
    "obs,_ = env_test.reset(seed=42)\n",
    "env_test.trade_fee_ask_percent = 0\n",
    "env_test.trade_fee_bid_percent = 0\n",
    "\n",
    "r= []\n",
    "action_stats = {Actions.Sell: 0, Actions.Buy: 0}\n",
    "buy_rsi = []\n",
    "sell_rsi = []\n",
    "lt=0\n",
    "while True:\n",
    "    actual_lt= env_test._last_trade_tick\n",
    "    obs = obs[np.newaxis, ...] \n",
    "    action, _states = model.predict(obs)\n",
    "    # print(env_test._last_trade_tick)\n",
    "    # break\n",
    "    # print('skipped',action)\n",
    "    if lt!=actual_lt:\n",
    "        # print(action,\"Last trade tick:\",actual_lt)\n",
    "        lt=actual_lt\n",
    "        \n",
    "        # print(action)\n",
    "    action_stats[Actions(action)] += 1\n",
    "    if action == 0:\n",
    "        # print(\"Sell\")\n",
    "        # sell_rsi.append()\n",
    "        # sell_rsi.append(env_test.signal_features[env_test._last_trade_tick][0])\n",
    "        pass\n",
    "\n",
    "        pass\n",
    "    elif action == 1:\n",
    "        # print(\"Buy\")\n",
    "        # buy_rsi.append(env_test.signal_features[env_test._last_trade_tick][0])\n",
    "        pass\n",
    "    else:\n",
    "        print('??')        \n",
    "    obs, rewards,_ , done, info= env_test.step(action)\n",
    "\n",
    "    \n",
    "    r.append(rewards)\n",
    "    if done:\n",
    "        print(\"info\",info)\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T08:17:19.846732Z",
     "start_time": "2024-04-15T08:17:17.122216Z"
    }
   },
   "id": "cba5553d42041bf1",
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "25746183.217944276"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_test.max_possible_profit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T08:17:20.045274Z",
     "start_time": "2024-04-15T08:17:20.038417Z"
    }
   },
   "id": "2b9db17990b38f35",
   "execution_count": 70
  },
  {
   "cell_type": "markdown",
   "source": [
    "OPTUNA"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dda4864de6c1e737"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([8.96185608e+01, 7.13673794e+01, 2.88762200e+06, 2.63181893e+00,\n       2.56088640e+00, 9.23030974e+01, 8.34402139e+01, 1.49329941e+08])"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_test.signal_features[env_test._current_tick] #obtener los valores del dia que se va a predecir"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T08:13:26.017119Z",
     "start_time": "2024-04-15T08:13:26.013147Z"
    }
   },
   "id": "5cdb4f322a0b1c2d",
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env information:\n",
      "> shape: (10, 8)\n",
      "> df.shape: (2768, 12)\n",
      "> prices.shape: (2768,)\n",
      "> signal_features.shape: (2768, 8)\n",
      "> max_possible_profit: 25746183.217944276\n"
     ]
    }
   ],
   "source": [
    "print(\"env information:\")\n",
    "print(\"> shape:\", env_test.unwrapped.shape)\n",
    "print(\"> df.shape:\", env_test.unwrapped.df.shape)\n",
    "print(\"> prices.shape:\", env_test.unwrapped.prices.shape)\n",
    "print(\"> signal_features.shape:\", env_test.unwrapped.signal_features.shape)\n",
    "print(\"> max_possible_profit:\", env_test.unwrapped.max_possible_profit())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T08:19:44.650759Z",
     "start_time": "2024-04-15T08:19:44.619429Z"
    }
   },
   "id": "bf2f10358d794004",
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1500x600 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABMIAAAI1CAYAAAA0MFY7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAD5jElEQVR4nOzdeXxU9b3/8dfJgIiEhBKChEwgbtW4VK21KnU0CLfiGh0CFbDFVuW2Vk2UYmutotalt7gkLrTF1qWtUCCMorU/WhdSpqLWpbZWgyuBJAaEoAlh5+T8/jgzw8xkljPJhGzv5+ORO8z3fM8530ky6Z2Pn8/na1iWZSEiIiIiIiIiItLHZXT3AkRERERERERERPYHBcJERERERERERKRfUCBMRERERERERET6BQXCRERERERERESkX1AgTERERERERERE+gUFwkREREREREREpF9QIExERERERERERPoFBcJERERERERERKRfUCBMRERERERERET6BQXCRESkz6mursYwDKqrq7t7KT2CYRjceuut3b0MEWprazEMg8cff7y7lxJh3rx5HHroobhcLk444QQACgsLueyyy7p1XSIiIpJ+CoSJiEhaGIbh6MtJcOquu+7i6aef7vI1P/744xFrGzBgAPn5+Vx22WU0NDR0+f17g9dff52rr76aY445hiFDhjBmzBimTp3KBx98EHP+kiVLOPXUUxk2bBg5OTmceeaZPPfcc47utXjxYi699FKOOOIIDMOguLjY0Xl33nknhmFw7LHHxjy+evVqTj/9dA466CBGjRrFtddeS2tra8Scyy67LOHvbbzfhy+++IKRI0diGAZVVVURx1pbW5k7dy6TJk1i+PDh3RYAKiwsdPTedLK2+fPn75fXEAxmB78GDhzIoYceyne+8x0++eSTtN7rb3/7GzfccAPf+MY3eOyxx7jrrrtiznvvvfe49dZbqa2t7dT9/vnPf3LVVVdx0kknMXDgQAzDcHxuMJAY7+vKK68MzU3ld7q4uDjmnEmTJkXcP9X3SVe+90RERDpqQHcvQERE+oY//OEPEc9///vf8/zzz7cbLyoqSnqtu+66i9LSUi666KJ0LjGu22+/nUMOOYSdO3fy6quv8vjjj/OPf/yD//73vxx44IH7ZQ091f/93//x8ssvM2XKFL7yla+wYcMGHnroIb761a/y6quvRgSfHnzwQa699lrOO+88fvGLX7Bz504ef/xxzj//fJYtW4bX6014r1/96le8+eabnHzyyTQ1NTlaX319PXfddRdDhgyJefztt99mwoQJFBUVcd9991FfX88999zDhx9+yP/7f/8vNO9///d/mThxYsS5lmXx/e9/n8LCQvLz82Ne/5ZbbmH79u0xj23evJnbb7+dMWPGcPzxx3dbhmJFRUVE8OEvf/kLixYt4v7772fEiBGh8XHjxiW91vz58xkxYsR+y5S69tprOfnkk9mzZw9vvfUWCxYs4LnnnuOdd95h9OjRabnHSy+9REZGBr/73e844IADQuPvv/8+GRn7/pvxe++9x2233UZxcTGFhYUdvt9f/vIXfvvb3/KVr3yFQw89NG5QOZbc3Nx2f1MBVqxYwZNPPsk3v/nN0Fiqv9Nut5u77747Yiz6e5zKNbv6vSciItJhloiISBf44Q9/aHX0f2aGDBlizZw5s8P3XrlypQVYK1euTDjvscceswDr9ddfjxj/8Y9/bAHW4sWLO7yG/am1tTXhccCaO3duh6798ssvW7t27YoY++CDD6xBgwZZM2bMiBg/4ogjrJNPPtlqa2sLjTU3N1uZmZnWhRdemPRe69evt0zTtCzLso455hjrzDPPTHrOt771Leuss86yzjzzTOuYY45pd/ycc86x8vLyrObm5tDYI488YgHWX//614TX9vv9FmDdeeedMY+/88471oABA6zbb7/dAqylS5dGHN+5c6fV2NhoWZZlvf766xZgPfbYY0lfU1ebN2+eBVhr165N+VynP5d41q5d6+j7EHwPR39PH3jgAQuw7rrrrrjnJns/RPvud79rDRkyJOm8pUuXOvq7ksyGDRus7du3W5bVub+T4SZMmGBlZWVZO3bsSDgv3u90vPePE/Gu2ZXvPRERkc5QaaSIiOw327ZtY/bs2RQUFDBo0CCOPPJI7rnnHizLCs0xDINt27bxxBNPhEpjgtkn69at46qrruLII49k8ODB5OTkMGXKlE6XKkXzeDwAfPzxxxHja9asobS0lOHDh3PggQfyta99jWeeeSZ0/IsvvsDlcvHAAw+ExjZv3kxGRgY5OTkRr/MHP/gBo0aNCj33+/1MmTKFMWPGMGjQIAoKCrjuuuvYsWNHxBouu+wyMjMz+fjjjzn33HMZOnQoM2bMAGDXrl1cd9115ObmMnToUC688ELq6+tjvsY1a9awfv36pN+LcePGRWTJABxxxBEcc8wx1NTURIy3tLSEygSDsrKyyMzMZPDgwUnvVVBQEJGBk8yqVauoqqqioqIi5vGWlhaef/55Lr30UrKyskLj3/nOd8jMzGTJkiUJr79w4UIMw2D69Okxj5eVlXHxxReHfl+iDRo0KOJn3JPt3buXn//85xx22GEMGjSIwsJCfvrTn7Jr167QnMLCQt59913+/ve/h96bwfLVLVu28KMf/YjjjjuOzMxMsrKyOOecc/j3v/+d1nWeddZZAKxduxaAW2+9FcMweO+995g+fTpf+tKXOP300x2/JsMweOyxx9i2bVu7EtHwHmGPP/44U6ZMAWD8+PHtSr2bm5tZs2YNzc3NSV/DwQcf7Oj94FRjYyMrV67E6/UmzWBN9ju9d+/edqWLycS6Zle/90RERDpDpZEiIrJfWJbFhRdeyMqVK7n88ss54YQT+Otf/8qcOXNoaGjg/vvvB+wSyyuuuIKvf/3rzJo1C4DDDjsMsPtVrV69mksuuQS3201tbS2/+tWvKC4u5r333uOggw5Ky1qDgbUvfelLobF3332Xb3zjG+Tn5/OTn/yEIUOGsGTJEi666CKWLVvGxRdfzLBhwzj22GNZtWoV1157LQD/+Mc/MAyDLVu28N5773HMMccAduArPICydOlStm/fzg9+8ANycnL45z//yYMPPkh9fT1Lly6NWN/evXs5++yzOf3007nnnntCr/uKK67gj3/8I9OnT2fcuHG89NJLnHfeeTFfY1FREWeeeWaHyvUsy2Ljxo2h1xJUXFxMVVUVDz74IBdccAE7d+7kwQcfpLm5mbKyspTvk4hpmlxzzTVcccUVHHfccTHnvPPOO+zdu5evfe1rEeMHHHAAJ5xwAv/617/iXn/Pnj0sWbKEcePGxSyDW7p0KatXr6ampibtgdjucMUVV/DEE09QWlrK7Nmzee2117j77rupqanhqaeeAuwSy2uuuYbMzExuuukmwA7qAHzyySc8/fTTTJkyhUMOOYSNGzfym9/8hjPPPJP33nsvbWWMweB0Tk5OxPiUKVM44ogjuOuuu0IBZyev6Q9/+AMLFizgn//8J7/97W+B2CWiZ5xxBtdeey0PPPAAP/3pT0Ml3sHHp556iu9+97s89thj+73B/p/+9Cfa2tpCAfF4kv1Of/DBBwwZMoTdu3dz8MEHc+WVV3LLLbcwcODAlK/Zle89ERGRTuvWfDQREemzokt+nn76aQuw7rjjjoh5paWllmEY1kcffRQai1caGSwnCvfKK69YgPX73/8+NJZqaeQLL7xgbdq0yaqrq7Oqqqqs3Nxca9CgQVZdXV1o7oQJE6zjjjvO2rlzZ2isra3NGjdunHXEEUdEvO6DDz449Pz666+3zjjjDGvkyJHWr371K8uyLKupqckyDMOqrKxM+NruvvtuyzAMa926daGxmTNnWoD1k5/8JGLu22+/bQHWVVddFTE+ffr0mKWRQIdL3P7whz9YgPW73/0uYnzjxo3WhAkTLCD0NWLECGv16tUp3yNZCd5DDz1kZWdnW5999pllWbFLu4KlbKtWrWp3/pQpU6xRo0bFvf6zzz5rAdb8+fPbHdu+fbs1ZswY68Ybb7QsK34ZX7jOlka2tLSEykajbdu2zdqzZ4/ja0WXRgZ/d6644oqIeT/60Y8swHrppZdCY/F+Ljt37my3vrVr11qDBg2ybr/99ogxJ9+H4Pf00UcftTZt2mR9+umn1nPPPWcVFhZahmGEypnnzp1rAda0adMizk/lNc2cOTNmaeTYsWMj/g4lKo0M/i1J9eebjtLIk046ycrLy4v7+xGU6Hf6e9/7nnXrrbday5Yts37/+99bF154oQVYU6dO7dA1u+q9JyIikg4qjRQRkf3iL3/5Cy6XK5QpFTR79mwsy4ponhxPeDnRnj17aGpq4vDDD2fYsGG89dZbHV7bxIkTyc3NpaCggNLSUoYMGcIzzzyD2+0G7LKvl156ialTp7J161Y2b97M5s2baWpq4uyzz+bDDz8M7Wzm8XjYuHEj77//PmBnfp1xxhl4PB78fj9gZ4lZlhWRERb+2rZt28bmzZsZN24clmXFzJ74wQ9+EPH8L3/5C0C77295eXnM12xZVoeywdasWcMPf/hDTjvtNGbOnBlx7KCDDuLII49k5syZLF26lEcffZS8vDy8Xi8fffRRyveKp6mpiVtuuYWbb76Z3NzcuPOCZaWDBg1qd+zAAw9sV3YabuHChQwcOJCpU6e2O/aLX/yCPXv28NOf/rQDq3du165d/OIXv2DMmDFkZWUxePBg/ud//oeHHnqId955h08++YRHH32U4447LuVytnDB353rr78+Ynz27NkAjnb9HDRoUKis1TRNmpqayMzM5Mgjj+zUe/N73/seubm5jB49mvPOOy9UNh2dafT9738/7a8pFZdddhmWZe33bLAPPviAN998k0suuSRpWXGi3+nf/e53zJ07F6/Xy7e//W2WL1/OlVdeyZIlS3j11VdTvmZXvfdERETSQaWRIiKyX6xbt47Ro0czdOjQiPFgadG6deuSXmPHjh3cfffdPPbYYzQ0NET03HLSmyeehx9+mC9/+cs0Nzfz6KOPsmrVqogPcB999BGWZXHzzTdz8803x7zGZ599Rn5+fii45ff7cbvd/Otf/+KOO+4gNzeXe+65J3QsKyuL448/PnT++vXrueWWW3jmmWf4/PPPI64d/doGDBgQCtIFrVu3joyMjFAZadCRRx6Z4ncjvg0bNnDeeeeRnZ1NVVUVLpcr4viUKVMYMGAAzz77bGispKSEI444gptuuonFixenZR0/+9nPGD58ONdcc03CecHgYnhPqKCdO3fG7dPU2trK8uXLOfvss9uV4NXW1jJv3jwefvhhMjMzO/gKnPnTn/7EggULKCsr46ijjqK2tpbnnnuOH/3oR6HXNGzYMObMmRPRhylVwd+dww8/PGJ81KhRDBs2zNF7s62tjcrKSubPn8/atWsxTTN0LPp7mIpbbrkFj8eDy+VixIgRFBUVMWBA+//39ZBDDol4no7X1Bs8+eSTAEnLIhP9Tscze/ZsHnnkEV544QVOPfXUlK7ZFe89ERGRdFEgTEREeo1rrrmGxx57jPLyck477TSys7MxDINLLrmEtra2Dl/361//eijD5KKLLuL0009n+vTpvP/++2RmZoau/aMf/Yizzz475jWCH7hHjx7NIYccwqpVqygsLMSyLE477TRyc3MpKytj3bp1+P1+xo0bF5FB8z//8z9s2bKFH//4xxx11FEMGTKEhoYGLrvssnavLTz7Zn9pbm7mnHPO4YsvvsDv97fr+fTJJ5+wYsUKFixYEDE+fPhwTj/9dF5++eW0rOPDDz9kwYIFVFRU8Omnn4bGd+7cyZ49e6itrSUrK4vhw4eTl5cH2M3EozU2NsbtW/X000+zffv2mMGFW265hfz8fIqLi0O9wTZs2ADApk2bqK2tZcyYMWn5+Zxxxhm89957EQ3Qf/jDH7Jt2zbeffddAI4//viYWTcdEb7JQaruuusubr75Zr73ve/x85//nOHDh5ORkUF5eXmn3pvHHXccEydOTDovXmClM6+pN1i4cCFHHnkkJ510UsJ5iX6n4ykoKADsjNhUr9kV7z0REZF0USBMRET2i7Fjx/LCCy+wdevWiKywNWvWhI4HxfvwWlVVxcyZM7n33ntDYzt37uSLL75I2zpdLhd3330348eP56GHHuInP/kJhx56KAADBw509KHc4/GwatUqDjnkEE444QSGDh3K8ccfT3Z2NitWrOCtt97itttuC81/5513+OCDD3jiiSf4zne+Exp//vnnHa977NixtLW18fHHH0dkgQVLNDtj586dXHDBBXzwwQe88MILHH300e3mbNy4ESAiEyhoz5497N27t9PrAGhoaKCtrY1rr722XRko2JlBZWVlVFRUcOyxxzJgwADeeOONiDKr3bt38/bbb8ctvXryySfJzMzkwgsvbHds/fr1fPTRR6HfiXBXXXUVAJ9//jnDhg3r4CuMfC2xDBkyhK9//eudvn5Q8Hfnww8/DGVogv0z/eKLLxy/N8ePH8/vfve7iPEvvviCESNGpG2tTqXympzqaUG11157jY8++ojbb7896dxEv9PxfPLJJwBxy48TXbMr3nsiIiLpoh5hIiKyX5x77rmYpslDDz0UMX7//fdjGAbnnHNOaGzIkCExg1sulyuiHBLgwQcfjBl86Yzi4mK+/vWvU1FRwc6dOxk5ciTFxcX85je/iZnhsGnTpojnHo+H2tpaFi9eHCqVzMjIYNy4cdx3333s2bMnoj9YsMQw/LVZlkVlZaXjNQe/fw888EDEeEVFRcz5a9asYf369Umva5om3/rWt3jllVdYunQpp512Wsx5hx9+OBkZGSxevDjiddTX1+P3+znxxBNDY3v27GHNmjUxv5fJHHvssTz11FPtvo455hjGjBnDU089xeWXXw5AdnY2EydO5I9//CNbt24NXeMPf/gDra2tTJkypd31N23axAsvvMDFF18ccxfSO+64o929f/7znwNwww038NRTTzFkyJCUX1d3Ovfcc4H2vyv33XcfQMTOo6m8N5cuXRrqnbe/pfKanAr+XGO9/ubmZtasWdOpEu1YEr1PFy5cCMD06dMTXiPZ73RLS0u7EkbLsrjjjjsAYmbBJrtmV7z3RERE0kUZYSIisl9ccMEFjB8/nptuuona2lqOP/54/va3v7F8+XLKy8sjeluddNJJvPDCC9x3332hUsNTTjmF888/nz/84Q9kZ2dz9NFH88orr/DCCy90SS+ZOXPmMGXKFB5//HG+//3v8/DDD3P66adz3HHHceWVV3LooYeyceNGXnnlFerr6/n3v/8dOjcY5Hr//fe56667QuNnnHEG/+///T8GDRrEySefHBo/6qijOOyww/jRj35EQ0MDWVlZLFu2rF2vsEROOOEEpk2bxvz582lubmbcuHG8+OKLcZvUFxUVceaZZyZtmD979myeeeYZLrjgArZs2cIf//jHiOOXXnopYGeNfO973+O3v/0tEyZMwOv1snXrVubPn8+OHTu48cYbQ+c0NDRQVFTEzJkzefzxx0Pjq1atYtWqVYD9oXjbtm2hD+NnnHEGZ5xxBiNGjOCiiy5qt85gwCP62J133sm4ceM488wzmTVrFvX19dx7771885vfZNKkSe2us3jxYvbu3Ru3NOv0009vNxbM/jr55JPb3f+hhx7iiy++CJVxPvvss9TX1wN2qW92dnbM++xPxx9/PDNnzmTBggV88cUXnHnmmfzzn//kiSee4KKLLmL8+PGhuSeddBK/+tWvuOOOOzj88MMZOXIkZ511Fueffz6333473/3udxk3bhzvvPMOTz75ZMzMuZ72mpw64YQTcLlc/N///R/Nzc0MGjSIs846i5EjR/LUU0/x3e9+l8ceeyxpw/x169bxhz/8AYA33ngDIPR7PnbsWL797W+H5sZ7n5qmyeLFizn11FPb9QWMlux3+q233mLatGlMmzaNww8/nB07dvDUU0/x8ssvM2vWLL761a+mfE1I/3tPREQkbbpjq0oREen7fvjDH1rR/zOzdetW67rrrrNGjx5tDRw40DriiCOsefPmWW1tbRHz1qxZY51xxhnW4MGDLcCaOXOmZVmW9fnnn1vf/e53rREjRliZmZnW2Wefba1Zs8YaO3ZsaI5lWdbKlSstwFq5cmXCNT722GMWYL3++uvtjpmmaR122GHWYYcdZu3du9eyLMv6+OOPre985zvWqFGjrIEDB1r5+fnW+eefb1VVVbU7f+TIkRZgbdy4MTT2j3/8wwIsj8fTbv57771nTZw40crMzLRGjBhhXXnllda///1vC7Aee+yx0LyZM2daQ4YMifl6duzYYV177bVWTk6ONWTIEOuCCy6w6urqLMCaO3duxFzAOvPMMxN+fyzLss4880wLiPsVbs+ePdaDDz5onXDCCVZmZqaVmZlpjR8/3nrppZci5q1duzbi5xo0d+7cuPeJXn+sdR5zzDExj/n9fmvcuHHWgQceaOXm5lo//OEPrZaWlphzTz31VGvkyJGhn7kTwd+3pUuXtjs2duzYuK9p7dq1ju+RTvPmzWt3/z179li33Xabdcghh1gDBw60CgoKrBtvvNHauXNnxLkbNmywzjvvPGvo0KERv0M7d+60Zs+ebeXl5VmDBw+2vvGNb1ivvPKKdeaZZ0b8ngV/9uG/07Ek+p6GC/7ObNq0qd0xp68p3nsq+u+KZVnWI488Yh166KGWy+WK+BsT/FuS7HWFv7ZYX9HvyXjv0xUrVliA9cADDyS9X7Lf6U8++cSaMmWKVVhYaB144IHWQQcdZJ100knWr3/963Z/m51eM6ir33siIiIdYVhWVB67iIiIiIiIiIhIH6QeYSIiIiIiIiIi0i8oECYiIiIiIiIiIv2CAmEiIiIiIiIiItIvKBAmIiIiIiIiIiL9ggJhIiIiIiIiIiLSLygQJiIiIiIiIiIi/YICYSIiIiIiIiIi0i8oECYiIiIiIiIiIv2CAmEiIiIiIiIiItIvKBAmIiIiIiIiIiL9ggJhIiIiIiIiIiLSLygQJiIiIiIiIiIi/YICYSIiIiIiIiIi0i8oECYiIiIiIiIiIv2CAmEiIiIiIiIiItIvKBAmIiIiIiIiIiL9ggJhIiIiIiIiIiLSLygQJiIiIiIiIiIi/YICYSIiIiIiIiIi0i8oECYiIiIiIiIiIv2CAmEiIiIiIiIiItIvKBAmIiIiIiIiIiL9ggJhIiIiIiIiIiLSLygQJiIiIiIiIiIi/YICYSIiIiIiIiIi0i8oECYiIiIiIiIiIv2CAmEiIiIiIiIiItIvKBAmIiIiIiIiIiL9ggJhIiIiIiIiIiLSLygQJiIiIiIiIiIi/YICYSIiIiIiIiIi0i8oECYiIiIiIiIiIv2CAmEiIiIiIiIiItIvKBAmIiIiIiIiIiL9ggJhIiIiIiIiIiLSLygQJiIiIiIiIiIi/YICYSIiIiIiIiIi0i8oECYiIiIiIiIiIv2CAmEiIiIiIiIiItIvKBAmIiIiIiIiIiL9ggJhIiIiIiIiIiLSLygQJiIiIiIiIiIi/YICYSIiIiIiIiIi0i8M6O4FdERbWxuffvopQ4cOxTCM7l6OiIiIiIiIiIh0I8uy2Lp1K6NHjyYjI37eV68MhH366acUFBR09zJERERERERERKQHqaurw+12xz3eKwNhQ4cOBewXl5WV1c2rERERERERERGR7tTS0kJBQUEoZhRPrwyEBcshs7KyFAgTERERERERERGApC201CxfRERERERERET6BQXCRERERERERESkX1AgTERERERERERE+gUFwkREREREREREpF9QIExERERERERERPoFBcJERERERERERKRfUCBMRERERERERET6BQXCRERERERERESkX1AgTERERERERERE+gUFwkREREREREREpF9QIExERERERERERPoFBcJERERERERERKRfUCBMRERERERERET6hQHdvQARERERERGRjjDbTPzr/TRubSRvaB6eMR5cGa7uXpaI9GAKhImIiIiIiEiv46vxUbaijPqW+tCYO8tN5aRKvEXeblyZiPRkKo0UERERERGRXsVX46N0SWlEEAygoaWB0iWl+Gp83bQyEenpFAgTERERERGRXsNsMylbUYaF1e5YcKx8RTlmm7m/lyYivYACYSIiIiIiItJr+Nf722WChbOwqGupw7/evx9XJSK9hQJhIiIiIiIi0ms0bm1M6zwR6V8UCBMREREREZFeI29oXlrniUj/okCYiIiIiIiI9BqeMR7cWW6MOMcNoGBADp61JpjqEyYikRQIExERERERkV7DleGictg0sMCI6pdvWIAFFU824TprIhQWgk87SIrIPgqEiYiIiIiISO9hmnhvWUTVEshviTzkboGqJeCtCQzU10NpqYJhIhJiWJbVfs/ZHq6lpYXs7Gyam5vJysrq7uWIiIiIiIjI/lJdDePHA2Aa4B8LjZmQ1wqedeCK9Qm3oADWrgWXa78uVUT2H6exogH7cU0iIiIiIiLSX5gm+P3Q2Ah5eeDxpCcQ1bhvN0iXBcW1Ds6pq7PXUlzc+fuLSK+mQJiIiIiIiIikl88HZWV2aWKQ2w2VleD1du7aeR3cDbKhoXP3FZE+QT3CREREREREJH18PrsvV3gQDOxAVDr6dXk8kJ+f+nmbNnXuviLSJygQJiIiIiIiIulhmnYmWKAVtWlAdSEsOhaqx1qYhgXl5fa8jnK5YNasyGsX2vdKKDe34/cUkT5DpZEiIiIiIiKSHn5/KBPMVwRlk6A+e99hdzNUrqjDW10NEyZ0+Da+UZ9TVh7r2mE7RkbrSBaZiPQ5yggTERERERGR9Ag0svcVQelUqI/auK0hyx733XhRh0skfTU+Shsr4l+7KMZJgwbZWWidyUQTkT5BgTARERERERFJj7w8TMPOBLMAosoVrcDz8tNbMadMTjkYZraZlK0oS3ztSTHKJHftgokTobCw8z3KRKRXUyBMRERERERE0sPjwX/sULtkMU7PLsuAumzwjyHlfmH+9X7qW+rjHg9de2ycCelq2C8ivZYCYSIiIiIiIpI2ywt3OprXmAnU1dl9xRxq3Nro/NqxBJr4d7phv4j0WgqEiYiIiIiISFr4fHdS8dU9jubmtQb+0egsuAWQNzQvtWvHYlkpB+BEpO9QIExEREREREQ6zWwzKfugMvlEC3K2gWdd4Hmes+AWgGeMB/eA4RhW7OOGBQXNYddOJIUAnIj0HQqEiYiIiIiISKf51/up37slbm+wcE0HwZ0eMA8YAOPGOb6HK8NF5ZfLANoFw4LPK1aAK06gLMLIkY7vKyJ9hwJhIiIiIiIi0mlO+3dh2F9zz4LCH+7F58lJqXm913sTVS/mkN8SOe5ugaol4K1xeKG334ZFi6C6Wv3CRPoRw7IsJ7HyHqWlpYXs7Gyam5vJysrq7uWIiIiIiIj0e9W11Yx/YnxqJ1l2XKxqCXjvWAZer7PzfD7MKZP5+xjYmAkjt9nX+izT7g/mWecwKyzI7YbKSuf3F5Eex2msSIEwERERERER6TSzzaSwspCGlgYsnH/MNCw7m2ttVT6utevA5XJ0nrVsGV9c+l1WHrKVsklQn73vmLsZKlekkB1mBOo5q6oUDBPppZzGilQaKSIiIiIiIp3mynBROakSsEghDoZlQF02+F0NKe3kuPeCC/EVWZROhfqoz7wNWVA6FXxFThcRWHB5ucokRfo4BcJEREREREQkLbw1ULWYdv27nGjMJKWdHHe9+AK3FrfaMbeoBv1W4Hn5JDAdNO+3T7Kgri6lYJyI9D4KhImIiIiIiEjnmSaUleGtgXUVcNtL2JlhDrPD8lqBDz90Nrmqin+Wl9jlkHECXaFMs7HOLhmSQjBORHofBcJERERERESk8/x+qK8H7Eb1t6yCZUuSZ4cZFhQ02w3ueeSR5KWJN9wAU6aw8YA9jpbVmGlnhVUXwqJj7ceEWWJ5eY6uKyK904DuXoCIiIiIiIj0ATEyqbw1ULIG7vTA3OCGkmFBKCOQLVaxIrDLY329HVArLo59j6VLYd48IJBB5sCHw6Gw3GEz/YwMGDfO2YVFpFdSRpiIiIiIiIh03siRMYfDs8PcUdlh7haoWhIVkIpXmmiacNVVoaeedXZAy4hTemlYkLMNbh2fQjP9tjb4xS9iX1BE+gQFwkRERERERKTzfvObhIe9NVBbASsfh4VV9uPaihhZWfFKE/1+2Lw59NRl2Vld0D4YZgQ3rjSI20zfAr5/HuyO/lQ8b552jhTpwxQIExERERERkc5ZutT+SsJlQXEtTPuv/eiKzubKzQWPJ/bJcUovq2L0IXO3wG0roekg4jbTx4BNmeCeHZUZ1toK1dVJX4uI9E7qESYiIiIiIiIdF1Wy2ClXXQUuV+xjcUovg33I/GPtxvh5rXbZ5JJjnN1y00F2mWREiWZ1NUyYkPLyRaTnUyBMREREREREOi6qZLFTjjyyQ6cFM83COW2mjwFYUD7JDqi1y1ITkT4l5dLIVatWccEFFzB69GgMw+Dpp59uN6empoYLL7yQ7OxshgwZwsknn8z69etDx3fu3MkPf/hDcnJyyMzMZPLkyWzcuLFTL0RERERERES6Qbzm9h0Rrz8YwGefpXSpZM30w1kG1GXbWWVA/F0rRaTXSzkQtm3bNo4//ngefvjhmMc//vhjTj/9dI466iiqq6v5z3/+w80338yBBx4YmnPdddfx7LPPsnTpUv7+97/z6aef4vV6O/4qREREREREpHskCl6lKlFmWYr3CW+mj8Msr8ZMICMjfp8yEen1DMuyOpz4aRgGTz31FBdddFFo7JJLLmHgwIH84Q9/iHlOc3Mzubm5LFy4kNLSUgDWrFlDUVERr7zyCqeeemrS+7a0tJCdnU1zczNZWVlJ54uIiIiIiEgX2b0bBg+GtrbOXysrCzZtggMOaH/MNGHYMMxtre36gSUqZ/QVwf+eB5szk99+5eOBEsuVK5UVJtLLOI0VpXXXyLa2Np577jm+/OUvc/bZZzNy5EhOOeWUiPLJN998kz179jBx4sTQ2FFHHcWYMWN45ZVXYl53165dtLS0RHyJiIiIiIhID7B6dXqCYAAtLZCfDz5f+2M+H76CVgrLYfxlML3UfiwsD9v1MTMTXnjB3sEyPx+wG+A33Ae524ibGWZYUNBsB9WA9JZ7ikiPktZA2GeffUZrayu/+MUvmDRpEn/729+4+OKL8Xq9/P3vfwdgw4YNHHDAAQwbNizi3IMPPpgNGzbEvO7dd99NdnZ26KugoCCdyxYREREREZGOSnfQaPNmKC2NDIaZJr5fzKR0KtRHJXo0ZNm7PvqKgNZWe9fJ0lL45BM7www4oA1+/We7L350z7Dg84oVYZll6Sz3FJEeJe0ZYQAlJSVcd911nHDCCfzkJz/h/PPP59e//nWHr3vjjTfS3Nwc+qqrq0vXkkVERERERKQzPvww/de0LCgvt8shAfOOn1N25g47ocuImhp4Xj4JTIN9gbnVq+0MswBvDVQtgfyoAiN3iz3urQkMFBSoR5hIHzYgnRcbMWIEAwYM4Oijj44YLyoq4h//+AcAo0aNYvfu3XzxxRcRWWEbN25k1KhRMa87aNAgBg0alM6lioiIiIiISGeZJixY0DXXrqsDvx88HvzL7qN+cvyp4bs+FgezuWJkqnlroGQNiXuMXXKJnVUmIn1SWjPCDjjgAE4++WTef//9iPEPPviAsWPtfWhPOukkBg4cyIsvvhg6/v7777N+/XpOO+20dC5HREREREREupLfDw0NzuZmdODjZ2Mj+P00WludTR89dF82V5zyRpdlN8Sf9l/7sV2j/Xvuid2jTET6hJQzwlpbW/noo49Cz9euXcvbb7/N8OHDGTNmDHPmzOFb3/oWZ5xxBuPHj2fFihU8++yzVFdXA5Cdnc3ll1/O9ddfz/Dhw8nKyuKaa67htNNOc7RjpIiIiIiIiPQQToNgAH/6Ezz0EKxa5fycvDxobOTD4Q6nz5q9L5vL44ERI+yeY9hlk453mywvh5ISZYaJ9EGGZVkJNpptr7q6mvHjx7cbnzlzJo8//jgAjz76KHfffTf19fUceeSR3HbbbZSUlITm7ty5k9mzZ7No0SJ27drF2Wefzfz58+OWRkZzuiWmiIiIiIiIdBGfD2bMgJ07k8+97Ta45RZ48UWYONHZ9YcNg82bMVdVM/bZiTRk0a4/WIgF7q1Q+4tduAYesG/8uuugogJfEZRNgvrsfYfczVC5Iqw3WLSVK6G42NlaRaTbOY0VpRwI6wkUCBMREREREelGPh9MjmzaFTfjyu2G2lo7u8o0ITsbtm1Lfo+yMqiooPrjFxn/x+TBs9tegltuiwpeVVfju2o8pVNp12g/uFtkRKP8cAsXwrRpydcpIj2C01hRWnuEiYiIiIiISB9nmnDNNRFDviIoLIfxl8H0UvuxsNwep7JyX4mhywU33ODsPhddBEDj9s8cTT9iC+0a5JvfGEfZeS5nu01GGznS2TpFpFdRIExERERERESc8/vh009DT31FUDoV6qMSMBqy7HFfUdT5N90EmZmJ75GTE2p6nzc0dtP7aHmttGuQ729YTX2mGbekMny3SRHpHxQIExEREREREefCGuSbht17K2HG1YpyzDYz8uCgQTEvbRpQXQiLjthFdW01ZpuJZ4wH91B3qJQxmmFBQTN42tz7dowMaNzaGPukKI2x4nKfOctEE5HeRYEwERERERERce7uu0P/9I8NNKBPlHHVUod/vX/foN8PTU3t5kaUV05qZfwfJ1JYWcjy95dTeU4lGLQLhgWfV6wA1/2V7XZ5TCmbrN2gs3NFpHdRIExEREREREScWbwY3n039DRmJlUMEZlZje2ztOKWV7Y0ULqkFF59laoXcshviTzuboGqF3Pw3rEMvN521/WM8eDOcmPEidSFssnWhQ8aUFDQLrtMRPqGAd29ABEREREREekFTBNmzYoYiplJFUPeQWGN56MyrRKWV2JhAOXvzGPty1DycoydKRf/KmYQDMCV4aJyUiWlS0oxMLDYl1IWkU0WHDYCC6ioaJddJiJ9gzLCREREREREJDm/H1oiU7I868Dd3L5kMSiUcbU+/CQPjBix77LJyivZ19DeZUFxLUz7r/3osoDZs+0gXRzeIi9VU6vIz8qPGHcfkGNnk9WED7qhqipuYE1Eej8FwkRERERERCS5GCWNLgsqV9j/Tti/a0NY43mXCy69dN9lHZZXNmQGGukfaz+awcBZXZ0dpEvAW+SltqyWg3fdxYjdcziMX7L2JxvxVm/khh9Wcu0Fc/j375+CtWsVBBPp41QaKSIiIiIiIsnFaR7vrYGqJXZ5Y332vnF3ix0E89bEOLekxC4/BD4c7uz2150Dm4aEXb/ZDsJ5a4jYyTKeYDbZyFbY+iXLziZzufjw6JP4V+ZhnP/Vk1QOKdIPKBAmIiIiIiIiyXk8kJMTc8dHbw2UrInRv8vCLjeMbjzv8cDw4fgO3sLc8dj1j3FKI4NtvTYdFDnckGU32K9aAt5NmxKv3eeDsjL+VF+/b+ylB+G++zjx400UfFjLkFd2wFEXKxgm0scpECYiIiIiIiKdFsy4aqeysn1wyeXCvPYayppvs58nCYLFmmMZdvll+SQoGZFD3PCVzwelpWBF1W7W18PUqdwSfP4scJvbXq/KI0X6LPUIExERERERkeT8/pjZYAnl5NhlkLEuN/0bCZvkQ+CYEX+OZQQa6WfGWZdpQllZ+yBYPA0NdtDM53M2X0R6HQXCREREREREJLkYzfKTamqK28i+8V+rOrmgsGsdkhv7gN9vZ345FQyYlZcn3IlSRHovBcJEREREREQkuTjN8pOKE0DL29qJtURfKzs/pXsnZFmOdqIUkd5JgTARERERERFJLtgsP1VxAmieQ4txN9t9vmIxLHtnyGRzCgYMxzPGE3tCR4N30LEgmoj0eAqEiYiIiIiISHLLl6fWI8wwoKCg/Y6RAa4ziql8eag9NSrQFXxeucL+SjSn4oASXBlxWuV7PPaulUaiRmRxdCaIJiI9lgJhIiIiIiIikliw6bxTwcBTRUX7HSODXC684y6nagnkt0QecrdA1RLw1thfCeeMnhB/HS6XvQtk+JqcrD1BAE9EercB3b0AERERERER6eFSbTrvdttBMK838bySErwVFZSsAf9YaMyEvFbwrANXWAaYt4b4c/Lj9AcLneyFqio7kJfsNTgJ4IlIr6ZAmIiIiIiIiCTmtF/W1VfD5Ml2NpWTQNK4ceBy4TJNimsTT3VZtJ/jNHPL64WSEjug19holz1u3gzXXRcZHHMawBORXkuBMBEREREREUnsww+dzcvNheJi59ddvdouu0zCAtoVNhpGaplbLlf7tV18MU898CdWrvw3x5x0JP/7s8uUCSbSx6lHmIiIiIiIiMRnmrBggbO5jzziKLAV0tDgaFq7IFhOjl3u2NnMLZeLz756Ks8cfSbvF52kIJhIP6BAmIiIiIiIiMTn9zsOWFFfb893atOmjq0pld0rkxjosj8W7zGtJDNFpC9QIExERERERETic9ofrCPzc3NTu3aQYUB5eWrZZ3EMdBlYmKxr/SeL3llEdW01ZlvnrysiPZN6hImIiIiIiEh8eXldNz/Zjo/xWBbU1dnZZ6n0JIvhX5v+SsOgn7H+080867PH3FluKidV4i1S03yRvkYZYSIiIiIiIhKfx2PvppiMYTjfxTHVa8eTarZaFF+Nj/v/9QNMY3PEeENLA6VLSvHV+Dp1fRHpeRQIExERERERkfhcLqisTD7PslLbxTH82ka7dvjOpJqtFsZsMylbUQZY7brxW9j9wspXlKtMUqSPUSBMREREREREEispsXdqTCQnx56XKq/X3gEy1cyw3NzUss+i+Nf7qW+pj3vcwqKupQ7/+hSa/4tIj6dAmIiIiIiIiCTm9yffqbGpKbUdI8N5vVBbCytX8u4pZzk758wzU8s+i9K41VlZpdN5ItI7KBAmIiIiIiIiiTntxdWZnl0uFxQXs3nsEc7mH3VUx+8F5P3rQ2fzhna8/FJEeh4FwkRERERERCQxp724OtGzK6jumK85m9iZ3SJNE8/sStzNYFixpxgWFGS58YzpePmliPQ8CoSJiIiIiIhIYsHdHeM1te/IjpGx+Hxc+MBNzuZ+/nnH71NdjWvzFipX2E+jg2HB5xWjL8eV0fHySxHpeRQIExERERERkcTCdo60ordYDAbHUt0xMprPB6WlDG36zNn8668Hs4M7OlZXA+CtgaolkN8SedjdYo9756+ERYvs+R29l4j0KAO6ewEiIiIiIiLSCwR2d2yedRXDmjbuG3e77SCY19vxa5smlJWB1S7MFl9dnd2cvzMlktjBsJI14B8LjZmQ1wqedeCygJpVsGqVPdHttoOBnXmdItLtFAgTERERERERZ7xe7txTSN3yv3LF4YOZeNYJdjlkZzLBwA5o1denfl5Hm/MXF8Mdd4Seuiwork1yTn09TJ4My5YpGCbSi6k0UkRERERERBwzjQxeHfMVPpl4gR1Q6mwQDDoe0Opoc/7iYsjM7Ni5s2apTFKkF1MgTERERERERByzAo3kM+I1zu+IjgS03O6ON+d3uWDOnI6d29QU6jEmIr2PAmEiIiIiIiLiWJtlJZ+UKo8HcnJSO+fKKzuXjfaTn0BGBz8SKxAm0mspECYiIiIiIiKOdUlGWEcccUTnzl+9GtraOnZuba3KI0V6KQXCRERERERExLFgRlhGOuNgfr9dcpiKjvYHC+poXzKAP/4RCgvB5+vcGkRkv1MgTEREREREpBuZbSbVtdUsemcR1bXVmG09O9MomBFmpDMjLNWgVGf6gwV1NpDW0AClpQqGifQyA7p7ASIiIiIiIv2Vr8ZH2Yoy6lvqQ2PuLDeVkyrxFnm7cWXxdUlGWKpBqc72BwM7kDZiBGze3LHzLQsMA8rLoaQkPbtnikiXU0aYiIiIiIhIN/DV+ChdUhoRBANoaGmgdEkpvpqemWnUJRlhHg8MHw6AaUB1ISw61n40Y92ms/3BwA5cXXpp565hWVBXZ5d2ikivoECYiIiIiIjIfma2mZQ9NQsrxg6MFvZY+YryHlkmuS8jLI2BMJcLrrkGXxEUlsP4y2B6qf1YWA6+oqj5nS1rDCopSc91OtNvTET2KwXCRERERERE9jP/H++kfk8TxIklWVjUtdThX9/zMo3aQhlh6b2urwhKp0J9VuR4Q5Y9HgqG5eZ2vj9YkMcDmZmdv066AnMi0uXUI0xERERERGR/Mk0af1cJZyWf2vjPF+GVRjvQMm4crF5tZx/l5dlBHJcLc89u/M88SOO//0EemXjO+Dau8RO6rGeV1QU9wsw2k7IPH7Rz4aKuaxlgWFA+CUrWgGv69PS+toxO5oekMzAnIl1OgTAREREREZH9ye8nb/0WR1PzfnwH1AaeuFxghpVKut34Lj2Jsj3PUj+0DQKxIfdf/kjlTZl4f/wEeNPfcD9YzJnOHmH+9X7qzS3xM+QMqMsG/1goLixM233x+6GlpXPXmDFDjfJFepGUQ9+rVq3iggsuYPTo0RiGwdNPPx137ve//30Mw6CioiJifMuWLcyYMYOsrCyGDRvG5ZdfTmtra6pLERERERER6X0aG/GsA3eznekUi2FBQTN41oUNmpH9wnxD6ykdtJz6zLaI8YYsKD2nFd/PJoMv/Q33u6JHWONWZz22GjOxM7DSduM09PZKV58xEdkvUg6Ebdu2jeOPP56HH3444bynnnqKV199ldGjR7c7NmPGDN59912ef/55/vznP7Nq1SpmzZqV6lJERERERER6F9OETz/FZUHlCnsoOhgWfF6xAlxxAmWmAWWTiFtKCHYpoVl+bbsAWmeFeoSl8Zp5Q5312MprBfLz03jjffc1DXjxELh5vP31YmGcHSvDDR+uskiRXibl0shzzjmHc845J+GchoYGrrnmGv76179y3nnnRRyrqalhxYoVvP7663zta18D4MEHH+Tcc8/lnnvuiRk427VrF7t27Qo9b+ls6qqIiIiIiMj+5vNBWRnU1wPgrYGqJXZAqz573zR3ix0E89bEv5R/bOQ50UKlhK4Giv1+KC5Oz2sgrEdYGrde84zx4B7qpqGlPhTIC2dY9vfF0+ZOb+DJ47FLTIfWM+t8aBqy79AdZ0LOdljwbIKfxY4dsHx5l5SgikjXSPuukW1tbXz7299mzpw5HHPMMe2Ov/LKKwwbNiwUBAOYOHEiGRkZvPbaazGveffdd5OdnR36KigoSPeyRUREREREuo7PB6WloSBYkLcGaitg5eOwsMp+XFuROAgGgRJBBxozSU/5X5hAHCytpZGuDBeV51SCkSRD7v7K9Pbjcrnw3T6NyVOh6aD2h5sGw+TwHSuj7dgBk7umBFVEukbaA2H/93//x4ABA7j22mtjHt+wYQMjR46MGBswYADDhw9nw4YNMc+58cYbaW5uDn3V1dWle9kiIiIiIiJdwzTtTDArdp2jy4LiWpj2X/sxXjlkuDyHLZbzWoko/0uHYI+wdDbLB/AWeamauoz8A3Iixt0tUPViDt47lqU988psM7n284V2nWeslxMYK5uUpEyyrCztJagi0jXSumvkm2++SWVlJW+99VZa/ygOGjSIQYMGpe16IiIiIiIi+43fH5EJZhp2aWNjph2o8qxLHPyKNT/YbL8hi8SlhPUZMG5cWl9OKBCW1qvavEVeSo4swb+2msY3q8nbCp4zinHNK+6SnRn96/00bG1IPMmwy1D9Y+1AZUz19fbPOY0lqCLSNdIaCPP7/Xz22WeMGTMmNGaaJrNnz6aiooLa2lpGjRrFZ599FnHe3r172bJlC6NGjUrnckRERERERLpfWGmiryhGT7Bmu3F+rHLIRPMrV0DpVDvoFR4Miygl3NsGq1enuUeY/ZjO0shwrgwXxYdNgMMmdMn1wzndrRIclKOmuQRVRLpGWksjv/3tb/Of//yHt99+O/Q1evRo5syZw1//+lcATjvtNL744gvefPPN0HkvvfQSbW1tnHLKKelcjoiIiIiISPcLlCb6iuzAVX1W5OGGLHs8ug9VsvlgN9vPj9pLzN1ij4cCa13WIyytl+0WTnerBAflqGkuQRWRrpFyRlhraysfffRR6PnatWt5++23GT58OGPGjCEnJ7Kee+DAgYwaNYojjzwSgKKiIiZNmsSVV17Jr3/9a/bs2cPVV1/NJZdcEnPHSBERERERkV5t3DjMARmUTWrDgnY1hVagQXz5JChZY5dJmoadCZZs/toK+5yEpZa9pEdYd/CM8ZA/ND9xeWSwzHRdggu507ybpYh0mZQzwt544w1OPPFETjzxRACuv/56TjzxRG655RbH13jyySc56qijmDBhAueeey6nn346CxYsSHUpIiIiIiIiPd/q1fjdbXZ5Y5zYkWVAXaAPFdiPTufHbbZvGFBQkPYAzb5AWFov2y1cGS4eOOeB+BMC38vKFUk2MahM826WItJlUs4IKy4uxoqz20kstbW17caGDx/OwoULU721iIiIiIhI79PQkLy/VMCyQHlkw1Bn85Net6Ii7QGa4KfBruoRtr95i7wsm7qMWc/OomlHU8SxnO2w4M+x+7eFlJdH7mZpmnbj/MZGOxvP41GQTKQHSWuzfBEREREREYmyaVPy/lIBD51if41wOD/udTMyYPHiyABNmrT1oR5hQcHdKqtrq6murQag+PMsiqfckDgTDOD88/f92+eDsrKIXUJxu+2MsS74WYhI6hQIExERERER6Upr1+JZZ+/22JAVucNjiEVEGeTmIexLvYox30jWt6qtDd57r1PLjsfqQ6WR4VwZLiYcOoEJhwZ2q3zxxX0/g0RM0370+aC0dN9uAkH19TB5MixbpmCYSA+Q1l0jRUREREREJIxpwsKFuCy7zxTYQawIUUEwe1LU8fBDgecVyfpWPfDAviBNGvWlZvkJffaZs3nTpsHSpXYmWKI2QrNmdcnPQ0RSo0CYiIiIiIhIFzFXVVOduZlFx8LwHbBkCeS3RE2KF08ywr7CuFugakmSvlUATU12r6o0s0KlkX08EOZ0t80tW2Dq1MhyyFiamuDOOzu/LhHpFJVGioiIiIiIdAFfjY+y6pnUX7ZvzN0M9/0VcrfbjfEfOsXhxQLBp9tWwk3+JJlg4RobU1ixM32xR1hMHo/d36uhIXGmVyoeeABuuknN80W6kTLCRERERERE0sxX46N0yWTqjchu9g1Z8K0psGUwTE6W0RXOsBPDfnuS/bRt0CBn5znNakpBqEdY3FS2PsLlspvcp1NTE9x6K1RXq0xSpJsoECYiIiIiIpJGZptJ2VOz7CSiqFhRsFF++SQYt97OEGvXMywOy4C6bHju8ANY/spSFp2aycpCMOPFo1wu2Lw5yWJNOyizaJHj4EywR1ifzwgDu7n94sXp3Rngjjtg/HgoLLQb7IvIfqVAmIiIiIiISBr511ZTv6cpbu+vYEBr9ZgEDfQTuMy7G+8zFzJ9UitnXQaF5eArsgNi1YWw6Fj70Wwz7d5V8YItPp8djBk/HqZPdxycCVYJ9vlm+UG5uekrjQzX0GDvMqlgmMh+pR5hIiIiIiIiadT4ZrWzeZkw7b924/uySVCf7ez6nx8Y+bwhCyZPhZwd0HTQvnF3M1SusPBeeSVkZ0Nx8b7eVD6fHYSJDvDU18PkyVBeDiUldp+sqH5W/SojDJz3WRsyBA480C5/dMKy7Eyz4PdafcNE9gtlhImIiIiIiKRR3lZn8z4cbj96a6C2Al54AoZvJ9QYvx0r8BWn3LJpcOR4QxaUTgXfwVtg4kQYNQqWLrXLH8vKEmc5VVTYGWLDh9ulgeH3628ZYU77rLW1wa9/DYYR90fYjmVBXV2X7O4pIrEpECYiIiIiIpJGnnoX+c3ED2hhH3vkpH39vVwWTFgLjzxrx7nalUoGA2DxYk8xjoX3IzMN7H5hU6fCjBl25pcTLS1wySVw0UWhoX6XETZuXLseYe3KUA1gxw740pegqopN2bmp3aMLdvcUkdgUCBMREREREUkX08T1yG+Z9Sbxg1bYx+qzwT82cthbY5dK5rdEjg/f0bHlBPuRRdwnKsPLkeXLYc4c+5qBoX6TEbZ6dUT2nK/I7ss2/jKYXmo/Bvu0UV0NJSX84ls/4YHTvsWyomJn93j//XSvWkTiUI8wERERERGRdPH7oaGBI77kbHpjZvsxbw2UrLGDV42ZkNcKJjDxso4vK9Z9UnbffXDnnf0vIywsW8tXZJebRifsBctQq+pexFv4OPc5zbgLmj8fbr5ZfcJE9gMFwkRERERERNKloQGwg1dOxJvnsqC4dt9z07Cb3zdk7St5TIXT9STU1gbz59PW9hWgH2WEBXqEmYa9qYEFMctQDQvKs1+hpAFSDmdt2mQHUYuLO79eEUlIpZEiIiIiIiLpsmkTAOPWQ+424vYJMywoaAbPusBAbm67PlThXBZUrth3boRgE30n9+msjz8O/bPfZIR5PJCfj39sYGfPOK87ZhlqKtQnTGS/UCBMREREREQkXdauxVcEh5XBpiHEDJoEA1kVK+wAF7m5dmlcol0cid8/LGd75HXj3icdDjssrDSyn0TCXC544AHH5aWx5sVsrh/N6e6UItIpKo0UERERERFJB58P3/MPxOwhFc7dYgenvDWBgRkzIMNZjkKwf1h1of0FULwWPh8M100KZCzFu0+KTCOyT5lnHbhGj6bto3RF1XoRr5e81ttg7dykU6PLUH1FdkllxM+m2c7wC/1sCgrszDMR6XKGZSX5zw49UEtLC9nZ2TQ3N5OVldXdyxERERERkf7ONDEPGUthaQP1WcQun7Pscsn6++CAtrDxF16Ayy4Dhw3W4wVW7vsr5G6PClx18NNerHuMaIWH/ubi1sl3sXnXZuZdfCbf/uokXBn9o8G72WZSWDGWhpaGmH3aDMsOPq6t2Pd9j2iub0TOBTvDz7vGgKoq8Hq7+BWI9G1OY0UqjRQREREREeksvx+/qyFhDykM2JQJq8eEjRUU2I8pBMFKp2IH28I0ZMG3psCWwTDtv3aj/WRBsHjlevHusTkTLrnYZI31YzYfMI/vPnc+hRWF+Gp8jtbe27kyXFQWzAKclaEma64PUH6eC3PpYgXBRPYjlUaKiIiIiIh01lNPpd5DyjCgogI++8zReU52LSybBJk74e+FsD4bxjTDWWuhOCo7LFFW2fVnx75HLA0t9ZQumUzV1GV4i/p+MMe79wiqlsT43sUoQw0114/DMqAu08R/Ui7FXbZiEYmmQJiIiIiIiEhnmCY89hh5Oc6m57UCbjdUVtqZQNXVjs5zElipz4azZ0aO33Wm3VB/wbN2oCaiXC9MQxZMnULiAFicAFz5U7MoObKk75dJ5uWF+rS1658W9Q11HBjdqt0iRfYnBcJEREREREQ6w++HrVvxtNpZVQ1ZJOwh5TnJC4uX2LsRgt0kfcQI2Lw54W2cBlZiaRoMk6fCkqXxM74sg8Rd/uOwDKjb04R/bTXFh03o+CJ7g02bAMiw7PLTcNGbC4xsbX96LHlDtVukyP6kQJiIiIiIiEhnNNoZPS7L3gmwdKod9LJiNEevWAGu+dfsC4KB/e/582Hq1IS3id6NMCWBINdV59q9vhLO66DGN6uhLwfCTBOuvx5o/22KVWqa32xn4m0ZnCAwekAOnjHaLVJkf1KzfBERERERkc4YOTL0T2+NvRNgfkvkFHdLYIfAzbl2Bli0KVPg/PMT3sazzs4460jWFgBGkiBYJ+Vt7bpr9wh+f8xNDeJtLvBplp2JZ5Gguf7/6/jOnn2Z2WZSXVvNoncWUV1bjdlmdveSpA9RRpiIiIiIiEgaBEvjdrng8acBCz6L7iG1dH5kNli42bPhz3+Oe32XBdPegXnfIBBdSf9rCAkGZxzcI1TyeUZxFy6oB2hs38vLyQYGw7fD4L3xmus32QG24uKuXHmv4qvxUbaijPqWfUHHEQeNYP6585lyzJRuXJn0FQqEiYiIiIiIdMaf/xx3F8bKFWG9pM4/H0pL41/H4wG3G6u+ASNG2peZAYtOcAFmp4Jgudtg80Hxy/VGbINNDjPHQplNL2fimlfc8UX1Bnnte3k52cCgaQi88IQdyIzZXD9GgK3fMU3w+/F9sJzSxop2v/2bt29matVU5nw6h1/+zy+7ZYnSd6g0UkREREREJBHTtHd2XLTIfjTNiGO+1b+LWRrXkGWXzPmKAgPl5Ynv43LBtGnEq330j4H6zE4EwSw7ODc/kHQWr1xvxjvOL5kfLPk8b078TLe+wuOB4cMjhpxuYPDZEDsgOu2/9mNEOWSMAFu/4vNBYSHmWeMpW1OBlaBUdN7qeVS9W9X5eyZ6T0ufp0CYiIiIiIhIPIEP6YwfD9On24+FhfY4YK6qpuwbW+PvwgiUT7JL6Bzd65574h5uvCRxD7GEAsGFyhVQmqSPWcn7zi972nrwfpYDN93U8bX1Fi4XlJVFDDndwCDuPLc7ds+4/sLns7Mk6+v3Zdclea9c9ZerOtczLMl7Wvo+BcJERERERERi8fkwp0ymekA9i46F6sJAQKuhwf7w7vPh/6Q64Yd3y4C6bLuEjs8+i38v07SDLJYVNw6Q9+JrHX4pOdth2RK7mT/Yj7UVsPJxWFhlP66tsMc96+y+Vk4sPRaqfnlZ388GC7rpJsjJCT0NbmAQnV0XZFhQ0GzPi2nHDli+PP3r7A1ME2bOJJgC5jS7btP2TfjXVnfsnlVVMHly+00P6utD72np+xQIExERERERiWaa+O6fxdgyGH8ZTC+1H8eWg+8oy/7w/r3v0Thot6PLNWaSuAQuzo6E4TxvbMJ9QC5GnFCZgcHIwaO58s1irv/HAC59G376d3jhcdh4z74gWJDLil2u57Kg7FVHLwsMuKrp9/1nVz+XCxYsAMPu4uay7Cw7SLAz5IoEO0Nu2dJ/AzAzZkDrvlQ5p9l1AI3fm5r692zpUrjkkvjHLcsORqtMss9TIExERERERCSKz3cnkyc00RCj79fkYN+v5mbybnbWuDtv8IjEJXAOGqa7LKjMmQHQLhgWfH6z55f87egf8dSJy7hh8tPcedy1TKhNEIiJduCB8NOfcpMfMnc5O2XT9k341/sd3qAP8HrtzCK3236apNQ0OgAZIdgQq7y8fwVgdu+GxYsjhjzrYITTUtP1KQQQTRNuvx2mTk3+Pa6vhzvvdLYI6bUUCBMREREREQljtpnMWjPPfhKdfBV4fuX5dpmk49K4H89PXD7osGG698slVE2tIj8rP2LcneWmamoV5x5+ATsz/sPWgf/g5cMNzIsudHTdkBtvhNtvx/Wl4cx52flpjVv72c6HXi9GbS2XXnInnx+YycUJSk2Tsiyoq7OzAvuL+fPbDbksmP8X7H528QK30aWmyQKIPh+MHYt561yqC4kscY5n7tz+maHXjygQJiIiIiIiEqa6tpqmttb4TbsN2DIEfn6Gw9K47G/hmjwl8U09HjvDyIhzU8OAggLwePAWeaktq2XlzJUs9C5k5cyVrC1bC8Dpvz+ajYN+yuYD5vGDv5ZQ+PLUfbtWJpOZaffACjSF/8k/wGhzdmre0H6486HLxZuHf5XFX/mm/TROqaljDrIC+4yPP445POU9KFkT5xzLfkuGSk2TBRADjfh9WQ0UlkeWOBeWk/h90d8y9PoZBcJERERERETCVNdWO5p3W7H9YTppadyXS5JfzOWCykr739HBsODziopQVpkrw0VxYTHTjptGcWExy99fTumSUhpbGyJObTC3UDo1yYf+oDlz9mWt3XQTq4/NwnLwiTHXlYVnTP/c+fB/PvDz5c98zjKNkhk5Ml3L6vkOOyzmsK8Injkq/mk/ejlGll0wgGiaUF0NixbBiy9CWRm+oyxKp0J9jBLnhO+L/pah188oECYiIiIiItJBZZPs4EeiXRidlj2Gek/lR5Y94nbb415vzNPMNpOyFWVYMerJgiPlk5IEaYLZYEEuF41l33O07Blvt6We/dQH+N6toqrgF0y4DGY4zTRKpD8FXq66CjIiwxGmYb+fLIibjbnouBi/x3l5dvZXYSGMHw/Tp8PEiZgN9XGvZwWel02CFwvjlEz2pwy9fkaBMBERERERkTDFhcXOJhpQnw3+sfbTmKVxWVmJm+RH83qhthZWroSFC+3HtWvjBsEA/Ov91LfE33HSMqAubJ0xhWeDBeSd5SCTDSh5s7V/BXEAX42P0qopNGZG1o4mzTRK5MEH+0853gEHwAUXRAz5x9rvp0QlyfXZcGf428nlgs8+sxvnR+26mux6VuB6Ey+LUzLpNIAtvY4CYSIiIiIiIkGmyZY3/PGbdcfQmJngYJvDJlvhXC4oLoZp0+zHRE32cd6oPu46c3Iis8ECPGM8uAcMj7sRQETj8n6UPRPKwLOIm2mUNAMvli1b+s+OhaYJb74ZMZTwfRRm7viwYJVpwtVX79t9swPXCxcKZJ6SYgBbehUFwkRERERERAB8PsxDxnL9q7eldNrIbQkOtnZ9tpTTRvWjWuMc+N73YgbbXBkuKr9cBsTeCCCicXk/yp4JZeAlyDRKmoEXT3/ZsdDvb5fBlRfv9zOGiEDjpk0x56RyvaBQIPOCgZ3r99YThfdQq67uP9mHMSgQJiIiIiIiEthhzu9qSFyeFUuy7LEuzpbyjPHgznJjxFm0EcjcOmNdnAv86U9xPxR7vTdR9WJO/I0A1uzbzbK/aGxuSD6JfRlJpmH3n3LcUL+srO8HKe69t92QZx24m0n+fnIYaHR8vSiWAXV7m/Cv70PlvtE91MaPh1GjYOnS7l5Zt1AgTERERERE+jfTtIMPltWhcqrPkp3TxdlSrgwXlZPsHSejg2HBZ6HMrVgS7ZDncuG9bgG1lTE2AljTfjfL/iBvbewMpGgfDgffd75GYbndfypmH6pY6uv7ds+13bvhL39pN+yyoHKF88ske6+6LLjvr4EnHdjMwWnJcY8XCPJHZ+CxeTNMnQo33NA96+pGCoSJiIiIiEj/Flam1ZFyqoTnHHjgfsmW8hZ5qZpaRX5W5I6T7gE5duZWTZILJMpa83pxLV1G8V535EYASXaz7Ks823NxN7cvF41g2b2sJh/yBvVZkYccNdTvyz3X5s+P2zvPWwO3rXR2GSfv1dzt2NHgDpQ5Oi057tHCgvxxzZtnv4/7kQHdvQAREREREZFuFRZ0CJZTNWTt6xcUj2HZJYKeeCWHABn7L/fAW+Sl5MgS/Ov9NG5tJG9oHp61Jq6fTUx+crKsNa8XSkrsoGFjoz3f4+lXmWBBrtH5VK6AyVOxM41i/Z4Y7MtCitFQ37DsPlcla+Jk6vXlnmsff5zw8E1+eOQk7ABijO9tvPedZRgYgYCPadilk8s6sHunYYF7mwtP/rjUT+5pYvRii+mqq+Dii/vN+1kZYSIiIiIi0r+FBR3Cy7MSZfwEjyUsOQTYvn2/lrm5MlwUFxYz7bhpFBcW4zqj2M7cMuJE9YwUenyluJtln7Vp077MpUTB0gSZSAkb6rvd9s+jrzY3P+ywhIeD70GD2Js0QIz3XWkpe0fZ72NfEaFy1IdOSW1poes/Z+J6eXVqJ/dEy5c7m7dpU98ux42iQJiIiIiIiPRvHg8MHx566q2xG8FHN4gPF2oWn6zkELq3zM3lgkq7f1i7YJjRP3t8dYppwvXXA3DEls5fLmafqyuvtAMY0c3NCwv7xo6SV12V9Pct3nswaycsqorxvvv+93l39X+4vOQblE6lXTlq3B5hUeMR7+veXp5qmvDoo87n9/bXmwKVRoqIiIiISP/mctl9dObODQ15a+yyNf9YO1gxchtg2Y3x81rtsqwMpw24u7vMzeu1ewCVlUWWSbnddhCsn/X46pRO9pOLFvMan39uNzeP7utUXw+TJ8OyZb37Z3bAAXYwcd68uJWlYL8HX8mH+8ZBWyCFp3kwzJgMb+bBL18ITMzJgeJitq9tYtGx79ixreiLBktVw8YNyx667SU7qBl8X4cyzT78MB2vtvtUV0NLgmh+tJEju2wpPU3KGWGrVq3iggsuYPTo0RiGwdNPPx06tmfPHn784x9z3HHHMWTIEEaPHs13vvMdPv3004hrbNmyhRkzZpCVlcWwYcO4/PLLaW1Nw18RERERERGRjihq30zIZdmN4af9FyashQmBf4eaxSeTStlhV/N6obYWVq6EhQvtx7Vre3dApTvE6CcXt4TWIm4mkmFBQXOc/nJPPpm4ufmsWb2/TPKXv4Q5cxL20LthItzzDWiLCmqZBsz7hn0cgGuvBeC1T//BjoEt8SNrUePuFli2BG5ZFed9/cgjvfv7/NJL3b2CHivlQNi2bds4/vjjefjhh9sd2759O2+99RY333wzb731Fj6fj/fff58LL7wwYt6MGTN49913ef755/nzn//MqlWrmDVrVsdfhYiIiIiISEeFlbs5MmhQwkwWoGeWHarHV+c57CdnhP5PCn2ugjZtSryGpia4806nK+657r6btxYsZNnR43nlKx47OPa3v8HVV7M7w84EA2JndwH3joPdGcDcufjOHMntf7vE0W2vfg1WPg5rK5KUNtfX9+6+WevXpzb/s8+6Zh09kGFZiULNSU42DJ566ikuuuiiuHNef/11vv71r7Nu3TrGjBlDTU0NRx99NK+//jpf+9rXAFixYgXnnnsu9fX1jB49Oul9W1payM7Oprm5mays6OJfERERERGRFFRX2z2Y0ik3F379a2Vc9TWmaffqamgIZW35iqBsEtRn75tW0AwVx80BoOy9+6jPNCOO3b8CJjvpLxdPTg5s3Nh7g5k+X/tS3REjYP58yM2l4sbxXDcp+WW++y84/wMonUrsksgYVj5uZ385snChHTjujW66Ce66y/n8lSvtAHkv5jRW1OU9wpqbmzEMg2HDhgHwyiuvMGzYsFAQDGDixIlkZGTw2muvcfHFF7e7xq5du9i1a1foeUsqda4iIiIiIiKJpNIk+qCDYPt2TGNf/7B2vYUA7r9fQbC+KLj5wOTJoaHofnJ5reBZD678P8HatZS03YH/ufk0bvyYvK0WntsedlZam0hTk52t1BsDFz5f7B5omzfD1KkwezYfFwwBtiW91GMnwNKjnQXBDMsuh4xZjhpPd/f364yBA53Pzc3tGSXc+0mX7hq5c+dOfvzjHzNt2rRQNG7Dhg2MjGrCNmDAAIYPH86GDRtiXufuu+8mOzs79FVQUNCVyxYRERERkf4klQ+7xx6LrwgKy2H8ZTC91H4sLLczg0Ly89O6ROlBvF647baIofB+csW14GoD6urA78c18ACKLypn2v8+SHHDwM4HwYIaGtJ0of3INO1MsESFaffey2GHn+z4kq2DSJ4JFqscNVl1WU5O7w0OmSb89rfO58+f33uzCzugywJhe/bsYerUqViWxa9+9atOXevGG2+kubk59FVXV5emVYqIiIiISL/n8ST/UBzg8xZROhXqo6Y3ZNnlWb4iek6DfOk6n3/ubF54tqHPZ/eMS5dkvcR6orBdNxO56tF3cJERd7OBEAelkAA5O6BqSVhPMMOAAUkK5MKq0nodv995oHTOHDtDrx/pkkBYMAi2bt06nn/++YjazFGjRvFZVBO2vXv3smXLFkaNGhXzeoMGDSIrKyviS0REREREJC1cLliwAEj8udsckEHZ3mdjlmFZgeflk8C8ZGq/yq7od0wT/vhHZ3OD2YbBTKh0ys1N7/X2B4dlyAdsbOJ69xTHga5kFi+NaoxvWbBlS+KTWlt776YETsu9r73W3qSgn0l7ICwYBPvwww954YUXyMnJiTh+2mmn8cUXX/Dmm2+Gxl566SXa2to45ZRT0r0cERERERGR5L71LSgpSTjF/63TqN+7Je6Hc8uAumzwV//eDnxI3+T32/2skgnvu+QwEyolvbH8NoUy5F8eVMK3jvlWp2+Zsz2F5vjR5s3rne/lqHZUcV14Ydeuo4dKORDW2trK22+/zdtvvw3A2rVrefvtt1m/fj179uyhtLSUN954gyeffBLTNNmwYQMbNmxg9+7dABQVFTFp0iSuvPJK/vnPf/Lyyy9z9dVXc8kllzjaMVJERERERKRLPP00dd/9AWZ0pMvlgtmzaaxzts1f445NduBD+ian2TYzZuzLDHRapnb22c7mDR/eO8tvPR57d0gn8vJ40vskIwZ0riLs2lfpeF+23pwVJnGlHAh74403OPHEEznxxBMBuP766znxxBO55ZZbaGho4JlnnqG+vp4TTjiBvLy80Nfq1atD13jyySc56qijmDBhAueeey6nn346CwKpyCIiIiIiIt3lwzlzOXL2Mn7rvQauvtre/XH7djj/fPLWJymlCshrJbWdKKV3cZrV9KUv7fu3035egwc7m1dS0jvLb10umDkTSNL+K9Bnz5Xh4tLMcR27lwU52+CmWDFpt9sOJjrxwAO9Lyssqh1Vp+f1MUm6w7VXXFyMlWCHh0THgoYPH87ChQtTvbWIiIiIiEiXMtss9g44gOcmfIsrrvrGvgONjXjWgbvZboxvxSiPNCxwt4BnHantRCm9i8djB1IaGhLvfnjrrXDssfYuk077eTn9vZkwwdm8nsY0YfFiIEn7r/vuCwX6SsaeTcUXK1K7T+DHsuDPMbLBDAMqK+G//4W5c5Nfq6nJzvAsLk5tDd3J6e9RP/071WW7RoqIiIiIiPQ2Zpv9qXlARtTH9Lw8XBZUrgh8xo7+cG3ZQxUrwOXWrpF9mstlB1IcJIFQXm4Hf2L08zINqC6ERcfaj6YBfPnLztbw8ccpLBjMNpPq2moWvbOI6tpqzLZuynBy2istrHzSc95VuLc62EEyTEELLFsS1SAfICcHqqrs4ORNN0FmprML9rYMz2Cw1ogdbmwD9ua7++3fKQXCREREREREAsxAcCMj+gNk8IOlExUVvbNsTZzzeuG22xLPsSyoq7ODP1G/P74iKCyH8ZfB9FL7sXC2C1/jS87uf+ut4PM5muqr8VFYWcj4J8Yz3Ted8U+Mp7CyEF+Ns/PTavlyZ/PCAk+ugQdQ2XSy41vcvwLWVsQIghmG/fPwegMXdsGcOc4u2tsyp1wumDYtZrDWws7G2/zzX/Tbv1MKhImIiIiISP9mmvDii3DzzRz+4P9xWu3bDDTaIue4XJgV91E2KfA8OtHCsIfKL83BvCjx7pPSRxxxhLN5jY37ssgMA18RlE6F+qge8A2ZJqWDn8VX5PD+wWyzBHw1PkqXlFLfEpmF1dDSQOmS0v0bDPP57CCxE1GBJ+/eIyh/1dmpB7fGaY5vWfCb30SOFdnfbCf9ynoVn8/e8TKBju4f0BcoECYiIiIiIv2XzwcHHwwTJ8Idd3DUbx9g0eKf8avrzm2XceM/KZf6bOI2N7IMqNvThH+9dozsF1Ltw+T1Yi5dTNl5LjsIEfV7FAxMlE8KlEkmEp5tFofZZlK2ogwrRsgjOFa+onz/lEmaJlx5pbO57hgle2PGUPK+s9PzWhMcDC8pNU24/nrAeb+yXsE04TvfiXs4+FpH/OzHvW8TgDRRIExERERERPonnw8mT7abYUfJ3N5iHwsLhjVuddYnyOk86eWc7AQZlU3kPymX+kwzcTA1G/xjHa4hQe8q/3p/u0ywiHthUddSt38Ct3feCVuc7brK5Ze3DzyddVZoswojTiqTYUFBc2CzingOO2zfvzvQr6xXmDEDtm1LOMUABn7akDCQ2pcpECYiIiIiIv2PacK118Y9HIpTzJoVyprIG+osA8jpPOnFwrKJEorKJnIcTHXYwz1RVlqPCdyapl0Wmsr8aMXFuIbnUBnYPDI6GBZ8XrEiTlkk2D+Hq67a97yhwdl6nM7rCXbvhiVLnM/vbZsApIkCYSIiIiIi0v/4/c4+4DY1QXU1AJ4xHtxZbow46TwGBgVZBXjG9LJ+QpK6DmYTOQ6mJirvA7vxe5LeVXkHjXR2L4fzOszvd54NFo/LBQsW4K2BqiWQ3xJ52N1ij7drkB/u+uvhgAP2PXeS0ZfKvJ5g/nxnu5kG9bZNANJEgTAREREREel/UsmECATCXBkuKifZmS3RwbDg84pJFbgyelE/IekYp78/UfOSBVOxILcVTluf4JrBHU2T7E7qWZ+4lBALcrbZ87pUqllHxcWxx71eWLYMb0s+tRWw8nFYWGU/xtwlMii4O+Qvfxk5npvrbD1O5/UEYT3QTAOqC2HRsfZjdN+5PTkjet8mAGmiQJiIiIiIiPQ/IzuWBeMt8lI1tYr8rPyIcXeWm6qpVXiLvOlYnfR0qTbKD0gUTA0MsikTDi8j/u6RbjdUVdmBoQRcTz9D5YpAE/44wbCmg2D5B88mfg2dlcp7LTMzfiAM7Ne8bh2u51+geHMm0/4LxbVxyiGHDIF77oHt29sHwQDy89uPxeJ0Xk8Q6IHmK4LCchh/GUwvtR8LyyN/p7ZcNKV3bQKQRgqEiYiIiIhI/5NKk+ioD4veIi+1ZbWsnLmShd6FrJy5krVlaxUE6088HjsgZcTJ7EpQuhgvmBquIQtKp9qBCwtoA948+lS4/3746KOkQTB274bf/Y6SNZCzI84cw+6FV77xif2zc6QT552XPDjjcsGECfDEE7G//4Zhf/3+9zB7dmQ5ZLjgzzCZzZvtvmXV1bBoEeYLf6N62b0s+s01VD9dgblnd/Jr7C9XXYXvaIPSqVCfFXko/HcK4IuJk/b/+noIw7JSKSDtGVpaWsjOzqa5uZmsrKzkJ4iIiIiIiAQFd4t0yjAcZeBIP+PzQWmp/e/wj9XB4EyS35nde3eT88s8WndvibmLpGHZva/WVkRlPLnddvP5eNf2+eB//xc2b6a60M4GSmblzJUUFxYnn9gRixbB9OnO5rrdUFvrPFPJ54Oyssh+bQUFdtmok/fr0qUwdWriOTk5MHgw1NfjK4KySVCfHbbk1gwqj56N97sxss72M7PNpPC2YdQbrQl/p17+XQ4b3qrh5MN6UdmnA05jRcoIExERERGR/sM07Q/OqSovj72bnfRfXq8d7IounXNYuri6fjWte2IHwQAsA+qywT826kBDgx2A8/nanxQIzplNdhBsWbzyyihdunNkKg3Z6+tTy9b0eu3A2cqVsHCh/bh2rfOgtZP+X01NoSBYzEyrIW2UrpuH77EbnK+7i/jX+6nPiB0Eg32/U1ddOAnT6L/hoAHdvQAREREREZH9xuluf+EsC+rq7HMT9S+S/sfrhZIS+3ejsdEO+ng8jjKaGl9a7ugWjZlRA5ZlZ52Vl9v3Dt4rEOT1HWW1y1pKxululh0SLEF0+r5Ltbm+y9Xx96XDe5mGnQlmQbsgk2XYmVbl795LyZ47cA2MU4q5HzgNaL46ZjRtva84MG36bwhQRERERET6n1Q/ZKfrXOm7goGYadPsRydlfT4feT+vcHT5vNYYg+HB2SC/H9/Q+phZS/Ga5RtAQVYBnjFduHugy2X3NnMqlQyyznJ4L//YQGAxUabV0Db8zzyYvrV1gNOApsv6Em1tXbyYHkyBMBERERER6T868yF7f35Al74rkLnlWQfuZjubKBbDgoJm8KxLcK2Ghn2X/bQhbtYSBu2CYcH7VnzzPlwZXbh7oM8H112XfF6CDQa6TLJNDwLaZeXFm/fvf6RhUR3nGePBneWOvSMp9k6lBxojGdR2jDLCRERERERE+gWPx25+nar9/QFd+q5Aea7LgsoV9lB0MCwUpFoR1Sg/2lVXhXqF+Q/alDBrKXrc3QJVi8F7djncfrvd1L66On298EzTvu7kye3KItu9pGAgqqLCeaP8dHC57I0HwtcQQ8ysvFjzcBgx6yKuDBeVkwKvJ+oHHgyOHT3kGgxcmAqEiYiIiIiI9APLl9vNr1O1vz+gS98VVmLrrYEfvdw+EJZh2ePemiTXamkJNc5vPMThDoCW/XXvXwPXb2iAuXPtnR3Hj4fCwtiN+FPh88HYsfZ1Y2gXcnK4wUCXiLfpQea+oJbj7L0zvt2FC3XGW+SlamoVQweOjBh3Z7mpmlpF/qBiACwFwkRERERERPq4ju4YmZNjNyUXSYewEltfEcz7BrRFRYZMwx73Odn10bKgvJy8zFHO7m/YgajZZ9v3aSfRrpROBHauDJZtmgZUF8KiY+3Hdve8//7UdnrsCoHdJ2/4YSXXXjCH//zhKbj22tBhR9l7L2fiGj9h3wHTtDPs0p1p54D3sPN59P0Z/PjlU7m1pYSV0//G2rK1eIu8ZAQy39QjTEREREREpK/ryI6RYGeQhTclF+mMQF8qMwNmnR8Yi9XTC/t4zGBVtLo6xv3x7+RuI25j/HCWAXXZdhP49gcDFygvTz14Eww2WxamAbefASPnwPjLYHqp/VhYHhXgO/jgnpFt6XLxQdFJPHP0mWz86mlw1lkRh701ULUE8lsiT3O32OPe8+bYryNYEjpypJ1hl85MOyduuAEOOojS39/HL55/lbn3Lae46BxcP7kRgIzA71N/Lo0c0N0LEBERERER2S+0Y6T0BIG+VNU/mkzTkATzDGgaYmdRTVib+JK+Iijbegebhqa2lMZMO9DmH2v/O6/VLgN0he9KWVzs/IKBYLOvyA7ixXp9DVlQOjUQPKqh52xCYZqc+PHbjPloHZmv7IBvX2hng4aVUntroGRNrO8X8J//2JsCPPqoXbIaLZhpl84S0GDWWXW1/fyDD2DJktjz5s0DwHXoRUD/Lo1UIExERERERPoH7RgpPYXXS/WGybBpWdKpyQJhviI7sGQ5SQWL8uFwO0OrPnvfmLvZLgP01pB6ALixEV8RTJ4af4pl2OWE5ZOg5AMD17hxKa877Xw+KCtjbjBj9FngVjdcdhncey/QPmA49d2ojQyWJflZhmfalZR0PgvO54NZs1LreXjffRgVk9iZ8R9eWreOAzOPxjPG07W7hvZACoSJiIiIiEj/ENwxMtVm+S4X9IQP69LHOKl5TMw0oGxSoBoy1uWs2OOGBcO3w63j21dSRmRspRgANkeNpGxS8Cbx54VKMwssilevTi3rLN2CPc2iM6QaGkJBMF+R/X2OGzCMI2a2XUcy7WKtefLk1E/7sskLG0r4fFAr896AeW/AiINGMP/c+Uw5ZkrH19PLqEeYiIiIiIj0Dx3dMdI0YfXq9K9H+i+fj+J7qhxNLU6QDeYfGwjOxAs6xQmCBQNnsQJoVuB5+XkuzG+kFgD2j0myniiNmXRv2XFYT7N2AmPBjLv6rMjDwYBhvA0NfEV2tl3M/miBAFuH1xzWyN+p4Ov4fEBrxPjm7ZuZWjWVG56/oeNr6mUUCBMRERERkb7PNO0yoo5SjzBJl0DwpbgWcrYTv7m9BTnboHhd/Es1ZqZ+e3cL3LYSmg4ibsDKMqAu08TfkFoAuHH7ZynNz2ule8uOk2ygkSjjLhQwnNR+Q4OkwbNPnoPduzu+5sCOnE4lzRwE5q2eR9W7zoKzvZ0CYSIiIiIi0vfdeWfHssGC1CNM0iUQfHFZsODZwFh0MCzwfMGfo/pQRclrjX8s3P0rYGEVrHwc1lbAEVucnde4NbUAcN6/PnQ2MRDk87S57ZLl7pIkwJ0s4y7W7puOgmdnW5gPP9QlazYNu6/comPtx2B5ppNMvav+chVmW4o7hfZC6hEmIiIiIiJ9W9iOaSkzDHB384d16VvCAhneGli2pH3/qRHbYP5fEvefArvnlLvZzjSy4pRBulvgmtciA2pOA2h5Q1MIAJsmnp8twF0ayIRyUh55332dbxrfGQkC3KYBLx7i7DKVX7fnBstYw3+W0ULBszo/xVyfwmIDEqw5Xi+z0vecXXrT9k341/spLixOfV29iDLCRERERESkb6uuhlaHn/zDGYFP8hUV3fthXfqWqECGt8bO2Mrdtm9scyZcf3b8/lNBLstu2A520Ctc8HnFivZZZcEAWvQ5oXMxKMgqwDMmhQCw34+rriG0noQMaBoC/m1JIn1dzeOB/Px2w8H+Xnec6ewyTx9tz514GVw0zdk5jbs/d7zMCAnWHK8cs+JU55dPNQuwN1IgTERERERE+rbq6o6d53ZDVRV4vWldjvRzHo/9uxXgK4KpU2HTQZHTkjVjD/LW2Ds85rdEjrtbAjs/xog1JQ+gWVRMqsCVkUIAOJDp5q2B8993eMo9c+0dELuLywVXXBExtORomBwjoBRTjEBi6wHObv3h+rfsbNVUuVzwta9FDDkpx8xoc3b5lLIAeykFwkRERERERMKdeiqsXAlr1yoIJunnctklgXS8GXvw3BcPgZvHw79Gwe+ehhcej+wFlqi0MmEA7S+ZeL9cktrrCmS6mQa86k4yN3jKVqC8vGMBoXQJu/ePJsK3pmD/LJKVdlpx5hiBYwl6u2HBI4dvxVxVndpaAZYutXfADeNk99C2jCTrsqDggNzUsgB7KfUIExERERGRvq24GO64w9ncwYPhH/9QKaR0rdxcICyAEUd4M/bi2n3jviKYdb5dXhhypr0L5YJnI+cm4q2BkjX29Rsz7d5hnnXgslrhxRfhm990/poCmW7+AfVsdrCbZW7gXlh19gYCxcXO79UFbpgI934DZ73NSDIv2TUM++fu/6Sa4vETHN4QO2h31VXthp3uHnr++/DnI2McsOwlVxx6VWpZgL2UMsJERERERKRvKy6GQYOczZ01S0Ew6XqBMsLlsYISsaaHBTp8RXbpXtNB7ec1DbaPJSunDOey7MDZtP/aj6F+YqWlqZUtulxQWek4KDPjP2H3SrITYpcqLmZ3Btw3bv/fenn9i6md4PfD5s3thj8c7uz02a/C0iWR/egACoJltG0OfyF7OWWEiYiIiIhI3zdoEOzalXzeRRd1+VJEyMvDV+S8iXlwl0fTgGsnBQYTlOWVTbIzvaKb5Kdk61Y7GOa0T55pwvDh5B19MvB60uklH4Q9SbATYpcrLmZ+8UGYGdv3+62f3PEa9+zZjWugw8ZiDQ3thkwDFpxE/FJN7GPulmC2H1wcMwuQ7v057EfKCBMRERERkb7N74eWluTzcnPt8i6RLmaeegplk5LPMywoaA6UEGIHLxoS9YKCfWV3Y9OwUMty1sOrqsoOoowfj2fp67ibSdyLKviaDAMKCrr3fedy8WFJ+u9vOGhOv2lwG/7n5ju7oM9n/yyiOP2duPK9A3EFJrXLAqQH/Bz2IwXCRERERESkb3NadjVjhsoiZb/wr/hN4ubmARZQsWJfZpfTssNU5yZUF+jhFc8NN8CUKbBpEwDLj4IdA4j52oxgL6rga7IsqKjo9vedcfjh6b2gBed86Gxq48aPk0/y+ezsvBhlkU5/zkecdr79DyPqBxN83gN+DvuLAmEiIiIiItK3fejwE2lJirvkiXSQo+AHdnPz8J0fgyWSTqQy1zSguhAWHWs/ttulMl4weelSmDcv9NRXBKVx+pcBDN8e6EWVYDfL7nBK/inpvaAB/7PW2dS8gw9LPME0oazMDhrGOt/hzznv0u/bmXv5+ZEH3G7n5a99hHqEiYiIiIhI32WaUFmZfJ7b3W/KgqT75R18GGxIPu/PR9rBpWDgyLMO8puhIQtH/aBCDAOys+GLL0JDpmGX1S3/Mjx5PGwK24HS3QyVK8ICVrF6R0XtYGgadm8yizhrs2DwXrt3WcS6ysvtIHQ3ZiMVZBek/Zq5rfb3sT7ezyoQ19p8yMGJL+T3Q3193MOedfZ9GrLsXUajGRa4jSw8hxTDYS77e+3328HNvDz7714/yQQLUkaYiIiIiIj0XdXVsGVL8nmXX97vPgxK9/GcdxXuVlf8PloBBlA+aV+GlsuCB1YEDsY6NzBWGVZOGSp9u+660DRfERSWw/jLoGJcZBAM7KBKaXD3yXi986J2MPSPJXG5Z6zeZZaVvPRyP/CM8ZA/ND/5xBTkX/p97vtr4Emsn1Xg+3T9/yvDbEvQgy1JabfLsn/eYAe9Im4ReF5R+jtcGYG/by6XvZPutGn2Yz/8u6dAmIiIiIiI9F3V1c7mJWsGLpJGroEHUHn09UnnWQbURQWPvDWwbAnkxNjkMGe7fSyi9HD4cLv07YgjgH3li/VZie8LgSDcww/GDpZEBWic9qqKOc9pH78u4spwMeukWY7nZ+5qH3QKMiwoGDAcT8Hp5G7HDnglCA7W7dqEf32CQKCD0m5vjV1ymh+1J4i7Barc1+M9pjTpNfoTlUaKiIiIiEjf9cIL3b0CkZi83/0l5Q/VUNH056Rzo4NH3hq7f9iDX4d/jIGhe+Dbb8NZtWGZYEFNTfZjXl7y8sUwoSDcyQdTHGtCVLmk415VsebFKr3cz44YfoTjuXNehlvH20Gv8HLEUAbWASW4Ruc7Dw5ujRMIjCrtDpazNmba30fPun0/b2+NXXba7vhLFzh+Xf2FAmEiIiIiItI3LV0Kr77qbG5xcZcuRSSWkvNnU/FE8kBYdPDIV2QHtOqz942tLIzq6xUU7MP10Uf4v5JFfXZU2lAScYM0Hg8MHgw7dthPnfSqiu5dBlBQ0CP68+UNdRaMy22Fm/xw7Kb2PwN3i70jpvemCeDxkDd4BNB+p8d29/54Ixxtts+8u/POUGl3rJ95dC83lwXFtVEX7+Zsu55IpZEiIiIiItL3RDXyTigrS4Ew6RaeMR7cWW6MOOlZhgUFZEUEj+KVNkb09QoX7MO1ejWNV1+W8hrz/pWgNC9sJ0NHvapWxMhYu+SSHtGnapx7HC4j+Toe+ov9Grw1UFsBKx+HhVX249qKQFAqPx9cLjw/no+7OUkZZTN4vNdBYSH4fPsOmiZUVAAd+JmH6wHZdj2NAmEiIiIiItL3RDXyTuib3+wRH8Sl/3FluKicZJe+RQfDDADDoCLve6HgUaLSxoi+XrHiao2N5E282PHaQkGamx+J3UPP74edOyOGEvaqiu5dFnTPPZEBoG6yun41ppW8V+DIISNC/w5mYE37r/3osojIcHNNnkLloBIgRjDMsn+OV7wZeN7QAKWl+74Xfj98/nnHf+aG0WOy7XoaBcJERERERKTvWb7c+dwvf7nr1iGShLfIS9XUKvKzInctdGcVUDW1Cu+XS0JjyXZmjNVcPyQvLywDLbGIDK719bF3dYxTcpcwUyrmoi27dLObN6yIWwIaPe/7l9pBJiPquxgcq6jYF1g3Tbx/fDNmcDDYRH/uWTC2HHxHBb7pwe9F4PvboZ95cG3ha5EQ9QgTERERERGbadofeBsb7XIaj6f7PkSZpr3jY3DXx+Ji+8vJekwTfvc75/fKyUl9fSJp5C3yUnJkCf71fhq3NpI31A5auTJc8GUT3G5oaKAxM06NXZR2TdoDmUHBDLTSJaUYgEXs64V6XQWDV7GCXglK7mL2qkqkrs7+29ONJcpOe4TlnVUCVR4oK4P6+n0H3G478OT17hvz+6G+Hi/QBnznYthxQPtrNmTB5KmwbImFtybwvRg5Eujgbpyx1iIhCoSJiIiIiAhUVcEPfhBZTpifDw88sP8/TPl8MGvWvt3uAO64ww5YLViQfD3V1bB1q/P7HXxwh5Ypkk6uDBfFhcUxDrjsnQMnT+74zoxhmUHBDLSyFWXUt+wL5OS2wox3oOT9yN0I7QvGCBJ5PHbAJTwY1Bnd3NQ9mC3X0NIQM0BoYODOcuMZ44FCF5SUJP8PB4HX5CuCKVMT3NyOSjLrfHvnR1djYygQlvLP/J577KwyZYLFlXJp5KpVq7jgggsYPXo0hmHw9NNPRxy3LItbbrmFvLw8Bg8ezMSJE/nww8jmelu2bGHGjBlkZWUxbNgwLr/8clpbHf50RUREREQkvW64AaZMad9Tq6EBJk/ev/17fD77nuFBsKCmJmfrCWaROZWfn3yOSHcqKYGsrNDOjEmbr4fvzHjbbe2Cx94iL7Vltayc/jcW+gxWPg6N98L9fw3rdRXkcsG4ce1vFgzQpUs3N3VP3K/Nfl4xqcLO0gP79RcXw7Rp8bNV8/IwDbh2UuhC8RnQNASqC+3z+OwzgNR/5lu3KgiWRMqBsG3btnH88cfz8MMPxzz+y1/+kgceeIBf//rXvPbaawwZMoSzzz6bnWFN9GbMmMG7777L888/z5///GdWrVrFrFmzOv4qRERERESkY5YuhXnzEk4x//dKqj9+kUXvLKK6thqzrYt6+ZgmXHtt8nllZenrJ5Sbq2bS0vP5/dDSkvrOjEOHwk03xbykK8NFccNApv3Hah/8CmeasHp17GNeLyxZQltGJ9uP95D3Yfx+bW67X1tRitmxHg/+r42gIUGPr2jVXxlqfy8CgcGUf+YPPdTt/dZ6upR/W8855xzuuOMOLr64/W4TlmVRUVHBz372M0pKSvjKV77C73//ez799NNQ5lhNTQ0rVqzgt7/9Laeccgqnn346Dz74IH/605/49NNPO/2CRERERETEIdO0yyET8BVB4be3MP6PE5num874J8ZTWFGIr6YLssT8fjsLLZn6OM27g1LpMzR/vrInpOcLKxv01sCSJTBie+SUmDszXn994t9vp+WIieZNmcILt1RiQZyOYw70oPdhKFtu5koWeheycuZK1patTT0IBuBy2c31UzEqz/5eBEtPSXE3zqamxH8fJb27Rq5du5YNGzYwceLE0Fh2djannHIKr7zyCgCvvPIKw4YN42tf+1pozsSJE8nIyOC1116Led1du3bR0tIS8SUiIiIiIp3k98cuQQzwFUHpVKjPihxvaKmndMnk9AfDUukRlGhucbGzBvg/+hGUljq/p0h3CSsb9BXBdZNg05B9h0e0wr1/jQqIZGbCzTc7vm5n5n14xiS+f9FPac6J6rdXUABz5rTfYTHcnDk97n0Y7Nc27bhpFBcW7yuH7IC8s0qSTwpT/MKHsHu3HQybNg2wA4wp7cbZzf3Werq0BsI2bNgAwMFRzSYPPvjg0LENGzYwMtD0LWjAgAEMHz48NCfa3XffTXZ2duiroKAgncsWEREREemfEnxYMg0omxTI8Ij6DGsZYFlQ7puV3jLJVHoE/e1v8ct/XC67qT6w17B77iw61n40g69l9uykJaEiPYbHA8OHxw1ONw2Bb02xg2QhTzyRPMsqmHUUL1BlGKEdJxPZY7bx1yPHMe/XK2DlSli40H5cuxZ++Ut7M45AdlNIbq6d2vbLXyZeYy/nGeMhf6iDPoQW5GyD4k8sO0PONOHRR4F9f4KDu3FO+2+MXm7hurnfWk+X1kBYV7nxxhtpbm4OfdXV1XX3kkREREREejfThN/8Ju5h/1ioT9TXxoC6vU34/3BH+tbk8cDo0c7mPv44FBbGb5zv9eJ7dA6FszMYfxlML4Xxl0Hh9Rn4fjvb3llNpLdwuTCvvSZhcBqgfBKYI0fAsmXOdnsNb3gfKxhmWRE7TsZkmuS99SoXvvd3Dl/zL/t9HN1A3uuF2trIIFljo71JRx/nynDxwDkP2E/iBa4C4wv+HAhuffwx3HlnwozduBwELvu7tAbCRo0aBcDGjRsjxjdu3Bg6NmrUKD4L7H4QtHfvXrZs2RKaE23QoEFkZWVFfImIiIiISAf5fJhjC6he9/f2mVIBDUOdXarh3lvTt6ukywX/+7/O59fXx91F0lfjo3T9PTRktkWMNwy1KK2/r2t6nIl0If+lnoTBacuAumzw/2OhsyBYkNdrZ2wNH97+WLISY58PCgv51o9n8sCz8/juzd+LH6B2sstiH1VyZAmZB2QmnJOzHUrWBJ4cckjcjFUzXpZrULLApaQ3EHbIIYcwatQoXnzxxdBYS0sLr732GqeddhoAp512Gl988QVvvvlmaM5LL71EW1sbp5xySjqXIyIiIiIi0Xw+fD+bTOHUxshMqfLIsqrw/kOJvHgo6d3F8YgjUj9n1qyI+5ttJmUryrBipF8Ex8pXlHfd7pciXaBx+2fJJwGNOzd37AZbtrR/x2zZYvfvihXY8vnsY/X1keMNDfHP6afu9N9J6+7WhBm2TUPsTFwyMuCYY6C1td00X5H9tzru3+7bbkstCNpPpRwIa21t5e233+btt98G7Ab5b7/9NuvXr8cwDMrLy7njjjt45plneOedd/jOd77D6NGjueiiiwAoKipi0qRJXHnllfzzn//k5Zdf5uqrr+aSSy5htNM0aBERERERSZm5Zze3P3Ipk2M1wM+yew8FP1Dltv8MFtPyI8FsSLKLYyo60tumqckuIwrwr/dT31Ifd7qFRV1LHf712llNeo+8oc7eG07nhZimHcy2rFCcJpR1dIxF9VgL87qoYHfYOe0Ex8rL0xcg78XMNpOKVysczV1aBNVj2jB/+0i7Y3E3Lwn+7T49B266KQ0r7vtSDoS98cYbnHjiiZx44okAXH/99Zx44onccsstANxwww1cc801zJo1i5NPPpnW1lZWrFjBgQceGLrGk08+yVFHHcWECRM499xzOf3001kQaGYpIiIiIiLp56vxMfauEcw9dYedlZCox5AB+Q4DYVsOCmQxpGuXsqg2K4lElAgtuxdzz24AGl9a7uj8xq3aWU16D88YD+4sN0actCIDg4KsAjxjUuwP5fdHZHXFzDqaXI/Pd2fcc9qxLKirS1+AvBfzr/fz+c7PHc2df0rg+z1mWUSGbrLNSwDKz4lRJikxDUj1hOLiYqxYUd8AwzC4/fbbuf322+POGT58OAsXLkz11iIiIiIi0gG+Gh+lSybbiRoJPiiFegyNBc86GL7dDnQl05hJenYpM024+mpHU31F9gfD+uzgSAvue9xUXjifvMpHwUF1UMqZMyLdyJXhonJSJaVLSjEwIkp/g8GxikkVuDJS7A8VFsQOZh1Ff+JvyILS9+ZSVXMs3iKv88B3ugLkvVhHAu4NQ+2fQ9US8NaEbV4Sh2VA3Z4m/Ov9FBcWd3yx/USv2DVSREREREQ6xmwzKXtqVtIgWLjGTHvnsrJXnc3PGzwiPbuU3XknbE7e3yhuidDuTZRWTWHT3hbczWDE+e/3hgUFB+Smnjkj0s28RV6qplaRn5UfMe7OclM1tcoOUqVq5EjAYdZRsLee08B3OgLkvdzIISNTPic6Q7cxcZ/9EGW5OpNyRpiIiIiIiPQe/rXV1O9pchwEA8gLlEXe5IcHToWmwcQ+PxBo2vz9mZ3fpczng7lzk05L+GEdO8g1+2y4fwVMnWo/t8LmBYNjFTvOTD1zRqQH8BZ5KTmyBP96P41bG8kbmodnjKfTv89Js44g1Fuv2OMBt9tujB+rYsww7OPpCJD3U+EZunkOS9WV5eqMMsJERERERPqwxjerU5qf22qXRZqG/QFsxtuBA7GyqwK9xq5vXtK5HRiDjbcdCH1YjxPYC354HLHDLivKb4k87m4JlBtZR3V8vSLdzJXhoriwmGnHTaO4sLhzQbDP7N0oU8o6crmgstIeMKLejMHnFRWdD5D3AZ9tc7bbZzwvHgLj1pM4y7Wj/eH6KWWEiYiIiIj0YSNffw8cfsAFmPEfWH5UdP+txEJZIh3tTZOs8XYYxx/WM2Haf6FkjR08a8y0syo86+yyT4o7uFaRviZQvphy1pHXC1VVdhA7/P3rdttBMG8HyjT7oM5mad1xJvz6JJj5b7hvXIws1870h+unFAgTEREREemrTBOefwEudn7Kl3bGbpadTKd606TQUNvxh/XAPJcFxbVRB3NyFAgTCRo3DlwuPOtMcrbHL4U2MHBnuSOzjrxeKCnh7hsX0LhmLd7zvkbxFZOVCRYmuNtnQ0tDxAYHqdicCfeOswP7b46O/I8U7iw3FZMqOtYfrp9SIExEREREpK/y+/nMbEk+D8CyywYfOSl2/61kOpX1kEJDbc86u0SoISsyKyLICLwOz7oEF1mwQB/URYJWrwbTZHlRIAgWiwWWYbXPOjJN8PsZ+vkm/p35Jb446VS9t6Ik2u0zVcuPgj8thYO3Q6Pv9+R9qSAt/eH6G/UIExERERHpqxoaHGdQAVz5ZuL+W7GkpTeNxwNZWcnnYWd4Va4I3Dv686RlB/GueNN+ahpQXQiLjrUfTQM7G6ykpONrFelrGhsxDZh1QeB5nPd/pnEgJUeGvXd8PigshPHjufq3c/nTop9y9vmn2uMSId5unykJ9GS85lzwfOkEpp347c73h+unFAgTEREREemrNm0KZVDFa7IM4GqDpVUGR2xJ7fLBz8ud7k3jcsH//I/j6d6a2I3wgx8U554FB/8IDp4D4y+D6aX2Y2E5+EY22T3JRMSWl0f1WGg6iPhBcANarZ1U11bbz30+KC1t19vvwM822OMKhrXjLfJSW1bLypkr+ePoqxm6s2PX2ZQJ/unfSO/i+hkFwkRERERE+qoXXkiaQYUFi6qgdO5i8swDU7q8O8tN1dSq9PSmKSpKabq3Bmor4LaXCL2OcE0HtS/zasiy+5/53n+6EwsV6WM8HqqPH+po6q/f+PW+XV6t9tF1IzhWXm7PkwjB3T5nXPkgP8roeDCrccIpaVxV/6NAmIiIiIhIX7R7Nzz3HBA/g6qgBZYtgSnfug1yc/Gs2Zk4e8yC3Fb4/TL4/bAfs7asNn0NmjvYvP6RkwL/iM5kMdqPBXuKlW98ArNNH9JFADsj85vfdDT1b5/8DXNVdeJdXi0L6uqUeZnETT//O0ONQR06N+9LBWleTf+iQJiIiIiISF909tkRT4MZVCsfh4VV9uPaCvB+mg033QSNjQmzxwzLjivd97cD+cdhP2XsObPT25tmi12XmaiNdHTPr+qxqfc0swyos77Av14f0kWCiif9wNG8ll0t+D+pdnbRFHaD7Y9cGS6+edT5KZ+XMzincz0ZRbtGioiIiIj0Grt3w/z58PHHcNhhcNVVcMABsedVV7cbdllQXBs1eOSRdkZIYOfGYPZY2aRAkCnA3QIVK+DzIy9nkLmHg1b74bCS9OwQZ5pw/fVA/JiWr6j9moZv7/gtG7fqQ7pIUHFhMZkDM2ndk3x3jcb6NaF/mwb4x0JjJuS12ru1uoLR7BR2g+2vikakVhIOcO0p16pBficpECYiIiIi0hvMmQP33hvZl2f2bPvrl7+MnPvgg86v++Uv248eD7jdUF+PtwZK1sT5gFvzsD3/WeBmN1RWgreT5ZF+f8JSK1+R3dsrOltsy+CY0x3JG6oP6SJBrgwXc74xh7nVc5POzXu8CogdnHY321ml3hpg8+YuWm3fUVxYzB3+OxzPzxmcw02em7pwRf2DSiNFRERERHq6iy6Ce+5p35y6rQ3mzYMbbogcX7LE+bW//W370eWC++4LDQezx6b91350xapZbGhIzw5xCUqoTMP+sG1B7D5gMRrlJ2JYUJBVoNIikSg3eW4iZ3BO3OMGBgWtLjzroKoIJk+F+qzIOaENKYqwszzVMD+h4sLihN/zaAsuWKBssDRQIExEREREpCdbvBiWL08859577XJIsD94vvOOs2sPGAATJux7npub2trStUNcghIqf7I+YMGm+HF2xIyYatlzKyZV6MOkSBRXhosFFyzAiPFmMwJvsornTHxFcMkUEm9IMQnMejXMTyb4PU+mIKuAZVOXpW9zkn5OgTARERERkZ7KNOHSS5PPa2uDhx6y/+33w44dzq5/442RPb460tw6HTvEeTyQEzsrojHT2SWGR73knO2QEzXm3mpQVbpUHyZF4vAWeamaWkX+UHfEuDvLTVVeOQBTp4CZIJJgGVCXbQex1TA/OW+Rlznj5iScc+8379XfrTRSjzARERERkZ5qxgzYu9fZXL8fysrgxRedzR84EOZG9QNKobl1uybZnzbQ4Ryr5cuhqSnmobzkvbsBWLLULt8M72kGUWu8ZzGuY0o7ukqRfsFb5OXsQ8/nsNvuxTQ+5/GZ32TS4ePh5z+ncJLz6zRmoob5DphtJov+uyjhnNl/m423yKtM1jRRIExEREREpKcxTTugtXix83M+/xwKCxM2nY/ws5+13/Ex2DC/oaF9P7IwMZtkr7uOyprBqWctmKYdwIvDs85uwN2Qta/sKpxh2TtaxutjVlwLjB5tbyDQ2ab+Iv3EAQMGcGDbVwD4hvsMXBZU//lh6s93fo0PD8my/6ZIQv71fupbEv/drmupw7/eT3Fh8f5ZVB+n0kgRERERkZ7E57MDWmefndp5f/+78yBYTg7cFGPnMZfL3gUSwIjdlCu4g2O7Jtm7N1O6pBRfTYqN85PsGOmy7F3oINDjK0zwecWKOM38AW67DdavVxBMJAUuw8DCZGfGf1jy7p+ofvZBGnalsAukBY+cOhAzXm8/CWloaUjrPElOgTARERERkZ7C57N3YXQa0Oqo732vfTZYkNcLVVWQn9/uUKIdHK1AZ/ryFeWYbSk0zm9I/uHOWwNVSyC/JXLc3WKPe2tinFRQAMuWwS23xH+tIhLT0+8/RcOgy9k46Kf8719mMv7f11GeSmzegPo9TfjXq1l+Mhu3bXQ07/lPnu/ilfQfKo0UEREREekJgiWCCUoS02bRIrj77sTBsJISO1ursdHu87N5M3f+/nLqs1tin4MdDEu5hGfTJkfTvDVQsiaq59e6GJlgV18NkyfbJVkKgImkzFfjo3RJKVZUCubmIezbidVhplfjVjXLT6Zpe+z+iNGWvLuE3134O/UJSwMFwkREREREeoJAiWC7JvSxgj3hBgxw3lA/qL7evl9xcfw5LlfEcV+Nj7knxQ+ChUvpw2+c3SLbufBCXM88Y/f8SmTy5MSvS0TiMttMylaU2Rme0cEug32BMAtHwbC8oWqWn0yG4axQb8feHVTXVjPh0AldvKK+T6WRIiIiIiI9QWMjviIoLIfxl8H0UvuxsNzuyxVXqkGwsPs5ZbaZlP2/sn0fgpNI6cNvYLdI04DqQlh0rP3YrrfQmWfajfzj9C7DMOxySDXnFumwpI3bjbCvBAwLCrLceMbo/ZhMKg3wX1r7UtctpB9RRpiIiIiISA/gG/AhpVPbx5oasuzm9HF7YXVUnvNglX+9n/qt9ckzQCwoODA3tQ+/OTmxd6Fstpvkh17zwQfbjfxLS+2gV3gJaTA4VlGhckiRTkgpmzP8j1XY34bQJhbuK1XG50BxYTEHZBzA7rbdSefWtdTthxX1fcoIExERERHpZmabSVndgthN6APPyyfFyJLqqNzclDKnUtmtrGL4DOcffn0+fA/9MPYulIEAYCgbLj8/fiN/t9se186QIp2SUjZn4O9RdOl2aBOLvUekb2F9mCvDxflfPt/R3IKsgi5eTf+gjDARERERkW5mZ1w1xM24sgyoy7Z7hxXXknofsWgzZqSUObVpu7OG9gB86UvO5vl8mFMm2/sDQMwAoGHZAcCSbW5cwcBdrEb+aowvkhaeMR6GDx7Olh1bnJ1g2H+P7l8BB7dG/T1KIeu0v7vq5KvwrfElnXfWIWfth9X0fQqEiYiIiIh0M6flSI2ZxCwjHNEKl74DJe87DIqVlKS0vtyDch3PLf/gAUrabmqfFWaa+4JXI0dCWRn+MZGvI1ooAHj7FRSHB7qiGvmLSHq4MlyUDf0f5u5YnNJ5B7fCtP+GDaSYddrfFRcWkzM4h6Yd8XeQzBmck1I/MYlPpZEiIiIiIt3MaTnSh8OJWUa4ORMqTnPYXL8DDeXzs/KTTwIwoG5vE/611ZHjPh8UFsL48TB9OkycCPX1NGY6u2zj4A5uCCAiqTFNbrr7H+Rsx/HmGGBngkWYP19ZmilwZbhYcMGChHMWXLBAPdfSRIEwEREREZFu5skfh7s1I9RkOpph2c3jHzkpdhlhuHa9tSIuZHSoobxnjAd3ltvx/MY3q/c98fnsBvf19k504btDbnQYCMvb6nytItIJfj+uugYWPBv4M5MkGGZYUNBsZ6KGzJljv+clJd4iL8umLiN/aOR/eHAPdbNs6jK8ReqBmC4qjRQRERER6azwsr8O9KxyvbyayufaKJ1qf7C0YuzAduWbMNdBe5iI3lprwsokc3JgwYIONZR3ZbionFTJ5CWTHc0PBa5ME7sJmL2IWGWdrrbAJgAxgnuGZTfe9pxRnPKaRaQDGu0ybW+N3fA++v0aLrQ75IrA35nsbHjkEZgyZf+stQ/yFnkpObIE/3o/jVsbyRuah2eMR5lgaaaMMBERERGRzqiqsoNfwbK/8ePtMkBf8sbHIY2NoQ+e+S2Rh4I7sB3hsHc1RDbXB+Cyy2Djxk7tqugt8rKEKbja4s8JZYccWmwP+P2hTDBfUeyyztBOmFGZJ6EP2a/n4FIgTGT/CGtw762B2gpY+TiUr4bcbZFTQ7tD1gQGfvhDBcHSwJXhoriwmGnHTaO4sFhBsC6gjDARERER6R86mbUV0w03wLx57cfr6+3SoKqqxMGnHTvsMqLXXgPsD5Qla2LvCFldmPryGjOBESPgt7/t/Gs1TaY8shojC6ZMDYyFZ3FZdizr3peH4JpXHFiAnV1iGnZmScyyTsM+NzoTzt1iZ5p471igXkMi+4vHA/n50NAA2H97imvtr3ueT7JbbYbybKR30G+qiIiIiPR90c3aO5K1FW3p0thBsCDLgvJyOwAXy0UXwUEHwcMPwxtvhIaDHzyn/dd+DH7Q9KyDzJ2pLXFkKzBjRnoCSX4/NDRQWgPLltiBqgiB8sbrv7EN3+9vtMcC2SX+sYHyqni9zQywAp9Mhm+H216CtQ9m4L19aaey2EQkRS4XzJoV+1Ccv00hZ5zR1asTSQsFwkRERESkb/P5YPJkqK+PaNRePaAes3QyLF6c+jVNE664Ivm8ujo7gBTtootg+fKUbun6+il8tTGlU+zAU2FhiifF0bjv5t4auH8FdopX1IfhhiwoXTcP37tVdnbJiBGOd4cE+Hww3Doels8vU8Ntke5wxBFA5MYW1YVhZczxKCNMegmVRoqIiIhI72eaUF1tfwEUF9tfEMpuiNWo3d0Mlbddgvef/4R773V+vzvvhJbolKjAUoyo8qH6OiLysXbsSC0INngw/PGPsGsXpz8ynVWHOD/1syFAbq7zExIZOTL0T9OA6yYFnkR9OA4163/2KkqKLsY1diz/v707j4+qPPs//pmMJARCAgnbyASC1CVVa60r6GgQClhtQycBRG21j4+0ghL21gURxWqJYqKVWvz5uLQCJmGUaitWgehU0arVqhVRKVvisAXJwhY4Ob8/ziRkkplkkkw28n2/XmOcc+5zzn2SQ0guruu6HSV7w75MzfG+p0mvylZ/HJG25nCE/n65plZPsLp2726T6Ym0lEK2IiIiItK5eTwwYACMHg2LFlmv0aOtbffdByUlIRu1F8db2z2vLrGytMJhGLB4cfCppELKDBh5I1ybaX0c+OkN5P/fnOOD5s5t2v0dOgQbNoDDwRVbmnaoowKr30+ENVbqaNpgx5E9eJ+7Dz78ENc265doW91SqhBMG+ww9+P98/0Rm7OIhMfTb2/D3y9TQxxYq9G+SEemQJiIiIiIdD7VGWAzZ1pljyUl9ceUlMDChQ02aq9uzj5jHBh/WQ15eY1fu7AQDhyot9mTChlBfnnc28Nk4vaHmff4eGvDV181fo26Hn4YLrqINGMQSQepV45YT/XqjVVOqzwxEmple4Rb6uhbagUM7aaVSQLhB8MAfA8taFkfNxFpEqPKIOvvM63vjQ19v6y9z2aD5OTIfa8RaWUKhImIiIhI51K78X1OTqPDw8peSrDG8ctfhm5uX626/LIWwwZTfux/E+I62XtWU/BpXk3/nSYxTZg2DXvOoyx7uXpbqLHWFHLWgP2R3MituFgr28NREeYhew7V/L97IxTkwaDgFaXBjy+n4QUHRCSivNu9FJUVhdwf8P0SrCAYWN+LtbqrdBIKhImIiIhI5+HxWA3Ui0L/olZX2NlLccD+/cGb29e2sX6DnMIhUNKDBldFxAZTX/4lxu8eDG9CdeXnQ3o67kWrWLU2qf6qjX7JZVCwNgn3olWRXXHR5QKnE2y2RksdbUBymTWuNvdG2JoDbzxrrQ4ZKphnq85o20boBQdEJOJ85eGtyFHzfdXphIICre4qnYqa5YuIiIhI52AYkJVlZUc1QdjZS9XjfKF/ETSOVuLd+Cq+s/yN8LdZZX+FYTaw33N0P95VS0gLb3igigorIOR2405PJ/2tQrz/LaQ4roo9g/rQr/hbBlVE4bosDXt2WuSzM+x2yM2FzEzs2MhdY5I50QpambUCgNXBsZxXTexBvlR2E0ZtgSdftvoNEer4NRw/voGviYhEjqNXeH2+HOMy4LFbrQC5MsGkk1EgTEREREQ6B6+3SZlg1aqzl4rjAwMu1WwmOMs4nr0UouGzZ6OHrJd+RdHEgzXbqldRawrfU7lNOyDgYH9AyG7HPnIUaSNHNf9czeF2W9kfWVm4NxZRkBdkZbkyK4gVcmW56lP5SyXDOl5NuEXahGuwC2e8k+KyYswgKZs2wBnvxDX/BdCKrtJJKRAmIiIiIh2fYcBjjzXr0OpG7Q1mL1VnH8XGBm347NnoITMvE9M0A8ofq1dRu3t9+PNxbN/XrPuwDu4AASG3G9LTwevlp998w8pXtuCNBsP2LXlrn8X1wd6gmWBBT7UR0r+w+g354gKz7Gr07q0m3CJtxB5lZ/JZk8l+JzvECBs543KxKwgmnZgCYSIiIiJiMQxYuxb+9CerDO/SS+G22yA6un3n5fHAzTfDvsAAkmFrJIBSS9jZR4cOwYsvWn3Iqq9TZZD14pR6QTCwgmo2E546DxIPwL6G+oTVyTxryvwB6Nev4wSE7HZIS8NmGNj/8iC3v/Upg0p3k/b53qafyoS0rQ0MuOEGlV6JtBHPRg8PvfNQyP1zRszBnap+YNK52UyziU0WOoCysjISEhIoLS0lPj6+8QNEREREpGEejxVwqKjTUCsqCmbPhsWL229eGRn1N6cGCWr5yxQbKskLK/jUq5cVDEtLA7udwufuZeSWBY1OdeE6WDDS/6ZuMMx/jVV51vyaNf/8/IAAXbvzeGDKFCgpad3rrF9vfS1EpFUZVQYpuSkNrhqZHJ/MlqwtygiTDincWJFWjRQRERHp6qqDTXWDYABVVZCdDbNmtf28DMPKBKvDk2qVIxbV+Rm3ukzRkxr6lNXZR5M/sz4GzcAqL4fRoyElBfLzw+7pdeo+K9CVeLD+vqSDgUGwJs9/7tyOFwTLyGj9IFhycsfJghM5wXm3exsMggHsKNuBd7tWcZXOLeKBMMMwmD9/PkOHDiU2NpZhw4Zx3333UTvxzDRN7r77bhwOB7GxsYwePZqvvvoq0lMRERERkcYYhlX+2JhHHoGZM1t/PrXdf3/Qcsiscf4EqyBligAzxlnjwmXYoDAFVpxlfaw5tqgIY9JEdlWG19PLUWEFunY/BG88C3e9ab3eeAZ2PWTta/L8+/WDvLz2y8gLxjBg+vTWv47NBjk5KosUaSO+8vBWZw13nEhHFfEeYb/73e/4wx/+wLPPPsuZZ57JBx98wC9+8QsSEhKY7v8Lc/HixTz66KM8++yzDB06lPnz5zN27Fg+//xzunfvHukpiYiIiEgoXi988014Y3NyYMsWeOml1pyRxTCs69XhHRJYTliXaYMdCda4BvtO+TVUogj19wVTd9VJuwmjtlivugrDnf9d15N2xU1WNlRHCwR5vVBc3LrXSE62vv5u9SISaSuOXuEtxhHuOJGOKuKBsHfeeYf09HSuuuoqAFJSUlixYgX//Oc/ASsbLCcnh7vuuov09HQAnnvuOQYMGMBLL73ENddcE+kpiYiIiEgovib+y/7q1VaG0sSJrTOfal4vfPttvc2+uPAOD2dcdYli3erI4njICHV7JgGZXPVWnayjdk+yrxIh9+IwJg/4BvXuuH2xmvrMhKtvX7j+emtFyo4YABQ5wbkGu3DGOykuK8as950RbNhwxjtxDVa5snRuES+NHDFiBGvXruXLL78E4N///jf/+Mc/uPLKKwHYsmULO3fuZPTo0TXHJCQkcNFFF7Fhw4ag5zxy5AhlZWUBLxERERGJAMfxf9kPWSJY1y9/aWVstaYQwRZHkDZmzRkXTokiQfbVfe8ss1ajDNbgflUqpMyAkTfCtZmw4ArYFxvO7MExYFh4A9uDI4LZIH37wowZVkP8nTutElz/IgUi0rbsUXZyx1k9EW11vtlVv88Zl6NG+dLpRTwj7De/+Q1lZWWcccYZ2O12DMPg/vvv57rrrgNg586dAAwYMCDguAEDBtTsq+uBBx5g4cKFkZ6qiIiISNdjGBhvFeL9eh2+b3fg2FmBywarz2jCKob791sZW62ZsdS///Ep18qq6n/AmldxfJ2AlV/dMsVQGiuxrBcAC+KRNXDbe0EywWJi8JxyhAlBss0aO6/NBGdFFK6rpjY+gfbicsGgQS0vj7znHrjrLgW9RDoQd6qbgokFZK3JCmic74x3kjMuB3eqypWl84t4ICwvL4/nn3+e5cuXc+aZZ/Lxxx8zY8YMTj75ZG644YZmnfP2229nVq2VisrKykhOTo7UlEVERES6Bo+HgkduZurwfezp6d+WAElzoKRH/eHVqxgGzXhqrfK4al5rVbJgPbySDvgrFM3AYFhjZYq1PRxmiWJDBlSEKIesPBIy26xB1fOP+hH2btEtn2Brsdvh0UetVSOba+5cWLAgcnMSkYhxp7pJPz0d73YvvnIfjl4OXINdygSTE0bEA2Fz587lN7/5TU2vr7PPPptt27bxwAMPcMMNNzBw4EAAdu3ahaNWWvWuXbv4/ve/H/ScMTExxMTERHqqIiIiIl2Hx8O8P2SQPYp6wZmaIFiQEkGbaa1imP5FnaBPJMvj6jIMjAfu5/7LYMHI+rv3+eebeCgwgOcss4JgwcoUa6uMgr+d1vJpBi2/7NULb1J5ow32g0k6BMteBvfS2S2eW6tzu2HVKpgyBUpKmnbs/Plw772tMy8RiQh7lJ20lLT2noZIq4h4j7CDBw8SFRV4WrvdTlVVFQBDhw5l4MCBrF27tmZ/WVkZ7733HsOHD4/0dERERETEMMh/5GayLwmx30bIzKXaqzAeH2+DESMiPMnjPAuvYci0Yyy4IvjcTJu1KfYovPEMLC+A9c/AlpzGg2AASy+EqnB+Cg6VVWZCcmmI8ssRI8Ju6F/XC/ngLndapYedgdsNu3bBG2/w+xGTeOziCVT16hXy0wZAYqIywUREpF1FPCPsxz/+Mffffz+DBw/mzDPP5KOPPmLJkiX8z//8DwA2m40ZM2awaNEiTj31VIYOHcr8+fM5+eSTGT9+fKSnIyIiItLlGW8VMvXifU0r06sjILhjmlbp4qhRLZ5bXZ7/FJAZVYAZ3/A402aVS9qByZ814QJ9+7J5QClwtPGxwT5f/ijPklDll8OG4djXF9gb9pSq+5qlbQVu/GHn6pllt8OoUTy2vpIjx6q4dupPSfz5tXUX1zzupps61/2JiMgJJ+IZYY899hiZmZlMnTqV1NRU5syZwy9/+Uvuu+++mjHz5s3jtttuY8qUKVxwwQVUVFSwZs0aunfvHunpiIiIiHR53q/WsreZWUrV6pUBFha27IRBGFUGWfk3Nam3lm9IIowbF97gbt2guJhh465t7hRrMtT6Hgqx/9RT8U25zgqYNdKnDIL0NevZs8HxHdVJUdYX7MCP0qmc1UBp50MPgcfTRrMSERGpL+KBsF69epGTk8O2bds4dOgQmzdvZtGiRURHH2/4abPZuPfee9m5cyeHDx/mjTfe4LTTItCoQUREREQCFRTgW/Zwsw+3mdYqjQaw4iwoTLFWcWwN3t9MpshW1qTMNcdTeTB2bHiDr78eoqOZ+r/LsJu2sAJVoQQtf7TbMX71S3619+kGy01rc5bVWYxg2LDmT6odRfkDYceOHqXbCysxsZ6TwpQgz82MGWAY7TNRERHp8iJeGikiIiIiHcS8eZCdjSMlzPF16tlsprXpUDcYfePx7c5SyD3LjjtiEwXGj2f14dUQZstYmwnOeCeuoWkw1QVz5jQcXImKgieeACD6pGhmXTKH7Lez691z6Jq+QEEb5c+ahXfne5QdKQvrHsZ/DgX5dUosf/nLsI7taKozwqLfeZuo4uKgq306SyF3jYl74w6rtDYtrX0mKyIiXVrEM8JEREREpAPIz4fsbAD29AB7VQNj/WV8iXXK/RIPWh9LYgO3F8dD5hcL8WyMUInbCy9g/GU1/3dumOOrywmvzMUeZYfoaJg1q+FjZs+2xvkt/uFi5l4yF7st8MfhKBPijhwvWazLFqxRvt0Oc+fC4sX4yn1h3gRcvj1In7H33gv7+I7E7l8sK3bNX/GkQuZEKKrT56043truSQV84X+eREREIkmBMBEREZHOyDCsPl0rVlgfa2dDGQbccgtgBR0mTWiknNFfxhd7FBaus1ZhfONZiD1Wa38tpg0wYcaLUzCqWljiZhgwZQqFQ6AszHaxzjIoOHMh7tRaOWmLF1vBqLqN2GsFqepa/MPFHLzzEI+Mfohbu13KI2vg0CJ49iVrf91gWEA/r/gEqzfZI4/AwYM153f0coR1D1FVMPWfQXZ00gDRSVE2xm56h4T/t5SscQTt82b6388YB8bA/m09RREREUCBMBEREZHOx+OBlBQYORKuvdb6mJJyvAm51wslJRg2QgYlgvkmHu4ZCTGGlalUlBD6ONMGO46W4N1S2LJ78XqhrIwnLghv+GVbYOvzfXG776y/c/FiKyj1yCNw6631glTBRJ8UzYxLZvPYrwuZUeQkusrq11WQB4PqVDg6y6DgwFW4l66HkhJ49VWr31WtTDPXYBfObkmh+4/5s+9mvQPRwbL0HOEF0jqablSxYO0yvEPCeG4SwOtsKEVRRESk9ahHmIiIiEhn4vFAZiaYdSItxcXW9oICOHIE4HhQIkymzcp6mjEOHngjvGN8HxbCsFHhX6TeCXx4UqHgu+ENv2w72K+9vn7mV7XoaCs41VR2O0yaBA9bCwu4N8LVm2DphbC5Dwz71srgih70CTzoCnl9uwm5r0LmKP+XKEhAKP0LyA72+XU6weVq+tw7gHO3fcbJ5Xt5c0h4430fvQWn/rB1JyUiIhKEMsJEREREOgvDgKys+kEwsLaZphUE6tsXCLGyYSOqM3b29AxvvKO8kQENlXBilchljQt/fmlbgfT08A8Il2HAs8/WvPWkwrAsmDkOfn+R9XFYFnji/I3eQ/F6cf+jhII8K4OstoRDsLIAXnohxLG5uaEDfB1c/4p9QIhFBIJo9LkRERFpJcoIExEREeksvF4oKmp4zI4dcP/9QPhBiWD6VVir/BXHH+/tVJvNtAI9rsvSoLISli6FzZth2DCYOtXKzPJ4rMBd7Tk7nVbAx2319/IODjNrzYSkg5B2bFDrZE15vbB3L0BNs/e64cbqZu8FX67GHWrFQ3+PL/dGK/PLO8QKSDoqrAb79ZrjA8TGwp//XPM56Yy+TUgCrHsM+7kRERFpBwqEiYiIiHQW4TZSf/NNwApK9DoM5WE2oa9tUAXkrrECPzazTlDDtIJED7+fiN32Nxg9Bqpq9XyaMweuvhr+8peGSzjdbnxvvhL2nJa9AvacR1sna8r/uW2or1pN6ei+50mveshasbKuWj2+7KY/g60hvXpZAbhafcY6o43fOYdvevXFUb435HNTs9jA+0nYs9PaZZ4iIiIqjRQRERHpLEI0UjdsUJgCK86yPlavEGn3N2VvCpsJyaVWEC1U0/jqVSZnXVKO59UlgUEwsMoMV68OXcIJVglnZSWOJ/4c1rwWvheLe9Gq1sua8n9uw2r2fmQP3u0hyiNdLivrzRbG6gQAzzzT6YNgALaTTmLhqClgs/HThhYbyAP3zGWdtgRUREQ6P2WEiYiIiHQWLpfV/8tfwgdWGV/WuMDywr4VcP2nkL4J7vgHPHwJVETT6MqRNRk7a8A+Zy4A7ocfpooqJkysHnR8fHH3o1apYJ4VNAubaVolnL/8Ja4P9uK8NHQpHSY4y+HOF0uge2wTLtJELhcMGoQvrjis4b7yENl5drtV+pmZaQXDggUDAZKTISenU5dD1ma3VbE6NY5TLrmZc5cXkP7xvvqloVVO7I/knjD3LCIinZPNNEP97dxxlZWVkZCQQGlpKfHx8e09HREREZG2M3OmFUChTi+rEEGuQaVw7aeQfYl/Q50Sx9rvk0sh55143LP/H0yYAIBx+BApix0UVZUGvUZ1z6ctOSH6X4Wh+j4geCldQR64l66HUH25IuXeeyl8egEjb2x86Pob1pOW0sB8gvVH69cPrrvOavbvCr3yZGfj2ejhuoJfcbhqT8222KPxTP52LE+dOsK670GDTqh7FhGRjifcWJEywkREREQ6k/R0yMlpsJdVbcXxVhBs7tuw/GworpU5NuiAjSkpEzi132k4yq0G5vbstIBghXfnexSZwYNgcHyVSe+QMPphhVBdglk3s81ZZmWnuTcSfn+0ljj11PCavXdLwjW4kYb9brf1tfJ6rbk7HCdkIMiz0UNmXiZmnaUFDp1Uxv/1z+eqsdfgTlUGmIiIdBwKhImIiIh0AkaVgXdLIb6d63Cc1g2j8mh4qy3aABP+71z45mF4Z7C/VO2n1+P67VPYuzXcnypkCWDdcXFhDQup0VUWQ/RHiyiHA7sZepGAmtLR06YHb5Rfl93e+lls7cioMshak1UvCAbUBE5nrJlB+unp4X2+RERE2oACYSIiIiIdnGejh6wXp1B0tMTacC0kHmzCCWxQ0tMKMo3a4t/22E3QSBAMwPHhprAu4agIvr1O9WWDQq6y6HRa2VStzd/o3v1FMQV5ZvAMtfeTcM+/s/Xn0gl4t3spKitqcMyOsh14t3sbLiMVERFpQwqEiYiIiHRgVulZhtVzvVZEaV8z+sYXpvgDYXFx4QWWDAPXvN/jvB6K4mmwR5hrW51DbQ1kdzXVzTe3TUlhrUb37i9spH9hBt7DdrDna8XDar51q8MbF2ZWoYiISFtQIExERESkgzKqDKb85eZ6QTCgpuSx5v+bIiMjvGCO18vqviUcOinENUxrChmfW0GvEdut0svVp8Hz58CenseHJlXAre/D6SXNCIydemqYAyPA7YaCAsjKwl5UdDxDLTkZ8nO04mE1w8DxxJ/hqsaHOnq1QVmriIhImBQIExEREemg7p9/OSXR+0IHuqq3h1l/mFZdFnnttWFd3/Pl6uOrUoa4vGmDnOHWy14FRlTwsSVxsHDk8ffOUqsXl3tjGBNpi/5gtXWRRvct4vXi+mAvzksbWVige7/GFxYQERFpQyF+VBERERGR9mTMm0PusbfDGtvnUCMDTEg6AGnV5YslJY1fv8ogq+T50KtSmvUDZEYTMtOK462G9J7URga2VX+wuqob3U+ebH1UECyQz1ezsAAcX0igWs3CAonXqVG+iIh0KAqEiYiIiHQ0lZV48x9mX4/whufnw8J1WJGputEp//tlrzRtBUbvdi9FlXsazkYLVq4ZJtNmTe1XV0FlQz+R5uYqCNUR+Z8h90YoyINBZYG7+xyCe9ZD+qlXt8PkREREQlMgTERERKSjGTcOX1x4Q5MOWist3v0WrMqzGtfX5iyztteUICYnh5Vh1SYNzm2wJw6cs0Nkhs2YoZ5cHZV/hU1sNtwbYWuOFYytXs10Xw9YcAWkfHQjno2edp2qiIhIbQqEiYiIiHQklZWwfj2OivCGT3/3eKZXdUBi/TOwvMD6uDWnVhDMZoOcnLAyrNqywfmeHiHKJNPT22wO0kTVK2wC2GysPgPuGVl/NdPi8mIy8zIVDBMRkQ5DgTARERGRjmTpUsBaVdFZWr/3Ug1/3687vYGb7dhI2wqTP7MyxWrKIZOSrNUQw8ywcg120bdH3+bcQdP5SypnjKvVZywpqX16g0n4/CtsGs6TyRpH0H5ypr82d8aaGRhVRptPUUREpC4FwkREREQ6ks2bARpsRB607xdYgYlBgwLHJibCwoWwa1eTygztUXaW/mhp0+beAqYNdiSAd4h/w623qjdYZ+B2413/LEUJhOwRZ2Kyo2wH3u3e4ANERETakAJhIiIiIh3JZ58BVmZU4iHI2gB9DwYOSS6DvHxr/4qzoDDFn0nl88HWrbB+PSxfbn3cvRvuvrtZQaUJZ05g7oi5wXcGy1QLlb3WBDW90ZQN1mn4Du4Ob1xb9J0TERFpxEntPQERERERAQwDrr0WCgvxpELWOKwsG7++FXD9p5C+CfbGwsw6+52lkPvau7gNA9LSIjatxT9czIUnX8jU/BvYY6sVkQuW/dOEVSNDqemNtju84Iq0v3D7ybVl3zkREZFQlBEmIiIi0t48HujXDyM/j3svg4yJUBQfOKSkJ+ReDH/7DkwMsr84HjInmHhyfxXx6WWemYnv0hd5ZE3ET13DZkJyqdUbDQCHgiadhevDPSQdJGRGoA1Ijk/GNVhZfiIi0v4UCBMRERFpTx4PZGTgGfgtQ2bAgiuwIgd1m47brDjDkhEhmpJXN5z/dkWrNCW37ylhQJgrWTaqTsCkugdazhqr2T/JySqN7CwMg9U5t1ASG2K/CaYJOWOWYI9SzzcREWl/CoSJiIiItBfDgOnT8aRC5kQrq6tBNjCiCN2U3AY7Yg7j3VIY4YkCDsfxssXmMqHHEau3WW3OMijIA/cX/hvLyVGj/E7CeKuQrAtKrDchnsukg5C+u0/bTUpERKQBCoSJiIiItBevF+ObYrLGBc/yai7fh4WROVFtLheuYyfjLA2yimUTHIyBfT2s/088CAvXwZYccG8EnE4oKGjS6pbSvrz/LWxwxUhsVlmv97+FbTgrERGR0BQIExEREWkqw4DCQlixwvpohChFDDWuevuqVXiH0HAgoRkc5ZE7Vw27HXvuY+T6+4S1JBhW7dtYuGckrD4DeOQR2LJFQbBOxtcrsuNERERamwJhIiIiIk3h8UBKCowcaa3yOHKk9d7jCW/cvHnHt//+9/jimnBtE+xVoYNQNQ3nT0lrxo2Fwe3GvWgVBa/GMaisGceH6mv2oyiMaVNVDtkJOc5Li+g4ERGR1qZAmIiIiEi4PB7IzISiosDtxcXW9upgWKhxRUWQnR2wPey+W6YVR5r1sdWVvG4wrKbh/PtJ2C9LC/OkzeB2435nP1t/9BrrjetZboznjTMX4+w1qFlJbaYNdvSqwrviwYhPVVqfa2gaSfa4kCtGYkKSPQ7X0LS2m5SIiEgDFAgTERERCYdhQFaWtQReXdXbZsyAysrQ4+qe0ma9Eg8SOpDg5yyDgrVJLP7ZcxTkUS8jq6bh/MxlrZ9ZZbdjHz2GtHv/xOR7X2RU5lxyr3wUsNUPhoVZQun7Q3boElPp2KJjGintjWDdr4iISAud1N4TEBEREemwDAO8XvD5YNeu+hletZkm7NgBS5c2PM7PkwpZ4/z9wUKe0/qwcD3c+Q+w5y+zMrKiVpE+YzpeezG+OCurzFXlxP5Ibrv12HKnuimYWEDWmiyKyo7ff7/oBPYcLW30eMfOCrj/frj77tacpkSYd7uXkkMloQfYoMQox/vn+0n7ub62IiLS/mymGcY/V3YwZWVlJCQkUFpaSnx8Y+uMi4iIiDSNcfgQ3plufB+sx7H3CK5tYA/3J6Zbb4Xf/77+OW3gHQK+OPgq0WoS39hKkUkHYNkr4N6VCE8+GRjkqh2kczjA5eoQPbaMKgPvdi++ch+OXg5GOEcw7HcOio/uq+kJVpvNtLLZtuSAPTHJCjh2gPuQ8Mx8bSY57+Y0Om75+kQmr92tr62IiLSacGNFyggTERERqcVz+3iyjqymaCBwtbXNWQq5a8C9MYwTpKQEvDVscL8Lci+GfT1q7TBpuGLMhNhjkP4F8NCd9TO97HZISwtjQm3LHmUnLSUtYFvuaVlk/mcBNpOAYFhNX7M1/kBjSYkV3OuA9yX1GVUGf/7kz2GNdWzbp6+tiIh0COoRJiIiIuLnuX08mTGrKarzj4jF8ZA50SpnDMawQWEKrDgbCiu/xPAHezypMGAOLLiiThAMGm+bZLPKJr1DgAEDmn4zHYg7/TcN9zWrHWD0+dp0btJ83u1e9h7c2+i4fhXg2oa+tiIi0iEoI0xEREQEqxwy68hqzBjqBalMm5W9NGOclaFVu0yyXq+vymU4Z8DkTyH7kpbPyxcHDBrU8hO1p3fewb3R+txVl4c6/MGReiWnDke7TFGazlceXmDruk/8X2d9bUVEpANQIExEREQE8E4eQdH3Q+83bbDDn6GVttXa5km1MsXqxnKK4msFwVq4YJ7D6G71/+rM/JlAdvP45y6opKTOf69diKNXeIGt9C+B5GR9bUVEpENQaaSIiIhIZSW+rz8Oa6gvzvpo2GDKj0M0vLfVejWTzYTkUnBdPKnzNxgPNxNo+vTOf69diGuwC2e8E1uoB736Gd4O5OToaysiIh2CAmEiIiIiS5fiqAhvaPW4+11Q0oMWZ3yFYgI5r9mwP7GsdS7QllwucDrB1tASmUlw551tNydpMXuUndxxuQD1gmE20/qjkfN+Evb8VfUXexAREWknCoSJiJygjCqDws1rWZE3n8Kn5mOsXwuG0d7TEumYvvoK1zZrdUhb3TpHv5oMrW1WNljuxa07pRkbwH3VHIiObt0LtQW7HXKtgEnIYNiyZcoY6oTcqW4KJhYwKD6wj13Po734w+A7cBfuUhBMREQ6FPUIExHpbAzDWoLe57PKjVyuer88ejZ6yHpxCkVHS2q29f1iEUvvTmTCzCf1S4l0WcbRSrx/XYpv51c4Kmy4Bl6E3ZkMVVUA3PwhLBiJlY5VK15THRzLWWP1uSpMCbIKZISlnzsJHlzcuhdpS243FBRAVhYUFR3fnpxslc3p+1Kn5U51k356Ot7tXj4s+i/Zf9tFTNWZXHHVSAU3RUSkw7GZphni3z07rrKyMhISEigtLSU+Pr7xA0REThQej9VDp7j4+Lb4eCuTYtIka8hGD5l5GZh1fpEHwIRJn8HzE1di7z+gwWCayInG839zyfp8CUW9qmq2OUshdw0QE0PWyCPHV36sI7nUCoK5N1rvV5wF12aGcdHqn7KaUD5pM8EZ72TLjK3Yo07AP5dhBPOl8/rvzlLumLWU/hXf8uv/GcmgH4/R11dERNpEuLEiBcJERDoLjwcyMoLuMmzgHTGI4ivOZ2rUq5RR2eAv3okH4cmXj/9Sj9NplS0pI0NOUJ7bx5MZs7peY3ubWWfFx9p/bvw7Fq6HO71WJli1tSkw+sbGrzvxU3j1VCjvHuZETbDZbBRMLMCdqj+P0sl4PBy7bTonfVPrH2v094uIiLSRcGNFrdIjrLi4mOuvv56kpCRiY2M5++yz+eCDD2r2m6bJ3XffjcPhIDY2ltGjR/PVV1+1xlRERE4MhgFTpgRuslnlWTPHgGMOjPxhMdfbV1NmazgIBrAvFjImgifVv6G4GDIzrWCbyAnGyHuBrCP1g2AAZvX7YCs8+t8/elGQk4aZ4TXlX7DqhXBnCs7oJAXBpHPyeCAzE3vtIBjo7xcREelwIh4I+/bbb7nkkkvo1q0br776Kp9//jkPP/wwffr0qRmzePFiHn30UZ544gnee+89evbsydixYzl8+HCkpyMicmIoLISS4/2+PKmQMgNG3gg5I2BPzyaez/9L/JSrrYAa1cnBM2a0vKF+ZaXV7+e226yPlZUtO59ISxgG3t9OsUoeQwWvGgpq2aCkJxQOCdy8My68y++Mgyu2QtJB6qSe1WJar3v6TWTrb3YpCCadj2FYvd9Ms/4fp0j+/SIiIhIBEQ+E/e53vyM5OZmnn36aCy+8kKFDhzJmzBiGDRsGWNlgOTk53HXXXaSnp/O9732P5557jm+++YaXXnop0tMRETkxFBbW/K8nFTInQlFLK8Orf8FP8b83Tdixw+rd01zz5kGPHjBzJvz+99bHHj2s7SLtwevFZ5S1+DSFQwPfhxt83tPTKqlc9rJ/Q7BgmA3mjpjDgqkvnJg9weTE5/UGLoBQVyT+fhEREYmQiAfC/vKXv3D++eczYcIE+vfvz7nnnsuTTz5Zs3/Lli3s3LmT0aNH12xLSEjgoosuYsOGDUHPeeTIEcrKygJeIiJdkWGDrHEELfFqrmfOsRp/F6b4s8N8vuadaN48yM6u/y/+hmFtVzBM2oPPh6Oi5aepwvozUv1nJelAeMf181/bvRFW5YGzzo8wfXv0JS8zj8Vjsls+SZH2Eu7fG839+0VERCSCTor0Cf/73//yhz/8gVmzZnHHHXfw/vvvM336dKKjo7nhhhvYuXMnAAMGDAg4bsCAATX76nrggQdYuHBhpKcqItJ5XHYZAN4hhFzVrrn+/H3rBf4V9Da9hJvJTTtJZSUsWdLwmCVLYNEiiI5uzjRFmsfhwLXNeraL4ml2AHnZ+fDby4+/7xtmcG1QrXHujZD+hfXn2Pe7u3BcOArXYJeywKTzczgiO05ERKQVRTwjrKqqih/84Af89re/5dxzz2XKlCncfPPNPPHEE80+5+23305paWnNa8eOHRGcsYhIJ/D22wD4wuxL1FxF8ZBJHp7/FFgbDMMqy1yxwvoYqr/L0qWN934xDGucSFtyubCfPIjcNS04hwl7ewRu2tuTmt5eoY5JLgXXtsDNdmykGclMzriHtJQ0BcHkxOByWatD2kJEmm02SE62xomIiLSziAfCHA4H3/3udwO2paamsn37dgAGDhwIwK5duwLG7Nq1q2ZfXTExMcTHxwe8RES6DMOAxx4DiEiJV4Ns1u/1M16eirEqH1JSYORIuPZa62NKSuDKX9WBsldfDe/8mzdHfs4iDbHbrecXiG/OmjzVga5gK0oGW2nSf4wNyFlj9QerJyfHmpfIicJuh9xc6//rBsOq3+u5FxGRDiLigbBLLrmETZs2BWz78ssvGTLEWm5p6NChDBw4kLVr19bsLysr47333mP48OGRno6ISKdnvFVIYfw+Vpxl9fByloItVBZKJNhgx5E9eOdMxCguYu1QmD/Seq3tVoQxIQM8Hoz8Fyg8L4kVt42k8Mu/W/3FGuNfOEWkzRgGnjf/QOZEKOvejONDBbtq6dejX8D75AN2CvKsUsjAHclQUABurQopJyC323q+Bw0K3O506rkXEZEOxWaaZkR/nXr//fcZMWIECxcuZOLEifzzn//k5ptvZtmyZVx33XWAtbLkgw8+yLPPPsvQoUOZP38+n3zyCZ9//jnduzf+U2pZWRkJCQmUlpYqO0xETlyGgcdzP1kf/ZaimCM1m5MOQEkP63dzM0IN84PJegf+fI61smRtSQfhfz7rxorTjwb0K3OWQu6aIL/813bwIMTGtsp8RYIxFt5DSunCZvUHizkKR7o1Pu7PP/0zg+IH4Sv34ejlwDVoBPa334HiYtizB/r1s4IDLpcyYuTEZxjW6pA+n9UTTM+9iIi0kXBjRREPhAG88sor3H777Xz11VcMHTqUWbNmcfPNN9fsN02TBQsWsGzZMvbv38+ll17K0qVLOe2008I6vwJhInLC83jwPDKFzFEl9VaItJlWtVbSISsgVsMkYitJ1pyvzrUDttfd5y8HC5oJUy0zE/LzIzVDkYYZBmt/0JvR7ubVFN/4L3jmB42PW3/DetJS0pp1DRERERGJjHYNhLU2BcJE5ITm8WBMyCAlK/QqdzYTBpXBMy/C7jjYlAj3pUFVJAveGwqshdpnQr8D8MhrMKjcahRer0fST34Cq1dHcKLSobVjdohn4TXcfPAF9vVofGwA08p8/Oa5JIbdBsXH9mEG6Ypvw4Yz3smWrC1qei8iIiLSzsKNFUW8R5iIiLSAYUBWFt7BWGWHIQJRps3abwcmfwZp2yMcBCP0tRvcZ4M9cXB9Boy8EVJmgCe1zpi//AXmzo3IFKWD8i+iYMzMYu15icy/dyTzn7yWtb8YidEz1uoVtHZt4yuNtoDn6Xlkmi+wr6mVuP4g77ILFhJdvItc9zLACnrVVv0+Z1yOgmAiIiIinYgCYSIiHYnXC0VF+OLCG149Ltzxba0oHjImwr2XwYqzoDDFavjPww9DZWX7Tq56xcsVK6yPrRiU6VI8HkhJwTN1JAO6Pcron5ax6HJYdDmMvhEGZB3F88WLMHo0DBgQuApphBgHKsj6NLteWXFQdRK9khOSWTVxFe4Jd4PdjjvVTcHEAgbFBzYAd8Y7KZhYgDtVDcBFREREOpOT2nsCIiJSi88HgCPMlkbV4/o3oQXStH/C4xc2cV7NZQNMWHDF8U1WU30T95gxcM897dNI2eOBrCwoKqo1MSfk5mpls6aoW/a4dy9MnIjnDJOMicEPKYm1gqOr8sC9sQQyMmDVqsh93ufN4/73sim6ovGh1UGwhX0yOHVkhtXofrCrXoaXO9VN+unpeLd7jzfEDzJORERERDo+9QgTEelI1q6F0aMxbFZZYXF88JUhbSY4y2BLjtWD6+9XpTL2goaWa7T0OwBFD8PJs6CkvbLI/H/r3PAxLHsZomN7wnnnwaWXwhVXQFpa6wbGPB6raX/dv/5s/k90QYGCYeHwBxON4iK8Q6ysREcFjNgOp2RZz25DPeacZbA1x99DzumErVtb/nWfNw/PK9lWEC6MhSOSDlrPoHvpeuu5ExEREZFOSz3CREQ6MbsJkz+tV7Vl8W/MWeMPInTrhtdeFGxkPZdvhegq+NmnEZpoc9is17PnQuxdMG/4AXjrLfjtb61yuT59Wm9lScOAKVPqB8Hg+LYZMzp3mWRblHz6g4meXkWkzLD6wV2baX3sOw+KG+hvB9a+ogTwDvG/LyqyMstaorISY8lDZI0L/5AX8sG9rYeVlSgiIiIiXYICYSIiHcnu3YDVYP6hS0IPm/M2uKsTwIYPh/LysE5/xpU/g+XLSf/VI82fYwTziKtskH0JzBtda2N5OUycCPPmRe5C1e6/H0pKQu83Tdixo+VBmdYWKtjl8cCQITByJFx7rfVxyJDI9uHyL+jgOcMkc6J/ZdNayruHf6qA3nb+suBmW7oUb7LZ4CITNUxILoW0rcCcOW1fmisiIiIi7UaBMBGRjsThwLBB1jgabPS98mx/03mASy8lbUt4p0+7/AaYPBnXT27j5LiTmzY3k4gGwYCa+1syAirr/o2UnW2VKTZFQ9lQhmH1AAvH6tVNu25b8jejN64YSeEd17LitpEUnhWHccVIq99WcXHg+OJia3ukgmFeL0ZxUaPPaDgCeuFt2tSyeW3e3KRFI3LWgD2mO9x9d8uuKyIiIiKdigJhIiKtrSmlai4X3vP7NpzVYoMd1WVlSUlwxRWkbbP6HYUMVJmQdFI8aSlpANij7Pzy/F827T5sdT5Gig2MKFgarIH/TTeFXdpn5L9A4XmJVmDojmutwFBiIrzwgjXA64V9+4Ifa7NWtKxZ2TI3p1VWM2wxf0liQa8iHHNqlSRec5iU7xfiSW3g2ClTIlMm6fPhHUJ4mVcN6HcAXNtqbVi6tGXzGzYs7EUmFq73Z1Q+/7yywURERES6GAXCRERakz97J6BULSUldJBl9Wp89sNhndoXByxbBmlp2Ac5Wfayf0fdYJj//bLxTwWscjesz7Am3IhfpINgtWzuE2RjWZkVPGyE5/bxpGy4hpE/LavpVZUyAzyDyuCaa2D8eKvkMdixqdTrc5UyAzy/u6Fj9QrzlyTOG2UyYSLs6Rm4uygeMicSOhhWUhLW57JRDkeTMq9Cue7f/h531fbsaXpJau0g8759uLZZq5LaGggIO0vhzve6RXalShERERHpNBQIExFpLdWrExbVaWRfVBS8VM0//qvo8NJaHHMXWr/I2+2Qm4v7Cxur8qzV+GpzlsGqIXNxn5kZsH3PwT1NvaNWNezbEDuWLm3wOM9Tc8iMWV2vV1VRPGRMhF/8BCpfXg0PP1z/2FRrTNBjr6zAU3BfE+6glXm95McXkR2qd5zNinlmjbMy3OpludmAN95o+TxcLhzd+7b4NOlfBtlYt6yzIXWDzPfdh92E3DXW7rrBMJtpxXFzX7djL6tQEExERESki7KZZrClszq2cJfEFBFpN4YBAwY03Ji9WzdrpcTp061g1oABePqXkDHRvz9E9pUNcMY72ZK1NSDDC48HsrIwiovwDrEyxhyx/XD9+nHsGRPqnef5T57n+hevb/Yths2k4Uwy08oMOrjI+lgz9wqrdM5+Ujc4dChoCZtxtJKU22Mpiqtq8Br2Kpj1Djyw9vj5+x+ASROgJDbE/ExIIpZdd5cHfp7bibH8eRI/vZ6yMJrRT/oU3h7sL1/0c5bCknXd6Df9N/j2bsWxsxxXsgv7tFshOrppc1l4D0NKF1IcT9OzBE1ILoMtOXUywgAeecRatbMx/qCxgYl3COyIg/cGW2sdnPotnFwGs8cG3n9yqdUXzL1ImWAiIiIiJ6JwY0UKhImIRJphwOjR4ZehRUXB8OEY77xNygx/dlIDwQUbNgomFuBODfLLvGFY5WU+Hzgc4HKF7IFUuLWQkc+ODG+OLbBgPSxM87+pe1/+v4Hmvg0XF1vZTLWDF4kHIetduNMxAfvKvHrnLnwph5H/ntn4JPzXiauEipimzf+Nn73BqFNGNe2gVnDv1DNZMODz8AZX/81ua3ibs9TKoHL/aDY89FD4k1mxgnufuJYFV4R/CBzP0irIq7XqaW1//jNcd13DJzEMSEnB06uo3vNSzV4FM96Bq7+uE1SdngU5OU2btIiIiIh0CuHGilQaKSISSR4PJCRgvFnI2qEwf6T1WptSa5XHuqqq4O23w25Afs9ldwcPgoEV9EpLg8mTrY8NNAJ3DXbRt0fLS9xCMq0snPlvwao8iDtSf0iUeTwIlhmkRHFfD1hwBcQPy+eF/1c/4OV78bnw5uL/nFY0LfEJsAKG7c2YM4vs3mEGwarVfY6CPFfV5aMTtj/M2usvwagKsyeaw8GpwdcdaJCzrIEgGMCgQY2fxOvF06so6PNSzbDBw5fA374Dkz+DtK3+7LOUlKZPWkREREROKCe19wRERE4YeXkwaRKeVJhyNZTUama+6HJrVcdlL0P6F1Z5XnEc7IqzSvOigKowL3PqrqMRma49ys7SHy1lYsHExssXm6g68ydnjRWAcG+07nvtUPjT96yA1KXb4bZ/WvtTZvgTlkLM4WA0XFOUw4oV/+WlyautjZWVOLwfwynhTqp59/KXd59lPi6iLx/VPisM5udTuOoRKm5swjENrDga7H3BmVDAO/S5L57/N/FPoQOt1VwuHLF9gb2Nz8WEGe9C+qbjq0QWptTJ1DKB5GQrg7ERxjfFTPlxw88LNuu6S0bAonUQXf2HKymp8fmKiIiIyAlNgTARkZYyDCsDKz+/pvl6MCWx1r6kQ1DSo/mXc3y+vfkH1zHhzAnMfeNqsr99JfSgZgTJnGX+fky1Mn/sJoz5r/WqrTAleHlbMKu//AtXL7+aOSPm4HrxX+yKNYmqgqpWzG/+5OgOYt8cy8zf9+Sh659r2/5ShgFTp/JE61ewAvCteZCMvAwWXLaA+ZfPr9cbzagy8G734iv30X/aNJz/tvqEmSGej6gqeCEfMjcC3bvjOeNwvXLGmvLMRTlhBRrvr3wjvD8//gUDll5oBeKAhnv2iYiIiEiXoNJIEZGW8Higd2/Iz8ewwfRx/u3BAgP+bSWxDZzP5Hgvp7qH+0sNXebgZk83mMXnzCY/D+IP198Xfwiy3ml4Xphgq4K/PwvLC2D9M7BllRP3wjxwOsHWcBTNFxfmRP2n+etXf2XksyPpvXc210xo3SBYtaooePh7BxifF2S1z9bk9eLpt5eC77bR9fyf44VvLaTf4n54Nh6/V8+neaQsSmLksyO51nMto7cu5FBinBUnrfts+J+XlQWQ+YXNypT84E9kTgqxQuckawVPwAr+FRbCihXWR+N4uaZRZZCzc3WTbumrPrXe9OvXpGNFRERE5MSjjDARkeYqKIAJx1dj9A6B4sYymxrLrPKXdNXNwgooNXysiR3KG+NykVnu5KeLiygcYmVoAaRtAdd2GDSrkXnbrIygbkseYfKhAYFN+u12yMy0gmHB1mYZORLHlvXNmnZFt2Yd1iKrz4C83JuZmJ7eJmWSxjfFZI1rfFxr+PbIt2TkZbBq4ip47jkyY1bXK0fcd6wCbNDtWA8qTzpYsz05qjc5pcNx3zwGpk7FOMlOVm5K8HJG//ufe37OqO8c4M5Vv+KrmIOcug+yX4PYHr1g1iyYPx/vDi/fHvm2SfcREIcNpweZiIiIiJzQFAgTEWmO/Hy45hrAKr/yDoH81EaOCVeQoFNNqeHuJKsJfiTZ7ZCbiz0zk1FbYdSW4wGrwhTYG2bGlm/YADh7cuBGt9sKGGZlQVHR8e3JydbqfenpuFIG4yz9ptHVMuuJYE+zplzvl8P3kfFWIfaRkVtJsqbksLQYx5Y9uA72w963H973XqAoMWKXaZZrV06g75EqzBjqfc5NmxWkjT1axVMp92I//xQcCYNwDXYFlFV6txZSVFZEQw4cO0DvjT+HM633fwcevwCGflvO9NcWMvWRbHyPTGny/C+qriR2OsPqQSYiIiIiJzYFwkREmsrjgYlWIzBPKvV6HkWMCVnvwvhNtRqKr1rWOplIIQJWYZctAo5ejtDnTk8Hrxd8vsCMMcCe+xi5d2WE7K3W0eyPBe9/C0mLUCDMs9FD1pqsgEBRnwpILofyZqxyaa/yr1AaoUDhkaiqBjMdTRuUxh7Gcf/djCpNhCefhJTAZ9RX7gvvYkGyxbYkwsxxMHvMQTL/lgNnN2n6JFf4/yc3t30WOxARERGRDkWBMBGRpjAMmD4dsIJgmRNDt85qMRssPxse/jvYY7rD88+3bqP2ugGrV1/F4f1TWIf269EP1+AGsm3s9tCZbG43biOPvHsmMjGTtu1e2czVMovjwl3js2GejR4y8zIxTTNgHt/GWa9w3fixjTFfmzgqYG8sTGiHoOLECfDky/twZ2TAqlUBz2rIIGkTVEVB3lkQcxSOnETjXzfTyqR07bBBfl7bLnIgIiIiIh2WmuWLiDSF1wvFxRg2KxMsaM+jCNoTZ5Vd8sQTbfOLfHXAavJkGDIE1zZrVb+GGuVjwtIfLa23wmCTTJjAhIX5rCxo4Fqt4Bwf9DrU9OP2OFter2hUGWS9OKVeEKxJTIjCxh9XHmDyjQ+RttVaoTEv31rAoC3ti7UCw55UYMqUgCb3rsEuekX3ish1wg2CgbUapX3FC1afOhERERERFAgTEWmal14CrOBUUQJNC2CYNLz6Ygi+OGDIkKYdFAlpadhNK5hgg5DznuucSOaZEQg0ZGYy6b5VTNrcvWXnacLn+MZP4Km3E48fF6Y3/vb7eisaNsaoMlj737XMXzefO9feybTHrqToaEnLAqk2qMLkHd97cPLJNZsnfG4Fw5rzvLVkLiYwYxwY+0qsz4+fPcrOmGFjInKNcD5fSQdh1dok3ItWBSxoISIiIiKiQJiISLjmzbP6DNG03lk1bDC37Cz6HmjaYY7Yfu3T5DstDZKScG+EgjyrzKy2fhWQ92pPFv9ieeSu6Xbz/P+VEkvLejn9+AsaDgCZVi+tqe/DhJlPMnfE3CYFpP7abStpT4/kzqu6M/+aAay9/hKMh7KhsjLo+IL/FNDnd30Y/afRLPIu4rf/+C1/3P96k+6pIb5yf++1WjI3wqo86H40xEGtESCzwY4EfxZjrUAYwC3n39IKF/QzodtRuP0teOMZ2HXwFtyFu1QOKSIiIiL1KBAmIlLboUMwdSpccAGMGQOvvWZl/eTnQ3Z2zTBHRQPnCCI5PplVmfksfvYbcl4L8yATkkvB9evH26fJt90Oy5YB4N4IW3Ng/TOwvMD66HsYJvz6uYjPzd4tmjmuO5p1bL8KKxPqL5n5zKk8P3hGlP/9rI+6E51n9bJa/MPF5Gfmc5ItzNaZNnjzFPjt8GMsSt3N6FPfYcCeeXjO7W4FTA0D1q6FO+9k3owzmZA/gfLK8mbdUzgcvfwLEDidYDse0XNvhIoHYH4hxNaJ0SV279Nq8wkWKE5LSaN3dO/WuaANjnaDUf+FUdts2JfkqDG+iIiIiASlQJiISLXx46FHD/jDH+CDD+D112HcOOjeHW66CcMGhSmw4ixrVT5nKdga6J3VrwL+XJjI+uvfYEvWFtx7+sK+fQxqQjwk5+y52DPasbTL7bYanw8ahN2EtK0w+TNIO+bEXrCq1TJuUvulhj/YhMSDViaQ72GrLJA+fcj+7fvM7fsTa7XNWuwmzLVdwuIXKwLmn3lmJofvOkxmzwublS1VEgsZE0w8r2RjxMZQ+L+jyfrgt2T3/rzpJ2uCuOg4a6ECu70mY7F2MMxuwr1v2ih/ENYPXchy93LW37Ce3fP2WJlwrcBRQb3FEexRdp4a/1SrXK9a4VCsUsjoZiy3KSIiIiJdglaNFBEBKwi2enXwfceO4XGWkzXO3xfML+mAf9FBE8xaZXXVwbEn/gruRU/CsFHWBp8PgD09rLI8o4F/irBjY2XGStxnt8Pyf3XVXU3S4c8+asWMm7BXGfT3mX/yZRi1tdb2wkIYNYrFt65m0ZFDLM2fy+a9XzGs76lMnZBNdExs0NPZo+ysPO+39Fg7mspuTZy0zZrPDeOh+zGDvT2beHwzTfjuhOMLFbjdUFAAWVlQVHR8kNOJPSeHtDqBy8U/XEzPk3pyz1v3RGw+8YfBVZEYdJVQd6qbhUNuZMG2ZyJ2vQB2OyyPYKmuiIiIiJxwbKZptuH6XJFRVlZGQkICpaWlxMfHt/d0RKSzO3TIygQLojIKfvVjePr7/g11Al4mkHQISmodnlwKOX+Pwn1PndXqCgvxTB1J5sQGVpv0f0fOm5DHhDO7bpNvo8ogJTeForKiBsclHYBlr1glgAHuuAPuv79Z1y7cvJaRfx7drGPbw5E7jxB9Up0MKMMIO3BpVBnEPxDPwWMHIzKfBevgnqzQ2YLG0UpS7oilqGdVxFdcfYOfM2rBs5E9qYiIiIh0CuHGipQRJiInnspKWLoUNm+GYcOsnl8NlUp997tBN88bDQ+PgKoQmVumzQqGxR61yvJ2x1klYa5tYH9hZWAQDDAuvoiscQ0EwbDK2Fa6l5PZhYNgYGVm5Y7LJTMvEzNYnaIJkz6D5z3UK30EICmp2df2Hdzd7GPb2qQzJ9UPgoEV9AqSkRWMPcrOd5K+wye7PmnxfGIrYf70vAZLZu3dosn97mwyt2VDnWzKZjOtlSLTvh+BlSlFRERE5ISmHmEicmKZNw969MCYNZO1f/098/8yk/lXxvD3/7mctf/vTlbkzadw81qMAxVw660Y/ftSyFZWnGX1/zL8v5TPGw3Zl0BVI7+kmzarXNKOv3fWVrDfvcDqU1SHd80frdLKBs5pREHfLbuaefMnFneqm4KJBTjjnQHbqxvir1wVIggGMGBAs68bdllmO4uLjuN59/MROZdrcAtXJfUvSvCc7adh9bRz/2IxBUPmMuhAYJZa4kEro6xvUxaj8D8Dy14B+8mDmnCgiIiIiHRFyggTkRPHvHmQnY0nFaZcDSUBPZreguK3oBjYuAhnKUzeBCuuD+z75SyFh/5uZYIBYZdu1aySl5gI8+cHH7Nrc3jnCnNcV+BOdZN+ejre7V585T4cX36Da8Kc0AGwaoOaHxBxDXYxqNcgisuLm32OSLhwB/yzOgYY5Dl8dvyzx3uDtVD26Gwef//xxgdWf96DzGfu25D5m2lhX9P9i8WkH12E969L8b39Go5Xvbg+P4DdhDP3wsQJwa9TV9wReHY1uHfEWSWgIiIiIiINUEaYiJwYKithyRI8qZAxMbBnVzBF8VbGV1Gd0vHieLgm018O2YSSLUd1BsuTT4bsxeQYMCy8c4U5rquwR9lJS0lj8tmTSRs/A/sgZ8MHJCe3KCBij7Lz6JWPNvv4SPndWljlyMKZEHi/yfHJrJq4Cndq5FbsjI2OJf309IYHmTD7HXCWBW6uztBb/FFS2OWY1ezdokkbP4PJ2a+S9u9S7OvWw/LlTHh8PXNHzGnwWFsVTPoU9v/O3yNu2bJWXcBBRERERE4MapYvIq3LMKwV/Natg+3bYfBguOIK6xfmlv7SWt0QvLgYCgowVr/EkBlWMCusIJYZYlyo7UHYTCswsOWP3bH/6fkGeyNZTcJ7UNzTCNoXyWaC84CdLb89iL1bAz3NujqP53j/tdp/hdn8n9SCgga/DmFfZqOHKS9PoeRQSYvP1RQ1z1QO2Netx7jMdTwjrpcD12BXxDLB6hq/cjyrN9VfPTUmKoblzum4/ycbwwbeIVYWZE1PPBNYFbpBfnMV/KeAqX+byp6De2q29TwCE/4Df3wFoqv8Gy+4AP75z4heW0REREQ6l3BjRQqEiUjrKSiAm26CsrL6+5KSrAyO5v7i7PFAVhYUHV9VsDAFRt7YvNM1i2nFywoKonD/61DDDfn9PE/Ps5qEE9gk3Ob/TlwwZC7uXyxuhcmeYIJ8/UlOhpyciAZjjCqDwq2F3LHuDv5ZHKFAS0OB1upnKg/cFcmwZUubZzkdqjzE7Ndn837x+/SJ7cPsi2czethoK/jm8cD06VbwuZrTCbm5EQ+CVTOqDCsQuHAOjsIPjwfeqv3kJ7C6fvBORERERLoWBcJEpH3Nng1LlgTdFZBRknEDLmMQ9iqs/loDB1r9nVyu0AEAf0aQgRmQmfLS6ZA7vPVuqa5+B+CJV8B9zUK4++6wj/M8PY+sz5dQFGfUbEuusJPz3VkKgjVFdUagzwcOR8PPTARUHqtkyu/H8ELJmxxubodN/9+48UegrHv93cmlkLMG3F/YIpbZFnFt/HkPcOgQzJ0LX30Fp54K2dkQG9s21xYRERGRDk2BMBFpPz/+MbzyStBdnlTIGhfYoD5oqdPJJ8OYMRAXB8OGwdSpVsaVYUBKCp5eRfXO05SSxrA0UDrZ/SiUPgjRfZJg164mBwKMo5VWk/Bdm3EMGIbrqqkqh+wkjKOVeOdk8vqnL/Pby5t2rLMUctdA+hfgPaM7xRPGseftv9Nv90EGlfvLDJ2Rz2wTERERETnRKRAmIu3j/PMx/vVh0B5CnlTInOhPigkWYKqCi4tg9BZwbbWO2V19jqIo7DNnw49+hGfqyODniVAgzGZC4sFaDffrXgNYledv0N0KfZGkE6isxOjRnQGzTOs5aei58z8zC9fDnd5aZX3Vz057ZliJiIiIiJwgFAgTkTZnnPcD7o/7iNyLYV+tVRudpbDkNbjlaiiJpVnBqppMmthzSbn8I2u1x0hmf/nV9OrKsz7eNg6+qZV1NqgUHl0D7t0t7HEmnd+8eXheySZjov99iOcxucxGzqumFTiFVullJiIiIiLS1SkQJiJtKu/n5/EL5784GFN/n81sIAssTNUBqnvWw4Irmn+extT0aNoIzJ6NcdWVeL9eh2/fNhzl4LINwZ4WoVUvpfObNw/P3x5m+pgqimsFTPtWduP6oemkj/yVtcrj2+8o40tEREREpBUpECYibWZ89vmsPvBh4+VhLc3gMqHXESgP0mS8pedNPAR5eZBWvSLdnDlWI26RxlRWYjz+e7w7vPgG9sLx05/hGnaFtcqiiIiIiIi0CQXCRKTVGVUGk+enkt/tK2tDK5QqRkQDQbjapZA1pWsrV8KkSW0xMxEREREREYmAcGNFUW04JxE5gXg2ehg8vwf50V9ZQaa2DII1FL6vs6860LVwHcwoSqafEZhO5iyrFQRLTrYamCsIJiIiIiIickI6qb0nICKdj2ejh8wXMjC7tdMEQgXdggTInGX+nl97+8G6LTxkA+92L75yH44e/XFtB/sFu9W7SUREREREpAtQIExEGmcY4PWCz4cxsD9Z79zc9Ob3kegR1hgbLFgHadvBFweOCnBV9/zKXwp2O3YgLSXt+DHDWnlOIiIiIiIi0mEoECYiIRlVBt4/LcK37GEc35Tj2gbeIVB0I00PgrWR0/dB2tY6G+fOhczMtpuEiIiIiIiIdEgKhIlIUJ6NHqav/DnFUQdgjLWtbwVcWNz0cyUdgj+8ArdcDSWxBA+iRShjzFFR602vXvDUUzBhQstPLCIiIiIiIp1eqzfLf/DBB7HZbMyYMaNm2+HDh5k2bRpJSUnExcWRkZHBrl27WnsqIhImz0YPGXkZFNsOBGzfGwd/O60JJzLhkm2w69zlTChzsuxlf6wrRIbYhE/hrjdh/OdNn7PNhORD3XBdfwfcdRe88QZ8+62CYCIiIiIiIlKjVTPC3n//ff74xz/yve99L2D7zJkz+etf/0p+fj4JCQnceuutuN1u3n777dacjoiEwagymPKXKaEztMLM2oqqgpnvwEP/u9JahTE6BndmJgV5JlnjoCjh+Nh+FfD432CCPwBWmAIvfTf8OVevDJmTdD32ufeHf6CIiIiIiIh0Ka0WCKuoqOC6667jySefZNGiRTXbS0tLeeqpp1i+fDlXXHEFAE8//TSpqam8++67XHzxxa01JREJQ+HWQkoOl7SoTPF7Pnj/SYieNccKggG43VBQgDsri/ScIrxD/A3tje64vjyC3TieJubaBs5SKI4HM4wySmcZ5Lxmw/3RE82ftIiIiIiIiJzwWq00ctq0aVx11VWMHj06YPuHH37I0aNHA7afccYZDB48mA0bNgQ915EjRygrKwt4iUjrKNxa2OJz/ORLiJ4xC7KzA3e43bB1K/Z160n77XImP7aetE8rsB88DI88AldeCfHx2E3IXWMdYqtbRul/v3AdLC+A9c/Alhxw/2g2REe3eO4iIiIiIiJy4mqVjLCVK1fyr3/9i/fff7/evp07dxIdHU3v3r0Dtg8YMICdO3cGPd8DDzzAwoULW2OqItIK0i7IhN89HHyn3Q5pafW3zZhhvQwDvF7cPh8FJ31F1pe5FB3bVzM0uQxy1oB7Y63j09PrB91ERERERERE6oh4RtiOHTvIysri+eefp3v37hE55+23305paWnNa8eOHRE5r4jUl5aS1qLje9iiSXtgZfNPUB0omzwZ94S72Xr7btYPXcjyNXHHs7+qg2CxsbByJbz0UovmLCIiIiIiIl1DxDPCPvzwQ3bv3s0PfvCDmm2GYfDWW2/x+9//ntdee43Kykr2798fkBW2a9cuBg4cGPScMTExxMTERHqqIhJEWkoaifZe7DtW3qw+YTdfcAv2KHvE5mOPspP287vhujuhsNB6gRUsS0uzAmciIiIiIiIiYYh4IGzUqFF8+umnAdt+8YtfcMYZZ/DrX/+a5ORkunXrxtq1a8nIyABg06ZNbN++neHDh0d6OiISrspKWLoU++bNPOl0k3Hs2dArRzZgfOr41pidFfAaNcp6iYiIiIiIiDRDxANhvXr14qyzzgrY1rNnT5KSkmq233TTTcyaNYvExETi4+O57bbbGD58uFaMFGkvc+fCkiUYZhXeIXAkDhYmQu5FsK9n+Kfp1603rsGu1puniIiIiIiISAu0SrP8xjzyyCNERUWRkZHBkSNHGDt2LEuXLm2PqYjI+PGwejWeVMgaB0UJx3fFHQbXFqiywdspjZ/qup7DI1oWKSIiIiIiIhJJNtM0zfaeRFOVlZWRkJBAaWkp8fHx7T0dkU7JqDLwPrMQ3yP38VUi3DPSqoRsTl+wauvPeYS08TMiNEMRERERERGR8IQbK2qXjDCRE45hgNcLPh/0729t270bHA5wudqvoXvtedWai2ejh6xXp1NUXgyZ/rGN9QOrDpkHG2NC8gE7rqumRnT6IiIiIiIiIpGkQJiceOoGparfHzsG+/bBrl0QHw8/+xlccUXLg1QeD0yfjvFNMd4h4IsDRwW4toHdBPr2haVLYcKEiNxeXUaVgXdLIb731+H4fDsuczD2tCugpARuvRX27Dk+2OnEc88kMooerh/4aiwTzIZ1TJ3jbP4AWc53Z2HvFh2JWxIRERERERFpFSqNlBOLxwNZWVBU1OAwwwbeIbCjTxRv/+RcPut1GNuxo6Q7RjJ90iNEx8Q2fq3KSowp/4v3zT+x+jR4/hzYU6uxfN8KWPo3mPC5f8PcubB4cfPvLYiC/xQw9cWb2GOU1WxzlkLuGnBvrD/esEHv30BFNC0qgawtucJOzndn4f5FZO9NREREREREJFzhxooUCJMTh8cDGRk1b6uDXXUztII1hQ9gQlbi1eRMfzn0tebNw/NKdsPn8Z9r7tuw+A3/+/x8yMxs4IA6QpQ2Asx7fR7Zb2fXD2j5/0QvXA+n7gu893suh4Ujw798KPHEsHTgTQwacCquq6YqE0xERERERETalQJh0rVUVkK/flBmZUYFC3Y5S2Hyp/DQJWE0hTfBYe/Njjv31l8F0R8Ey5wY3nkA8vMgcyPWHH2+8MoxCwrglltg797j2046CeP753Df+EQWHn3d2haiZ1ft7c5SWPIaXOeGoxEqiF5/w3rSUtIiczIRERERERGRFgg3VhTVhnMSiRijyqBw81pW5M2ncE4mRnxcQBAscyIU1Xnui+IhO5wgmH+/r2o/sffF4NnoOb69shLjYSsTLNzzYIOpV1kZauzZY2V4NWbePKunWO0gGOA59RhDXB+y8NjrNecOed1aiuNh4oTIBcEAfOW+yJ1MREREREREpA2oWb50Op6NHrI8Uyg6VmJt6AWJWZD1LvzmH4QOUjWjJ9ZR0yAjL4NVE1fhTnXDY49x/6WNlEMGsSfOKtNM24qVEdaQ/HyMh7LxpgSWda4+g+NZaE1kVje6jyBHL0dkTygiIiIiIiLSylQaKZ2KZ6OHzBcyQmZjxR+Gsu4RvqgJST2S2DVnF6tHDSLj8l3NCqolfws/+Qqys14m1rDDkiWwfz+cf771/9HRsHYtBXOuZOq4qoDG+4NK4XA3KIklYk3um8sGOOOT2ZK1pX7ZqIiIiIiIiEg7UI8wOSEYVQbe7V585T769+zPDU9dTXG3w6GDQXV6Y0XSgo8SeOqUUqvksiXXMOGsXfCbt2FQOYzYDu8MBl+CnZe+Y5B3VpDzt+J9BZtf0kEo6eF/X+u6Nv9/CyYWWBlyIiIiIiIiIh2AAmHS6Xk2eshak0VRWVF7TwWAKAOqWiEBKqoKqjpKtz7TCnYV5AE/+QlZCRsoqtxTszs5PpmccTkKgomIiIiIiEiHEm6sSD3CpEPybPSQmZeJGenGVi3QGkEwgKp2LnWszVkGuX+Pwn31bHhwMem1MvIcvRy4BrtUDikiIiIiIiKdlgJh0uEYVQZZa7JaHgRry3LCloj0HP1ZXaYtcFvIa/n3TTwyjOXJU7H/61arXxlgj7KTlpIW4QmKiIiIiIiItA8FwqTD8W73RqQcst9BAhrOR1X5s69aKzjWQQJvmZ/Bu4MDV7as6fkVYo7pZ6TzwjUvtdEMRURERERERNqHAmHS4fhKi1t0vM20Svy+zvU3oY+34Rj/M/bmP8eEidQPBtVOPGtuIMt/jpijcKRbM88R7nUaWCig+1FY6bHeeoeALw4cFeDaBqvPgKxxgQGyBFsP/pjxf0w6a1IrTlpERERERESkY1AgTDocx9c7m32szR+QylkD0VWQthVYVQBuN5yTzqrf3cD0SyoorhUMcpbD5COnkd33y2ZfN+kQLHsZXj4jimfOqWr2eRpk1voYbFVJ4PkXwf5CHiQmklZYaG10ucBux71zJ+m7d+GNK8GXEIXjvDRcQ9PU80tERERERES6DK0aKR1LQQEv3HcN1/zUaFZ2VnKpFQRzbwSSkyEnxwqCVTMMjPVr8b71J3xU4DjnUlw/uQ17t2jy513NxNi/QjNWcHzjWRi1BSpXLifm82utuUeyTNKEOW/D8GKYcjWU9AzcnXgQnnwnCffMZYH3KyIiIiIiItIFhBsrUiBMOo558/C8kk3GRJoeRKqCv7/7Ha445QrsIy61gmD+TKimcGc5eDGxaRlp/SrA93Qf7Mv+H7jdzHt8PNl7Vls7IxEMM2H22/DQG8Cdd2K89y6F9u0UnhUHp59Bmm0oacOuwH5ZWpPvV0REREREROREEG6sSKWR0jHk51P5cDa/mt28w6+OOo0fvrapxdOYlvVnXvzT6CYdc11xIvZdu2uCUIunvQSPj+eh3asDV25shqgqWF4Ak3Ymwqonwe3GDozyv0REREREREQkfM0oAhNpospKeOghGD4cwzGAwkudrJg5msJNr2FUGWAYeO67lkGz/Ks8NiN4NNs5MSJTTUtJIyk2qUnHpGfcUS8Ta/G0lzj863KyvbGM2ApRBoFN+RtjWq+VL53EpEkLYfdulTyKiIiIiIiItJBKIyVA5bFKfv9uLt6PVtNrbyk/M8/mistuxD5yVPPK7ubNs4Jgpokntf6qhc7K7kwecjUPfVNgxYmaGgQzIbkMtvz4DWuOEeDZ6CEjL6PhFRqrr10RxZYHDmHvFh3iZB7IzMRzhmmVfNLIOf2Sy2zk9JqAe/5ylTuKiIiIiIiINEI9wiR8hgFeL7P/vZgl+1+tt7t7JTz3Rk8m/Pq5pmUlzZsH2dkAeFIhcyL1g121n74mBsGqV4gsWJuEu3BXRANGno0epntupvjYvoavPWQu7l8sbuRkHsjKwtOriOnjCFixsm9sX64/53quPuVK+OxTdu/eimPAMFxXTQ0dXBMRERERERGRAAqESXgKCmDqVC5M38P7gwgdjDJh7tuw+JZVAcEwo8rAu93Ljm+38faGfD7b+TEcOczZ3ZyM+Mu/SS6HEdthWBYUxTdw/maoWSFy0apWKRs0qgy8f76f1Z4HeP60w1bZZvW1y23knDmn8SBYzcmsYKPxTTHeHnvwDe2HI2EQrsEu7FHK+BIRERERERFpCQXCpHH+jK2ZoyHnEv+2BgJhACs9UUz60Vz44Q/xDPyWrL/PpKisqMHLRBlQ1dJYj//6Cwrh9BJwVICryon9kdzW751lGBjr1+J960/4qMBxzqW4fnKbMrZEREREREREOggFwqRh+fkwcSIvnAnXZBJ+plYVXLINeh6Dv3/Hvy2CWV6h9KuAJ/4K7sRLYNo0cDjA5VL/LBEREREREREJO1Z0UhvOSdqbvzyP4mKM/7mR+y+DBSNpWiArCt4e2loTDC7+MBQt60H0zr0QG9u2FxcRERERERGRE4YCYV2Fx4MxYzpeezEvnQZPz4SyThJTmv0ORK/+q4JgIiIiIiIiItIiCoSdoIyjlXj/uhTfrs04dlbgy3+GW66F0k4WS+pxBO7cMsgqgxQRERERERERaQEFwk5AnqfnkfX5EorijOMbm9IHrAO5+UOw5zyqXmAiIiIiIiIi0mIKhJ1gPE/PI3NbNmbPOjs6YRAMYPx1C1t/VUgRERERERER6RKi2nsCEjnG0UqyPl+CCR0/8GX6Xw3sT+7lxHX9nW01IxERERERERE5wSkQdgLx/nWpVQ7Z0iBYQwGqlvIHwM4vhrgjocfYbJBzZS72KJVEioiIiIiIiEhkKBB2AvHt2hyR8wwqg+5HaVlALNSxNhjWZxjvpz7M/nUXsMBrp9fhwCHJ0UkUTFyFO1UlkSIiIiIiIiISOTbTNFsz/6dVlJWVkZCQQGlpKfHx8e09nVZjVBl4t3vxlftw9HLgGjQC+1teWLcOtm+HwYMxRl6Od5BB8ZMPs2Hn+zx+elmLrnnDR/DUabNZfcPFZBRMsAJaTckwMyHpINywL5kc5w6q6hybdVEWOeNyat2kgfFWId7/FuLrBY7z0nANTVMmmIiIiIiIiIiELdxYkQJhHZRno4esNVkUlRXVbOt1GH64Gb67F9K2QEkPmHYV7K3bGL85TKtUcf+5K7FPnHR8Dq9mUVR+fA7dKyHlW/iiv3+DLfAcAKvWJuEu3EWlabD0g6Vs3reZYYnDmHr+VKJPio7AZEVEREREREREjlMgrBMr+DSPCZ5JjQ9sarZWQ+cBVk3Ix31mZsCugKy0Hv1xbQf73F/jOfghWeOgKOH42ORSyFkD7kWrtNKjiIiIiIiIiLQZBcI6qfz/m8Ok7Q9jtuGqj4ndE3nyJ082rSdXXh7GL2/G27sMXxw4KsBV5cT+SK6CYCIiIiIiIiLSphQI64Q8T88jY1t2ZLK8GtENG+lnuPnVBbeQltLMnlyGAV4v+HzgcIDLBXb19hIRERERERGRthVurOikNpyTNMA4Wsltnz8Ekej31YCzdsKSC+/iihvuaXlDersd0tIiMi8RERERERERkdYW1d4TEIv3r0v5Js5s9Wyw8Zvgh0NHaVVGEREREREREelyFAjrIL7c9kWbXCdtK1Ypo4iIiIiIiIhIF6NAWAfxr909WvcCJiQd8AfCHI7WvZaIiIiIiIiISAekQFgHsHlPBW8Yl9HnYCtdwL8cwrJXwD7IaTW1FxERERERERHpYhQI6wCG9YvjlRlXMKPXzVbQKsLreDrLYFUeuDcCubla2VFEREREREREuiStGtlBnDEwnrvvWkbF47vJ3rO68QNM6jXWjzsCs9+BU0tgTxz0OwCDysG1DeyJSbBqGbjdrTJ/EREREREREZGOToGwDmbxtJe48NM8bnl5CnuPltZs71Vp44f7+vDd5HNJy5yL65SReLd7KdxaCEBaShppyS7sb78DxcWwaxeUlEBUFKSlWS9lgomIiIiIiIhIF2YzTTOihXgPPPAAHo+HL774gtjYWEaMGMHvfvc7Tj/99Joxhw8fZvbs2axcuZIjR44wduxYli5dyoABA8K6RllZGQkJCZSWlhIfHx/J6XcYRpWBd7sXX7kPRy8HrsEu7FEKZImIiIiIiIiI1BVurCjiPcLefPNNpk2bxrvvvsvrr7/O0aNHGTNmDAcOHKgZM3PmTF5++WXy8/N58803+eabb3CrZC+APcpOWkoak8+eTFpKmoJgIiIiIiIiIiItFPGMsLr27NlD//79efPNN7nssssoLS2lX79+LF++nMzMTAC++OILUlNT2bBhAxdffHGj5+wKGWEiIiIiIiIiIhKedssIq6u01OpzlZiYCMCHH37I0aNHGT16dM2YM844g8GDB7Nhw4ag5zhy5AhlZWUBLxERERERERERkaZo1UBYVVUVM2bM4JJLLuGss84CYOfOnURHR9O7d++AsQMGDGDnzp1Bz/PAAw+QkJBQ80pOTm7NaYuIiIiIiIiIyAmoVQNh06ZN47PPPmPlypUtOs/tt99OaWlpzWvHjh0RmqGIiIiIiIiIiHQVJ7XWiW+99VZeeeUV3nrrLZxOZ832gQMHUllZyf79+wOywnbt2sXAgQODnismJoaYmJjWmqqIiIiIiIiIiHQBEc8IM02TW2+9lRdffJF169YxdOjQgP3nnXce3bp1Y+3atTXbNm3axPbt2xk+fHikpyMiIiIiIiIiIgK0QkbYtGnTWL58OatXr6ZXr141fb8SEhKIjY0lISGBm266iVmzZpGYmEh8fDy33XYbw4cPD2vFSBERERERERERkeawmaZpRvSENlvQ7U8//TQ33ngjAIcPH2b27NmsWLGCI0eOMHbsWJYuXRqyNLKucJfEFBERERERERGRE1+4saKIB8LaggJhIiIiIiIiIiJSLdxYUauuGikiIiIiIiIiItJRKBAmIiIiIiIiIiJdggJhIiIiIiIiIiLSJSgQJiIiIiIiIiIiXYICYSIiIiIiIiIi0iWc1N4TaI7qhS7LysraeSYiIiIiIiIiItLeqmNE1TGjUDplIKy8vByA5OTkdp6JiIiIiIiIiIh0FOXl5SQkJITcbzMbC5V1QFVVVXzzzTf06tULm83W3tOJiLKyMpKTk9mxYwfx8fHtPR2RNqNnX7oqPfvSVenZl65Kz750VXr2pa2Ypkl5eTknn3wyUVGhO4F1yoywqKgonE5ne0+jVcTHx+ubg3RJevalq9KzL12Vnn3pqvTsS1elZ1/aQkOZYNXULF9ERERERERERLoEBcJERERERERERKRLUCCsg4iJiWHBggXExMS091RE2pSefemq9OxLV6VnX7oqPfvSVenZl46mUzbLFxERERERERERaSplhImIiIiIiIiISJegQJiIiIiIiIiIiHQJCoSJiIiIiIiIiEiXoECYiIiIiIiIiIh0CQqEiYiIiIiIiIhIl6BAWAfx+OOPk5KSQvfu3bnooov45z//2d5TEmm2e+65B5vNFvA644wzavYfPnyYadOmkZSURFxcHBkZGezatSvgHNu3b+eqq66iR48e9O/fn7lz53Ls2LG2vhWRBr311lv8+Mc/5uSTT8Zms/HSSy8F7DdNk7vvvhuHw0FsbCyjR4/mq6++Chizb98+rrvuOuLj4+nduzc33XQTFRUVAWM++eQTXC4X3bt3Jzk5mcWLF7f2rYk0qLFn/8Ybb6z398C4ceMCxujZl87mgQce4IILLqBXr17079+f8ePHs2nTpoAxkfoZp7CwkB/84AfExMTwne98h2eeeaa1b08kpHCe/bS0tHrf93/1q18FjNGzLx2FAmEdwAsvvMCsWbNYsGAB//rXvzjnnHMYO3Ysu3fvbu+piTTbmWeeic/nq3n94x//qNk3c+ZMXn75ZfLz83nzzTf55ptvcLvdNfsNw+Cqq66isrKSd955h2effZZnnnmGu+++uz1uRSSkAwcOcM455/D4448H3b948WIeffRRnnjiCd577z169uzJ2LFjOXz4cM2Y6667jv/85z+8/vrrvPLKK7z11ltMmTKlZn9ZWRljxoxhyJAhfPjhh2RnZ3PPPfewbNmyVr8/kVAae/YBxo0bF/D3wIoVKwL269mXzubNN99k2rRpvPvuu7z++uscPXqUMWPGcODAgZoxkfgZZ8uWLVx11VWMHDmSjz/+mBkzZvC///u/vPbaa216vyLVwnn2AW6++eaA7/u1//FCz750KKa0uwsvvNCcNm1azXvDMMyTTz7ZfOCBB9pxViLNt2DBAvOcc84Jum///v1mt27dzPz8/JptGzduNAFzw4YNpmma5t/+9jczKirK3LlzZ82YP/zhD2Z8fLx55MiRVp27SHMB5osvvljzvqqqyhw4cKCZnZ1ds23//v1mTEyMuWLFCtM0TfPzzz83AfP999+vGfPqq6+aNpvNLC4uNk3TNJcuXWr26dMn4Nn/9a9/bZ5++umtfEci4an77Jumad5www1menp6yGP07MuJYPfu3SZgvvnmm6ZpRu5nnHnz5plnnnlmwLUmTZpkjh07trVvSSQsdZ990zTNyy+/3MzKygp5jJ596UiUEdbOKisr+fDDDxk9enTNtqioKEaPHs2GDRvacWYiLfPVV19x8sknc8opp3Ddddexfft2AD788EOOHj0a8MyfccYZDB48uOaZ37BhA2effTYDBgyoGTN27FjKysr4z3/+07Y3ItJMW7ZsYefOnQHPekJCAhdddFHAs967d2/OP//8mjGjR48mKiqK9957r2bMZZddRnR0dM2YsWPHsmnTJr799ts2uhuRpissLKR///6cfvrp3HLLLZSUlNTs07MvJ4LS0lIAEhMTgcj9jLNhw4aAc1SP0e8G0lHUffarPf/88/Tt25ezzjqL22+/nYMHD9bs07MvHclJ7T2Brm7v3r0YhhHwDQFgwIABfPHFF+00K5GWueiii3jmmWc4/fTT8fl8LFy4EJfLxWeffcbOnTuJjo6md+/eAccMGDCAnTt3ArBz586gfyaq94l0BtXParBnufaz3r9//4D9J510EomJiQFjhg4dWu8c1fv69OnTKvMXaYlx48bhdrsZOnQomzdv5o477uDKK69kw4YN2O12PfvS6VVVVTFjxgwuueQSzjrrLICI/YwTakxZWRmHDh0iNja2NW5JJCzBnn2Aa6+9liFDhnDyySfzySef8Otf/5pNmzbh8XgAPfvSsSgQJiIRd+WVV9b8//e+9z0uuugihgwZQl5env4CExHpAq655pqa/z/77LP53ve+x7BhwygsLGTUqFHtODORyJg2bRqfffZZQA9Uka4g1LNfu8fj2WefjcPhYNSoUWzevJlhw4a19TRFGqTSyHbWt29f7HZ7vdVkdu3axcCBA9tpViKR1bt3b0477TS+/vprBg4cSGVlJfv37w8YU/uZHzhwYNA/E9X7RDqD6me1oe/vAwcOrLcwyrFjx9i3b5/+PMgJ5ZRTTqFv3758/fXXgJ596dxuvfVWXnnlFdavX4/T6azZHqmfcUKNiY+P1z8oSrsK9ewHc9FFFwEEfN/Xsy8dhQJh7Sw6OprzzjuPtWvX1myrqqpi7dq1DB8+vB1nJhI5FRUVbN68GYfDwXnnnUe3bt0CnvlNmzaxffv2mmd++PDhfPrppwG/JL3++uvEx8fz3e9+t83nL9IcQ4cOZeDAgQHPellZGe+9917As75//34+/PDDmjHr1q2jqqqq5gfI4cOH89Zbb3H06NGaMa+//jqnn366SsOk0ygqKqKkpASHwwHo2ZfOyTRNbr31Vl588UXWrVtXr3Q3Uj/jDB8+POAc1WP0u4G0l8ae/WA+/vhjgIDv+3r2pcNo7279YporV640Y2JizGeeecb8/PPPzSlTppi9e/cOWFFDpDOZPXu2WVhYaG7ZssV8++23zdGjR5t9+/Y1d+/ebZqmaf7qV78yBw8ebK5bt8784IMPzOHDh5vDhw+vOf7YsWPmWWedZY4ZM8b8+OOPzTVr1pj9+vUzb7/99va6JZGgysvLzY8++sj86KOPTMBcsmSJ+dFHH5nbtm0zTdM0H3zwQbN3797m6tWrzU8++cRMT083hw4dah46dKjmHOPGjTPPPfdc87333jP/8Y9/mKeeeqo5efLkmv379+83BwwYYP7sZz8zP/vsM3PlypVmjx49zD/+8Y9tfr8i1Rp69svLy805c+aYGzZsMLds2WK+8cYb5g9+8APz1FNPNQ8fPlxzDj370tnccsstZkJCgllYWGj6fL6a18GDB2vGROJnnP/+979mjx49zLlz55obN240H3/8cdNut5tr1qxp0/sVqdbYs//111+b9957r/nBBx+YW7ZsMVevXm2ecsop5mWXXVZzDj370pEoENZBPPbYY+bgwYPN6Oho88ILLzTffffd9p6SSLNNmjTJdDgcZnR0tDlo0CBz0qRJ5tdff12z/9ChQ+bUqVPNPn36mD169DB/+tOfmj6fL+AcW7duNa+88kozNjbW7Nu3rzl79mzz6NGjbX0rIg1av369CdR73XDDDaZpmmZVVZU5f/58c8CAAWZMTIw5atQoc9OmTQHnKCkpMSdPnmzGxcWZ8fHx5i9+8QuzvLw8YMy///1v89JLLzVjYmLMQYMGmQ8++GBb3aJIUA09+wcPHjTHjBlj9uvXz+zWrZs5ZMgQ8+abb673D3x69qWzCfbMA+bTTz9dMyZSP+OsX7/e/P73v29GR0ebp5xySsA1RNpaY8/+9u3bzcsuu8xMTEw0Y2JizO985zvm3LlzzdLS0oDz6NmXjsJmmqbZdvlnIiIiIiIiIiIi7UM9wkREREREREREpEtQIExERERERERERLoEBcJERERERERERKRLUCBMRERERERERES6BAXCRERERERERESkS1AgTEREREREREREugQFwkREREREREREpEtQIExERERERERERLoEBcJERERERERERKRLUCBMRERERERERES6BAXCRERERERERESkS/j/cbKrkTTVS+MAAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.cla()\n",
    "env_test.render_all()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T08:19:35.087248Z",
     "start_time": "2024-04-15T08:19:34.880667Z"
    }
   },
   "id": "121c1cb2fc07c0b",
   "execution_count": 72
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ELSE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccd1cc4ef63ea420"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[1.20708733e+02, 7.71849239e+01, 6.87480000e+06, 4.23269108e+00,\n         3.67541744e+00, 1.25104950e+02, 1.02873050e+02, 5.12253000e+08],\n        [1.22203392e+02, 7.93182589e+01, 7.69730000e+06, 4.31013391e+00,\n         3.80236073e+00, 1.25933788e+02, 1.03676213e+02, 5.19950300e+08],\n        [1.20997704e+02, 7.33593066e+01, 9.17660000e+06, 4.22550989e+00,\n         3.88699057e+00, 1.26314896e+02, 1.04783105e+02, 5.10773700e+08],\n        [1.22093781e+02, 7.51845059e+01, 1.65333000e+07, 4.19849140e+00,\n         3.94929073e+00, 1.26819234e+02, 1.05739766e+02, 5.27307000e+08],\n        [1.07655373e+02, 3.81276857e+01, 4.66429000e+07, 2.97769518e+00,\n         3.75497162e+00, 1.26709398e+02, 1.05889603e+02, 4.80664100e+08],\n        [1.07635437e+02, 3.80997630e+01, 1.28467000e+07, 1.98570675e+00,\n         3.40111865e+00, 1.26640695e+02, 1.05964306e+02, 4.67817400e+08],\n        [1.06748604e+02, 3.68083649e+01, 1.01579000e+07, 1.11513418e+00,\n         2.94392175e+00, 1.26772160e+02, 1.05670840e+02, 4.57659500e+08],\n        [1.08432594e+02, 4.09045035e+01, 9.35290000e+06, 5.54688989e-01,\n         2.46607520e+00, 1.26851145e+02, 1.05436855e+02, 4.67012400e+08],\n        [1.08183487e+02, 4.04864266e+01, 7.66090000e+06, 8.94007013e-02,\n         1.99074030e+00, 1.26942737e+02, 1.05175264e+02, 4.59351500e+08]]),\n 0,\n False,\n False,\n {'total_reward': 28.1407413482666,\n  'total_profit': 1.7757067315852784,\n  'position': <Positions.Long: 1>})"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_test.step([2])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T08:19:54.450255Z",
     "start_time": "2024-04-15T08:19:54.443341Z"
    }
   },
   "id": "f3b2cffb0216619b",
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9dd56ff57396362a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
