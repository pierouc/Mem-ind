{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1QhUrEgbVhrH2EQLivXW_5W71BH5OEEir",
     "timestamp": 1691492010800
    },
    {
     "file_id": "18pOsXlNIEqBIeLnqcmpzLgWXmdm6TPQ7",
     "timestamp": 1690811564931
    },
    {
     "file_id": "1LktupNCjBej2uX9W6o1qA96RXI5U-1eB",
     "timestamp": 1689019044016
    },
    {
     "file_id": "1ggENrMWBScve4DG3kCWxdIMccf3ays8h",
     "timestamp": 1688364310748
    }
   ],
   "authorship_tag": "ABX9TyN1J6E6sZFblAZ33Vst3N+9"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CNEWrIob-gVz",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1697076559898,
     "user_tz": 180,
     "elapsed": 31900,
     "user": {
      "displayName": "Italo Sanhueza",
      "userId": "00255692059650224046"
     }
    },
    "outputId": "ecdbe15e-ac27-4992-86ec-18e5e6cbb651",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import os\n",
    "import tensorflow as tf\n",
    "# import gym\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Descargar los precios\n",
    "data = yf.download('AAPL', start='2011-10-21', end='2023-05-24', interval=\"1d\")\n",
    "\n",
    "prices = data['Adj Close'].values\n",
    "prices2 = prices[54:]\n",
    "# Imprimir data\n",
    "#print(data.head())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OtVkmVr_yzHc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1697076836082,
     "user_tz": 180,
     "elapsed": 824,
     "user": {
      "displayName": "Italo Sanhueza",
      "userId": "00255692059650224046"
     }
    },
    "outputId": "2ef8c2f4-ac17-4e45-b67e-396b14bd29af",
    "ExecuteTime": {
     "end_time": "2024-03-10T19:43:59.362349Z",
     "start_time": "2024-03-10T19:43:57.380522Z"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Función para calcular el RSI\n",
    "def calculate_rsi(prices, n=14):\n",
    "    deltas = np.diff(prices)\n",
    "    seed = deltas[:n + 1]\n",
    "    up = seed[seed >= 0].sum() / n\n",
    "    down = -seed[seed < 0].sum() / n\n",
    "    rs = up / down\n",
    "    rsi = [np.nan] * (n - 1) + [100. - 100. / (1. + rs)]\n",
    "\n",
    "    for i in range(n, len(prices)):\n",
    "        delta = deltas[i - 1]  # Cambio actual\n",
    "        if delta > 0:\n",
    "            upval = delta\n",
    "            downval = 0.\n",
    "        else:\n",
    "            upval = 0.\n",
    "            downval = -delta\n",
    "        up = (up * (n - 1) + upval) / n\n",
    "        down = (down * (n - 1) + downval) / n\n",
    "        rs = up / down\n",
    "        rsi.append(100. - 100. / (1. + rs))\n",
    "\n",
    "    return rsi\n",
    "\n",
    "\n",
    "\n",
    "# Calcular el RSI\n",
    "rsi = calculate_rsi(prices)\n",
    "rsi2 = rsi[40:]"
   ],
   "metadata": {
    "id": "RJuIRjVC-5sw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Definir el entorno para el agente de RL\n",
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, prices2, rsi2):\n",
    "        super(CustomEnv, self).__init__()\n",
    "        self.data = prices2\n",
    "        self.predictions = rsi2\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        self.reward_range = (-1, 1)\n",
    "        self.action_space = gym.spaces.Discrete(3)\n",
    "        self.precio_anterior = 0\n",
    "        self.positions = []  # Lista para almacenar las posiciones anteriores\n",
    "        self.total_reward = 0  # Variable para rastrear la suma total de las recompensas en un episodio\n",
    "        self.compras_realizadas = []\n",
    "\n",
    "        # Definir el espacio de observación\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(2, len(self.predictions)))\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.total_reward = 0\n",
    "        self.done = False\n",
    "        self.positions = []  # Reiniciar la lista de posiciones anteriores\n",
    "\n",
    "        # Devolver la observación inicial directamente\n",
    "        return self.get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        # Tomar la acción y actualizar el entorno\n",
    "        # Calcula la recompensa en función de la acción y el estado actual\n",
    "        reward = self.calculate_reward(action)\n",
    "\n",
    "        # Actualizar la suma total de recompensas en el episodio actual\n",
    "        self.total_reward += reward\n",
    "\n",
    "        # Actualizar el estado interno y verificar si el episodio ha terminado\n",
    "        self.current_step += 1\n",
    "        if self.current_step == len(self.data):\n",
    "            self.done = True\n",
    "\n",
    "        # Devolver la observación, la recompensa, si el episodio ha terminado y cualquier información adicional\n",
    "        return self.get_observation(), reward, self.done, {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_reward(self, action):\n",
    "        reward = 0  # Valor predeterminado de recompensa\n",
    "\n",
    "        if self.current_step >= len(self.data):\n",
    "            self.current_step = len(self.data) - 1\n",
    "\n",
    "        # Obtener el precio actual, la posición anterior y el valor RSI actual\n",
    "        current_price = self.data[self.current_step]\n",
    "        current_rsi = self.predictions[self.current_step]\n",
    "\n",
    "        # Calcular la ganancia o pérdida según la acción tomada\n",
    "        #if action == 0:  # Acción: no hacer nada\n",
    "           # reward = 0\n",
    "        if action == 0:  # Acción: comprar\n",
    "            if current_rsi < 40:\n",
    "                self.positions.append(current_price)\n",
    "                reward = 0  # Costo de comprar\n",
    "        elif action == 1:  # Acción: vender\n",
    "            if len(self.positions) > 0 and current_rsi > 65:\n",
    "                num_positions = len(self.positions)  # Número de posiciones previas\n",
    "\n",
    "                # Calcular el valor total de la venta (suma de todos los precios de compra)\n",
    "                total_sale_value = sum(self.positions)\n",
    "\n",
    "                if total_sale_value > 0:\n",
    "                    reward = (current_price * num_positions - total_sale_value) / total_sale_value\n",
    "                else:\n",
    "                    reward = 0  # Penalización por pérdida\n",
    "\n",
    "                # Reiniciar las posiciones después de la venta\n",
    "                self.positions = []\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def get_observation(self):\n",
    "        if self.current_step >= len(self.data):\n",
    "            self.current_step = len(self.data) - 1\n",
    "\n",
    "        current_price = self.data[self.current_step]\n",
    "        current_prediction = self.predictions[self.current_step]\n",
    "\n",
    "        # Crear una observación con forma (2, 621) concatenando el precio y la predicción\n",
    "        observation = np.zeros((2, len(self.predictions)))\n",
    "        observation[0, self.current_step] = current_price\n",
    "        observation[1, self.current_step] = current_prediction\n",
    "\n",
    "        return observation\n",
    "\n",
    "    def get_total_reward(self):\n",
    "        return self.total_reward\n",
    "# Crear el entorno y envolverlo en DummyVecEnv\n",
    "env = DummyVecEnv([lambda: CustomEnv(prices2, rsi2)])\n",
    "# Crear y entrenar el agente RL con PPO\n",
    "model_rl = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model_rl.learn(total_timesteps=50000)\n"
   ],
   "metadata": {
    "id": "Uow7PlmlL5os",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d9409830-2d8a-4dbf-a682-6781e3b7249e",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1697077015882,
     "user_tz": 180,
     "elapsed": 169814,
     "user": {
      "displayName": "Italo Sanhueza",
      "userId": "00255692059650224046"
     }
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cpu device\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 966  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 449         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.064628355 |\n",
      "|    clip_fraction        | 0.632       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | -37.1       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.159      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.115      |\n",
      "|    value_loss           | 0.0495      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 381         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053550612 |\n",
      "|    clip_fraction        | 0.758       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | -9.28       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.128      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.117      |\n",
      "|    value_loss           | 0.0108      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 356        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 22         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06909861 |\n",
      "|    clip_fraction        | 0.792      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.01      |\n",
      "|    explained_variance   | -0.391     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.11      |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.123     |\n",
      "|    value_loss           | 0.000969   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 29         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09243773 |\n",
      "|    clip_fraction        | 0.783      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.964     |\n",
      "|    explained_variance   | 0.552      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.13      |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.123     |\n",
      "|    value_loss           | 0.000564   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 335        |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 36         |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12537988 |\n",
      "|    clip_fraction        | 0.765      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.903     |\n",
      "|    explained_variance   | 0.234      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.132     |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.122     |\n",
      "|    value_loss           | 0.000676   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 332        |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 43         |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15271337 |\n",
      "|    clip_fraction        | 0.701      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.812     |\n",
      "|    explained_variance   | 0.317      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.125     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.113     |\n",
      "|    value_loss           | 0.000635   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 326        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 50         |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15161417 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.712     |\n",
      "|    explained_variance   | 0.582      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.132     |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.107     |\n",
      "|    value_loss           | 0.000575   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 326        |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 56         |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.19639173 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.611     |\n",
      "|    explained_variance   | 0.655      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.127     |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.104     |\n",
      "|    value_loss           | 0.000453   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 322        |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 63         |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16472109 |\n",
      "|    clip_fraction        | 0.557      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.573     |\n",
      "|    explained_variance   | 0.707      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.108     |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0914    |\n",
      "|    value_loss           | 0.000284   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 322        |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 69         |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13044956 |\n",
      "|    clip_fraction        | 0.502      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.548     |\n",
      "|    explained_variance   | 0.646      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0907    |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0782    |\n",
      "|    value_loss           | 0.000322   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 318         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 77          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.118620336 |\n",
      "|    clip_fraction        | 0.441       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.484      |\n",
      "|    explained_variance   | 0.684       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0863     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0712     |\n",
      "|    value_loss           | 0.000353    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 319        |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 83         |\n",
      "|    total_timesteps      | 26624      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13230827 |\n",
      "|    clip_fraction        | 0.486      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.526     |\n",
      "|    explained_variance   | 0.691      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.12      |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0843    |\n",
      "|    value_loss           | 0.000193   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 316        |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 90         |\n",
      "|    total_timesteps      | 28672      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10811326 |\n",
      "|    clip_fraction        | 0.46       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.541     |\n",
      "|    explained_variance   | 0.653      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0935    |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.0702    |\n",
      "|    value_loss           | 0.000426   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 316        |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 96         |\n",
      "|    total_timesteps      | 30720      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10671132 |\n",
      "|    clip_fraction        | 0.456      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.526     |\n",
      "|    explained_variance   | 0.712      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.105     |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0745    |\n",
      "|    value_loss           | 0.000293   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 314        |\n",
      "|    iterations           | 16         |\n",
      "|    time_elapsed         | 104        |\n",
      "|    total_timesteps      | 32768      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12059959 |\n",
      "|    clip_fraction        | 0.44       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.477     |\n",
      "|    explained_variance   | 0.78       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0967    |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.0679    |\n",
      "|    value_loss           | 0.000227   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 315        |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 110        |\n",
      "|    total_timesteps      | 34816      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11467713 |\n",
      "|    clip_fraction        | 0.487      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.551     |\n",
      "|    explained_variance   | 0.867      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0964    |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0787    |\n",
      "|    value_loss           | 9.98e-05   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 314        |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 117        |\n",
      "|    total_timesteps      | 36864      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09416495 |\n",
      "|    clip_fraction        | 0.439      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.554     |\n",
      "|    explained_variance   | 0.747      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0795    |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.0596    |\n",
      "|    value_loss           | 0.000256   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 314        |\n",
      "|    iterations           | 19         |\n",
      "|    time_elapsed         | 123        |\n",
      "|    total_timesteps      | 38912      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09954408 |\n",
      "|    clip_fraction        | 0.448      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.509     |\n",
      "|    explained_variance   | 0.803      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0609    |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0726    |\n",
      "|    value_loss           | 0.00022    |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 313        |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 130        |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11480657 |\n",
      "|    clip_fraction        | 0.492      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.532     |\n",
      "|    explained_variance   | 0.657      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0941    |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0759    |\n",
      "|    value_loss           | 0.000149   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 313         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 136         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.092246085 |\n",
      "|    clip_fraction        | 0.419       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.505      |\n",
      "|    explained_variance   | 0.827       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0865     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0663     |\n",
      "|    value_loss           | 0.000191    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 312        |\n",
      "|    iterations           | 22         |\n",
      "|    time_elapsed         | 144        |\n",
      "|    total_timesteps      | 45056      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09714949 |\n",
      "|    clip_fraction        | 0.442      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.527     |\n",
      "|    explained_variance   | 0.842      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0853    |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.0681    |\n",
      "|    value_loss           | 0.0002     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 313         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 150         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.109408244 |\n",
      "|    clip_fraction        | 0.458       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.469      |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.108      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0745     |\n",
      "|    value_loss           | 7.85e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 312         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 157         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.094981074 |\n",
      "|    clip_fraction        | 0.454       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.518      |\n",
      "|    explained_variance   | 0.788       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0982     |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0631     |\n",
      "|    value_loss           | 0.00012     |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 310       |\n",
      "|    iterations           | 25        |\n",
      "|    time_elapsed         | 164       |\n",
      "|    total_timesteps      | 51200     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0772367 |\n",
      "|    clip_fraction        | 0.397     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.47     |\n",
      "|    explained_variance   | 0.906     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0771   |\n",
      "|    n_updates            | 240       |\n",
      "|    policy_gradient_loss | -0.0605   |\n",
      "|    value_loss           | 0.000131  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7d5f6fc43a90>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_agent(agent, eval_episodes=20):\n",
    "    total_rewards = []\n",
    "    total_compras = 0\n",
    "    total_ventas = 0\n",
    "    total_transacciones_ganadoras = 0  # Variable para contar transacciones con recompensa > 0\n",
    "\n",
    "    for i in range(eval_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        num_compras = 0\n",
    "        num_ventas = 0\n",
    "        num_transacciones_ganadoras = 0  # Contador para el episodio actual\n",
    "\n",
    "        while not done:\n",
    "            action, _ = agent.predict(obs)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            if action == 1:\n",
    "                num_compras += 1\n",
    "            elif action == 2:\n",
    "                num_ventas += 1\n",
    "\n",
    "            if reward > 0:  # Incrementar el contador si la recompensa es positiva\n",
    "                num_transacciones_ganadoras += 1\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "        total_compras += num_compras\n",
    "        total_ventas += num_ventas\n",
    "        total_transacciones_ganadoras += num_transacciones_ganadoras\n",
    "\n",
    "        #print(f\"Episode {i+1}: Compras = {num_compras}, Ventas = {num_ventas}, \"\n",
    "         #     f\"Reward = {total_reward}, Transacciones Ganadoras = {num_transacciones_ganadoras}\")\n",
    "\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    avg_compras = total_compras / eval_episodes\n",
    "    avg_ventas = total_ventas / eval_episodes\n",
    "    avg_transacciones_ganadoras = total_transacciones_ganadoras / eval_episodes\n",
    "    avg_transacciones_ganadoras_per_compra = avg_transacciones_ganadoras / avg_compras\n",
    "\n",
    "    print(f\"Promedio reward en {eval_episodes} episodios: {avg_reward}\")\n",
    "    print(f\"Promedio de compras: {avg_compras}\")\n",
    "    print(f\"Promedio de ventas: {avg_ventas}\")\n",
    "    #print(f\"Promedio de Transacciones Ganadoras: {avg_transacciones_ganadoras}\")\n",
    "    #print(f\"Promedio de Transacciones Ganadoras por Compra: {avg_transacciones_ganadoras_per_compra}\")\n",
    "\n",
    "# Evaluar el agente entrenado\n",
    "evaluate_agent(model_rl)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0SyjcyWdfSbD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1697077060536,
     "user_tz": 180,
     "elapsed": 44656,
     "user": {
      "displayName": "Italo Sanhueza",
      "userId": "00255692059650224046"
     }
    },
    "outputId": "a6c956b0-0148-4f38-f034-69dfaf75542a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Promedio reward en 20 episodios: 2.7476978302001953\n",
      "Promedio de compras: 979.9\n",
      "Promedio de ventas: 899.55\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_agent(agent, eval_episodes=10):\n",
    "    total_rewards = []\n",
    "    total_compras = 0\n",
    "    total_ventas = 0\n",
    "    total_num_operaciones_ganadoras = 0\n",
    "    total_num_operaciones_perdedoras = 0\n",
    "    for i in range(eval_episodes):  # Agregar la variable 'i' en el bucle\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        num_compras = 0\n",
    "        num_ventas = 0\n",
    "        num_operaciones_ganadoras = 0\n",
    "        num_operaciones_perdedoras= 0\n",
    "\n",
    "        while not done:\n",
    "            action, _ = agent.predict(obs)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            if action == 1:\n",
    "                num_compras += 1\n",
    "            elif action == 2:\n",
    "                num_ventas += 1\n",
    "\n",
    "            if reward > 0:\n",
    "                num_operaciones_ganadoras += 1\n",
    "\n",
    "            if reward < 0:\n",
    "                num_operaciones_perdedoras += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "        total_compras += num_compras\n",
    "        total_ventas += num_ventas\n",
    "        total_num_operaciones_ganadoras += num_operaciones_ganadoras\n",
    "        total_num_operaciones_perdedoras += num_operaciones_perdedoras\n",
    "\n",
    "        print(f\"Episode {i+1}: Compras = {num_compras}, Ventas = {num_ventas}, Reward = {total_reward},Número de operaciones ganadoras = {num_operaciones_ganadoras}, Número de operaciones perdedoras = {num_operaciones_perdedoras}\")\n",
    "\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    avg_compras = total_compras / eval_episodes\n",
    "    avg_ventas = total_ventas / eval_episodes\n",
    "    avg_num_operaciones_ganadoras = total_num_operaciones_ganadoras / eval_episodes\n",
    "    avg_num_operaciones_perdedoras = total_num_operaciones_perdedoras / eval_episodes\n",
    "    print(f\"Promedio reward  {eval_episodes} episodes: {avg_reward}\")\n",
    "    print(f\"Promedio de compras: {avg_compras}\")\n",
    "    print(f\"Promedio de ventas: {avg_ventas}\")\n",
    "    print(f\"Promedio Numero acciones ganadoras: {avg_num_operaciones_ganadoras}\")\n",
    "    print(f\"Promedio Numero acciones perdedoras: {avg_num_operaciones_perdedoras}\")\n",
    "\n",
    "# Evaluar el agente entrenado\n",
    "evaluate_agent(model_rl)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4tNlAD20SCIF",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1691521077217,
     "user_tz": 240,
     "elapsed": 6993,
     "user": {
      "displayName": "Italo Sanhueza",
      "userId": "00255692059650224046"
     }
    },
    "outputId": "85daba10-0675-4255-a3b1-26f23283f008"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode 1: Compras = 108, Ventas = 103, Reward = [0.14611207],Número de operaciones ganadoras = 2, Número de operaciones perdedoras = 0\n",
      "Episode 2: Compras = 109, Ventas = 111, Reward = [0.15833612],Número de operaciones ganadoras = 2, Número de operaciones perdedoras = 0\n",
      "Episode 3: Compras = 117, Ventas = 109, Reward = [0.17216119],Número de operaciones ganadoras = 2, Número de operaciones perdedoras = 0\n",
      "Episode 4: Compras = 110, Ventas = 106, Reward = [0.20293912],Número de operaciones ganadoras = 2, Número de operaciones perdedoras = 0\n",
      "Episode 5: Compras = 125, Ventas = 94, Reward = [0.20413086],Número de operaciones ganadoras = 2, Número de operaciones perdedoras = 0\n",
      "Episode 6: Compras = 109, Ventas = 118, Reward = [0.19923548],Número de operaciones ganadoras = 2, Número de operaciones perdedoras = 0\n",
      "Episode 7: Compras = 114, Ventas = 108, Reward = [0.13213387],Número de operaciones ganadoras = 2, Número de operaciones perdedoras = 0\n",
      "Episode 8: Compras = 117, Ventas = 105, Reward = [0.18732303],Número de operaciones ganadoras = 2, Número de operaciones perdedoras = 0\n",
      "Episode 9: Compras = 100, Ventas = 115, Reward = [0.17022815],Número de operaciones ganadoras = 2, Número de operaciones perdedoras = 0\n",
      "Episode 10: Compras = 122, Ventas = 102, Reward = [0.15456696],Número de operaciones ganadoras = 2, Número de operaciones perdedoras = 0\n",
      "Promedio reward  10 episodes: 0.17271669209003448\n",
      "Promedio de compras: 113.1\n",
      "Promedio de ventas: 107.1\n",
      "Promedio Numero acciones ganadoras: 2.0\n",
      "Promedio Numero acciones perdedoras: 0.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Evaluar el agente entrenado\n",
    "def evaluate_agent(agent, eval_episodes=10):\n",
    "    total_rewards = []\n",
    "    for _ in range(eval_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action, _ = agent.predict(obs)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        total_rewards.append(total_reward)\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    print(f\"Average reward over {eval_episodes} episodes: {avg_reward}\")\n",
    "\n",
    "# Evaluar el agente entrenado\n",
    "evaluate_agent(model_rl)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LLZBHJ8WF9m7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1691521084704,
     "user_tz": 240,
     "elapsed": 3888,
     "user": {
      "displayName": "Italo Sanhueza",
      "userId": "00255692059650224046"
     }
    },
    "outputId": "45f3ef36-bb6b-4efb-bcf4-ba2a170062f8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Average reward over 10 episodes: 0.18665124475955963\n"
     ]
    }
   ]
  }
 ]
}
